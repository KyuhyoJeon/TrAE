{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from Models import ConvEncoder, ConvDecoder, mlpEncoder, mlpDecoder, TrAE, Classifier, TrLSTM, TrLinear\n",
    "from Dataset import data_provider, Trajectory2Dset\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to code for evaluating using accuracy, F1 score, AUROC, AUPRC of classification, and loss value of AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'is_training': 1,\n",
    "    'model': 'A', # ['A', 'B']\n",
    "    ### data loader\n",
    "    'root_path': './Data/',\n",
    "    'data': '2D/', # ['2D', '3D/old/', '3D/new/']\n",
    "    'data_path': 'csv/',\n",
    "    'data_type': 'normal', # ['noraml', 'mixed']\n",
    "    'checkpoints': './checkpoints/',\n",
    "    'Nsamples': 500, \n",
    "    'ratio': [7,1,2],\n",
    "    ### forecasting task\n",
    "    'seq_len': 3200, # [2144, 4576, 2656] # 3200 in paper\n",
    "    'label_len': 100,\n",
    "    'pred_len': 100,\n",
    "    'class_num': 5,\n",
    "    'individual': False,\n",
    "    ### transformer\n",
    "    'layer_num': 1,\n",
    "    'dropout': 0.1,\n",
    "    'max_len': 5000,\n",
    "    'd_model': 512,\n",
    "    'd_h': 8,\n",
    "    'd_ff': 2048,\n",
    "    ### Layers\n",
    "    'layer_num': 5,\n",
    "    'input_channel': 23,    # 2 in paper\n",
    "    't': 128,               # 16 in paper\n",
    "    'output_channel': 23,   # 4 in paper\n",
    "    'do_predict': True,\n",
    "    ### optimization\n",
    "    'num_workers': 8,\n",
    "    'itr': 1,\n",
    "    'train_epochs': 2000,   # 1000 in paper\n",
    "    'batch_size': 64,       # 64 in paper\n",
    "    'patience': 3,\n",
    "    'lr': 0.005,\n",
    "    ### GPU\n",
    "    'device': 'cuda:0',\n",
    "    'use_gpu': True,\n",
    "    'multi_gpu': True,\n",
    "})\n",
    "\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if args.data == '2D/':\n",
    "    args.data_path = ''\n",
    "    args.seq_len = 3200  # 3200 in paper\n",
    "    args.class_num = 4\n",
    "    args.input_channel = 2\n",
    "    args.t = 16\n",
    "    args.output_channel = 4\n",
    "    args.ratio = [3,1,1]\n",
    "elif args.data == '3D/old/':\n",
    "    args.data_path = ''\n",
    "    args.seq_len = 4576\n",
    "    args.input_channel = 3\n",
    "    args.t = 64\n",
    "    args.output_channel = 8\n",
    "elif args.data == '3D/new/':\n",
    "    args.input_channel = 3 # [3, 3, 3, 3, 3, 4, 4]\n",
    "    args.seq_len = 3200  # 3200 in paper\n",
    "    args.t = 16\n",
    "\n",
    "if args.model == 'B':\n",
    "    args.seq_len = 400\n",
    "print(args.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Dimensional Trajectory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 .pickle 로드 및 전처리 후 .npy 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 2144, 5), (2000, 1), 0.24863266944885254)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "'''\n",
    "Trajectory types: Normal, autopilot_lag, LOSrate_bias, LOSrate_delay\n",
    "Target distance = uniform(4500, 5500)\n",
    "states = [x, y, vm, path_angle] + 1 control input, 5 states\n",
    "label = 0, 1, 2, 3 for normal, autopilot_lag, LOSrate_bias, LOSrate_delay respectively, 4 classes\n",
    "minimum length of trajectory = 1360 (1360*0.01 = 13.6 seconds)\n",
    "maximum length of trajectory = 2131 (2131*0.01 = 21.31 seconds)\n",
    "fixed sequence length = 2144 (2144*0.01 = 21.44 seconds)\n",
    "'''\n",
    "start = time.time()\n",
    "with open('Data/2D/Trajectories.pickle', 'rb') as f:\n",
    "    Dataset = pickle.load(f)\n",
    "\n",
    "# normal = Dataset['normal_PNG']\n",
    "# lag = Dataset['autopilot_lag']\n",
    "# bias = Dataset['LOSrate_bias']\n",
    "# delay = Dataset['LOSrate_delay']\n",
    "seq_len = 2144\n",
    "\n",
    "label = 0\n",
    "x = []\n",
    "y = []\n",
    "for tr_type in Dataset:\n",
    "    states = Dataset[tr_type]['states']\n",
    "    inputs = Dataset[tr_type]['actions']\n",
    "    for i in range(len(states)):\n",
    "        tr = np.concatenate((states[i], np.insert(inputs[i], 0, 0).reshape(-1 ,1)), axis=1)\n",
    "        terminal = tr[-1].copy()\n",
    "        terminal[2]=terminal[4]=0\n",
    "        tr = np.concatenate((tr, np.tile(terminal, (seq_len-len(tr), 1))), axis=0)\n",
    "        x.append(tr)\n",
    "        y.append(label)\n",
    "    label += 1\n",
    "x = np.array(x)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "np.save('Data/2D/x.npy', x)\n",
    "np.save('Data/2D/y.npy', y)\n",
    "# x = np.load('Data/2D/x.npy')\n",
    "# y = np.load('Data/2D/y.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".npy 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 2144, 5), (2000, 1), 4.354981899261475)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.load('Data/2D/x.npy')\n",
    "y = np.load('Data/2D/y.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Dimentional Trajectory old version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 .pickle 로드 및 전처리 후 .npy 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 4576, 16), (700, 1), 438.2133049964905)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Trajectory types: Normal, Burn time, Xcp position, Thrust Tilt Angle, Fin bias\n",
    "Target Distance = 4000\n",
    "States = ['Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Accm_Cmd_1', 'Accm_Cmd_2', 'Accm_Cmd_3', 'PhiCmd'], 16 states\n",
    "label = [0, 1, 2, 3, 4] for normal, burn_time, xcp_pos, thrust_tilt, fin_bias respectively, 5 classes\n",
    "minimum length of trajectory = 412 (412*0.01 = 4.12 seconds)\n",
    "maximum length of trajectory = 4569 (4569*0.01 = 45.69 seconds)\n",
    "fixed sequence length = 4576 (4576*0.01 = 45.76 seconds)\n",
    "'''\n",
    "start = time.time()\n",
    "seq_len = 4576\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i, tr_type in enumerate(['Normal', 'Burntime', 'Xcpposition', \"ThrustTiltAngle\", 'Finbias']):\n",
    "    gid = pd.read_excel(f'Data/3D/old/Gid_{i+1}.xlsx', sheet_name=None)\n",
    "    msl = pd.read_excel(f'Data/3D/old/Msl_{i+1}.xlsx', sheet_name=None)\n",
    "    \n",
    "    for sheet in gid:\n",
    "        tr = pd.merge(msl[sheet][['Time', 'Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3']], gid[sheet][['Time', 'Accm_Cmd_1', 'Accm_Cmd_2', 'Accm_Cmd_3', 'Phi_Cmd']]).to_numpy()[:, 1:]\n",
    "        terminal = tr[-1].copy()\n",
    "        terminal[3]=terminal[4]=terminal[5]=terminal[6]=terminal[7]=terminal[8]=terminal[12]=terminal[13]=terminal[14]=terminal[15]=0\n",
    "        tr = np.concatenate((tr, np.tile(terminal, (seq_len-len(tr), 1))), axis=0)\n",
    "        if sheet == 'Sheet1' and tr_type == 'Normal':\n",
    "            x = [tr for _ in range(140)]\n",
    "            y = [i for _ in range(140)]\n",
    "        else:\n",
    "            x.append(tr)\n",
    "            y.append(i)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "np.save('Data/3D/x_old.npy', x)\n",
    "np.save('Data/3D/y_old.npy', y)\n",
    "# x = np.load('Data/3D/x_old.npy')\n",
    "# y = np.load('Data/3D/y_old.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".npy 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 4576, 16), (700, 1), 0.07254505157470703)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "x = np.load('Data/3D/x_old.npy')\n",
    "y = np.load('Data/3D/y_old.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Dimensional Trajectory new version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 .pickle 로드 및 전처리 후 .npy 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2500, 2656, 23), (2500, 1), 16.053280115127563)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Trajectory types: Normal, Burn time, Xcp position, Thrust Tilt Angle, Fin bias\n",
    "Target Distance = 4000\n",
    "Total states = ['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'err_FinBias_1', 'err_FinBias_2', 'err_FinBias_3', 'err_FinBias_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4', 'err_BurnTime', 'err_Tilt_1', 'err_Tilt_2', 'err_delXcp'], 31 states\n",
    "Used States = ['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4'], 23 states\n",
    "label = [0, 1, 2, 3, 4] for normal, burn_time, xcp_pos, thrust_tilt, fin_bias respectively, 5 classes\n",
    "minimum length of trajectory = 2449 (2448*0.01 = 24.48 seconds)\n",
    "maximum length of trajectory = 2635 (2635*0.01 = 26.35 seconds)\n",
    "fixed sequence length = 2656 (2656*0.01 = 26.56 seconds)\n",
    "'''\n",
    "start = time.time()\n",
    "seq_len = 2656\n",
    "x = []\n",
    "y = []\n",
    "tr = {}\n",
    "for i, tr_type in enumerate(['Normal', 'Burntime', 'Xcpposition', \"ThrustTiltAngle\", 'Finbias']):\n",
    "    N = 1 if i == 0 else 500\n",
    "    for tr_i in range(N):\n",
    "        tr = pd.read_csv(f'Data/3D/new/csv/Type_{i+1}_{tr_i+1}.csv', header=None)\n",
    "        tr.columns = ['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'err_FinBias_1', 'err_FinBias_2', 'err_FinBias_3', 'err_FinBias_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4', 'err_BurnTime', 'err_Tilt_1', 'err_Tilt_2', 'err_delXcp']\n",
    "        tr = tr[['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4']].to_numpy()\n",
    "        tr[0] = 4000 - tr[0]\n",
    "        terminal = tr[-1].copy()\n",
    "        terminal[3]=terminal[4]=terminal[5]=terminal[6]=terminal[7]=terminal[8]=terminal[12]=terminal[13]=terminal[14]=terminal[19]=terminal[20]=terminal[21]=terminal[22]=0\n",
    "        tr = np.concatenate((tr, np.tile(terminal, (seq_len-len(tr), 1))), axis=0)\n",
    "        if i == 0:\n",
    "            x = [tr for _ in range(500)]\n",
    "            y = [i for _ in range(500)]\n",
    "        else:\n",
    "            x.append(tr)\n",
    "            y.append(i)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "np.save('Data/3D/x_new.npy', x)\n",
    "np.save('Data/3D/y_new.npy', y)\n",
    "# x = np.load('Data/3D/x_new.npy')\n",
    "# y = np.load('Data/3D/y_new.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".npy 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2500, 2656, 23), (2500, 1), 0.24094510078430176)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "x = np.load('Data/3D/x_new.npy')\n",
    "y = np.load('Data/3D/y_new.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Network Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_loader = data_provider(args, 'train')\n",
    "train_c_set, train_c_loader = data_provider(args, 'train_c')\n",
    "valid_set, valid_loader = data_provider(args, 'valid')\n",
    "test_set, test_loader = data_provider(args, 'test')\n",
    "total_set, total_loader = data_provider(args, 'pred')\n",
    "# next(iter(train_loader))[0].shape, next(iter(train_loader))[1].shape, len(train_loader), train_set.data_x.shape, train_set.data_y.shape\n",
    "# next(iter(valid_loader))[0].shape, next(iter(valid_loader))[1].shape, len(valid_loader), valid_set.data_x.shape, valid_set.data_y.shape\n",
    "# next(iter(test_loader))[0].shape, next(iter(test_loader))[1].shape, len(test_loader), test_set.data_x.shape, test_set.data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrAE(ConvEncoder, ConvDecoder, args).to(args.device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AE_train(model, dataloaders, criterion, optimizer, num_epochs=10, channels = 2, channel_type = None):\n",
    "    \"\"\"\n",
    "    model: model to train\n",
    "    dataloaders: train, val, test data's loader\n",
    "    criterion: loss function\n",
    "    optimizer: optimizer to update your model\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = 100000000\n",
    "    \n",
    "    if channel_type == None:\n",
    "        cl = 0\n",
    "    elif channel_type == 'Rmi':\n",
    "        cl = 0\n",
    "    elif channel_type == 'Vmi':\n",
    "        cl = 3\n",
    "    elif channel_type == 'Wmb':\n",
    "        cl = 6\n",
    "    elif channel_type == 'Accm':\n",
    "        cl = 9\n",
    "    elif channel_type == 'ang':\n",
    "        cl = 12\n",
    "    elif channel_type == 'Fout':\n",
    "        cl = 15\n",
    "    elif channel_type == 'Fcmd':\n",
    "        cl = 19\n",
    "    cr = cl + channels\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()            # Set model to training mode\n",
    "            else:\n",
    "                model.eval()            # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(args.device)[:,:,cl:cr].type(torch.float32).transpose(1, 2)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    outputs, encoded = model(inputs)\n",
    "                    loss = criterion(outputs, inputs)   \n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()        \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            if phase == 'val':\n",
    "                val_loss_history.append(epoch_loss)\n",
    "            if phase == 'val' and epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_val_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "    return model, best_model_wts, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "----------\n",
      "train Loss: 115654.4000\n",
      "val Loss: 104900.9413\n",
      "\n",
      "Epoch 2/1000\n",
      "----------\n",
      "train Loss: 39626.1866\n",
      "val Loss: 89533.8837\n",
      "\n",
      "Epoch 3/1000\n",
      "----------\n",
      "train Loss: 4395.8451\n",
      "val Loss: 9051.5105\n",
      "\n",
      "Epoch 4/1000\n",
      "----------\n",
      "train Loss: 1712.5054\n",
      "val Loss: 2440.2448\n",
      "\n",
      "Epoch 5/1000\n",
      "----------\n",
      "train Loss: 1384.3184\n",
      "val Loss: 1592.4000\n",
      "\n",
      "Epoch 6/1000\n",
      "----------\n",
      "train Loss: 1290.1875\n",
      "val Loss: 1478.2727\n",
      "\n",
      "Epoch 7/1000\n",
      "----------\n",
      "train Loss: 1250.5116\n",
      "val Loss: 1472.1470\n",
      "\n",
      "Epoch 8/1000\n",
      "----------\n",
      "train Loss: 1211.2242\n",
      "val Loss: 1332.7149\n",
      "\n",
      "Epoch 9/1000\n",
      "----------\n",
      "train Loss: 1164.0927\n",
      "val Loss: 1289.6269\n",
      "\n",
      "Epoch 10/1000\n",
      "----------\n",
      "train Loss: 1122.0343\n",
      "val Loss: 1218.0771\n",
      "\n",
      "Epoch 11/1000\n",
      "----------\n",
      "train Loss: 1088.5536\n",
      "val Loss: 1172.0240\n",
      "\n",
      "Epoch 12/1000\n",
      "----------\n",
      "train Loss: 1061.0233\n",
      "val Loss: 1183.6876\n",
      "\n",
      "Epoch 13/1000\n",
      "----------\n",
      "train Loss: 1035.6090\n",
      "val Loss: 1173.5838\n",
      "\n",
      "Epoch 14/1000\n",
      "----------\n",
      "train Loss: 999.7187\n",
      "val Loss: 1080.8990\n",
      "\n",
      "Epoch 15/1000\n",
      "----------\n",
      "train Loss: 977.0312\n",
      "val Loss: 1048.1596\n",
      "\n",
      "Epoch 16/1000\n",
      "----------\n",
      "train Loss: 956.7837\n",
      "val Loss: 1014.0250\n",
      "\n",
      "Epoch 17/1000\n",
      "----------\n",
      "train Loss: 942.5845\n",
      "val Loss: 1027.7953\n",
      "\n",
      "Epoch 18/1000\n",
      "----------\n",
      "train Loss: 914.2194\n",
      "val Loss: 991.3246\n",
      "\n",
      "Epoch 19/1000\n",
      "----------\n",
      "train Loss: 903.4125\n",
      "val Loss: 974.4980\n",
      "\n",
      "Epoch 20/1000\n",
      "----------\n",
      "train Loss: 891.4883\n",
      "val Loss: 957.2493\n",
      "\n",
      "Epoch 21/1000\n",
      "----------\n",
      "train Loss: 879.8281\n",
      "val Loss: 970.3652\n",
      "\n",
      "Epoch 22/1000\n",
      "----------\n",
      "train Loss: 874.0259\n",
      "val Loss: 1027.5858\n",
      "\n",
      "Epoch 23/1000\n",
      "----------\n",
      "train Loss: 881.8846\n",
      "val Loss: 1087.2554\n",
      "\n",
      "Epoch 24/1000\n",
      "----------\n",
      "train Loss: 888.3311\n",
      "val Loss: 1052.6534\n",
      "\n",
      "Epoch 25/1000\n",
      "----------\n",
      "train Loss: 872.7480\n",
      "val Loss: 994.6686\n",
      "\n",
      "Epoch 26/1000\n",
      "----------\n",
      "train Loss: 883.2731\n",
      "val Loss: 1170.1908\n",
      "\n",
      "Epoch 27/1000\n",
      "----------\n",
      "train Loss: 865.9972\n",
      "val Loss: 1144.6080\n",
      "\n",
      "Epoch 28/1000\n",
      "----------\n",
      "train Loss: 842.4652\n",
      "val Loss: 1077.9653\n",
      "\n",
      "Epoch 29/1000\n",
      "----------\n",
      "train Loss: 857.1451\n",
      "val Loss: 6417.7612\n",
      "\n",
      "Epoch 30/1000\n",
      "----------\n",
      "train Loss: 837.2411\n",
      "val Loss: 1279.7760\n",
      "\n",
      "Epoch 31/1000\n",
      "----------\n",
      "train Loss: 842.6930\n",
      "val Loss: 919.8263\n",
      "\n",
      "Epoch 32/1000\n",
      "----------\n",
      "train Loss: 829.8825\n",
      "val Loss: 924.1828\n",
      "\n",
      "Epoch 33/1000\n",
      "----------\n",
      "train Loss: 844.0002\n",
      "val Loss: 931.6632\n",
      "\n",
      "Epoch 34/1000\n",
      "----------\n",
      "train Loss: 835.8416\n",
      "val Loss: 910.7879\n",
      "\n",
      "Epoch 35/1000\n",
      "----------\n",
      "train Loss: 831.0383\n",
      "val Loss: 929.2917\n",
      "\n",
      "Epoch 36/1000\n",
      "----------\n",
      "train Loss: 839.7786\n",
      "val Loss: 983.4997\n",
      "\n",
      "Epoch 37/1000\n",
      "----------\n",
      "train Loss: 842.7742\n",
      "val Loss: 903.0369\n",
      "\n",
      "Epoch 38/1000\n",
      "----------\n",
      "train Loss: 830.4067\n",
      "val Loss: 917.7661\n",
      "\n",
      "Epoch 39/1000\n",
      "----------\n",
      "train Loss: 822.0317\n",
      "val Loss: 917.6024\n",
      "\n",
      "Epoch 40/1000\n",
      "----------\n",
      "train Loss: 827.8244\n",
      "val Loss: 1019.7189\n",
      "\n",
      "Epoch 41/1000\n",
      "----------\n",
      "train Loss: 834.0883\n",
      "val Loss: 978.6121\n",
      "\n",
      "Epoch 42/1000\n",
      "----------\n",
      "train Loss: 830.2050\n",
      "val Loss: 892.5153\n",
      "\n",
      "Epoch 43/1000\n",
      "----------\n",
      "train Loss: 821.0337\n",
      "val Loss: 875.4060\n",
      "\n",
      "Epoch 44/1000\n",
      "----------\n",
      "train Loss: 813.2989\n",
      "val Loss: 1017.2194\n",
      "\n",
      "Epoch 45/1000\n",
      "----------\n",
      "train Loss: 816.9080\n",
      "val Loss: 897.6345\n",
      "\n",
      "Epoch 46/1000\n",
      "----------\n",
      "train Loss: 820.4790\n",
      "val Loss: 1042.0012\n",
      "\n",
      "Epoch 47/1000\n",
      "----------\n",
      "train Loss: 817.0492\n",
      "val Loss: 1277.2255\n",
      "\n",
      "Epoch 48/1000\n",
      "----------\n",
      "train Loss: 836.8097\n",
      "val Loss: 1462.0322\n",
      "\n",
      "Epoch 49/1000\n",
      "----------\n",
      "train Loss: 858.5961\n",
      "val Loss: 1530.1801\n",
      "\n",
      "Epoch 50/1000\n",
      "----------\n",
      "train Loss: 835.2576\n",
      "val Loss: 893.6979\n",
      "\n",
      "Epoch 51/1000\n",
      "----------\n",
      "train Loss: 875.5527\n",
      "val Loss: 929.6410\n",
      "\n",
      "Epoch 52/1000\n",
      "----------\n",
      "train Loss: 850.5529\n",
      "val Loss: 920.9489\n",
      "\n",
      "Epoch 53/1000\n",
      "----------\n",
      "train Loss: 851.5580\n",
      "val Loss: 884.8329\n",
      "\n",
      "Epoch 54/1000\n",
      "----------\n",
      "train Loss: 853.9281\n",
      "val Loss: 955.6871\n",
      "\n",
      "Epoch 55/1000\n",
      "----------\n",
      "train Loss: 810.4159\n",
      "val Loss: 1235.0117\n",
      "\n",
      "Epoch 56/1000\n",
      "----------\n",
      "train Loss: 813.9542\n",
      "val Loss: 4983.8712\n",
      "\n",
      "Epoch 57/1000\n",
      "----------\n",
      "train Loss: 802.4431\n",
      "val Loss: 1312.7243\n",
      "\n",
      "Epoch 58/1000\n",
      "----------\n",
      "train Loss: 822.3437\n",
      "val Loss: 911.4753\n",
      "\n",
      "Epoch 59/1000\n",
      "----------\n",
      "train Loss: 829.7752\n",
      "val Loss: 935.1841\n",
      "\n",
      "Epoch 60/1000\n",
      "----------\n",
      "train Loss: 806.5252\n",
      "val Loss: 1070.7021\n",
      "\n",
      "Epoch 61/1000\n",
      "----------\n",
      "train Loss: 797.0815\n",
      "val Loss: 1257.1810\n",
      "\n",
      "Epoch 62/1000\n",
      "----------\n",
      "train Loss: 798.0820\n",
      "val Loss: 1278.2751\n",
      "\n",
      "Epoch 63/1000\n",
      "----------\n",
      "train Loss: 819.8831\n",
      "val Loss: 1433.6344\n",
      "\n",
      "Epoch 64/1000\n",
      "----------\n",
      "train Loss: 801.4525\n",
      "val Loss: 895.1887\n",
      "\n",
      "Epoch 65/1000\n",
      "----------\n",
      "train Loss: 818.7897\n",
      "val Loss: 2984.5103\n",
      "\n",
      "Epoch 66/1000\n",
      "----------\n",
      "train Loss: 813.0198\n",
      "val Loss: 1250.5360\n",
      "\n",
      "Epoch 67/1000\n",
      "----------\n",
      "train Loss: 814.3378\n",
      "val Loss: 878.5069\n",
      "\n",
      "Epoch 68/1000\n",
      "----------\n",
      "train Loss: 816.3651\n",
      "val Loss: 992.6124\n",
      "\n",
      "Epoch 69/1000\n",
      "----------\n",
      "train Loss: 819.8409\n",
      "val Loss: 1416.6166\n",
      "\n",
      "Epoch 70/1000\n",
      "----------\n",
      "train Loss: 807.3249\n",
      "val Loss: 976.4086\n",
      "\n",
      "Epoch 71/1000\n",
      "----------\n",
      "train Loss: 811.9699\n",
      "val Loss: 1351.4695\n",
      "\n",
      "Epoch 72/1000\n",
      "----------\n",
      "train Loss: 800.7061\n",
      "val Loss: 1195.7955\n",
      "\n",
      "Epoch 73/1000\n",
      "----------\n",
      "train Loss: 842.8763\n",
      "val Loss: 980.5855\n",
      "\n",
      "Epoch 74/1000\n",
      "----------\n",
      "train Loss: 810.0628\n",
      "val Loss: 1023.7946\n",
      "\n",
      "Epoch 75/1000\n",
      "----------\n",
      "train Loss: 814.1746\n",
      "val Loss: 2757.6485\n",
      "\n",
      "Epoch 76/1000\n",
      "----------\n",
      "train Loss: 805.4864\n",
      "val Loss: 1264.9962\n",
      "\n",
      "Epoch 77/1000\n",
      "----------\n",
      "train Loss: 833.1822\n",
      "val Loss: 888.4816\n",
      "\n",
      "Epoch 78/1000\n",
      "----------\n",
      "train Loss: 816.6936\n",
      "val Loss: 1084.2324\n",
      "\n",
      "Epoch 79/1000\n",
      "----------\n",
      "train Loss: 823.1152\n",
      "val Loss: 1279.6787\n",
      "\n",
      "Epoch 80/1000\n",
      "----------\n",
      "train Loss: 802.2020\n",
      "val Loss: 884.0195\n",
      "\n",
      "Epoch 81/1000\n",
      "----------\n",
      "train Loss: 809.5379\n",
      "val Loss: 1053.5559\n",
      "\n",
      "Epoch 82/1000\n",
      "----------\n",
      "train Loss: 806.8187\n",
      "val Loss: 1014.3737\n",
      "\n",
      "Epoch 83/1000\n",
      "----------\n",
      "train Loss: 797.1702\n",
      "val Loss: 893.6006\n",
      "\n",
      "Epoch 84/1000\n",
      "----------\n",
      "train Loss: 810.4195\n",
      "val Loss: 860.1947\n",
      "\n",
      "Epoch 85/1000\n",
      "----------\n",
      "train Loss: 819.6545\n",
      "val Loss: 913.9501\n",
      "\n",
      "Epoch 86/1000\n",
      "----------\n",
      "train Loss: 807.3733\n",
      "val Loss: 1704.7763\n",
      "\n",
      "Epoch 87/1000\n",
      "----------\n",
      "train Loss: 799.3659\n",
      "val Loss: 871.8230\n",
      "\n",
      "Epoch 88/1000\n",
      "----------\n",
      "train Loss: 795.5988\n",
      "val Loss: 1533.1831\n",
      "\n",
      "Epoch 89/1000\n",
      "----------\n",
      "train Loss: 827.4003\n",
      "val Loss: 1130.0561\n",
      "\n",
      "Epoch 90/1000\n",
      "----------\n",
      "train Loss: 811.0587\n",
      "val Loss: 1079.8855\n",
      "\n",
      "Epoch 91/1000\n",
      "----------\n",
      "train Loss: 824.6413\n",
      "val Loss: 1373.1003\n",
      "\n",
      "Epoch 92/1000\n",
      "----------\n",
      "train Loss: 811.7555\n",
      "val Loss: 1052.2848\n",
      "\n",
      "Epoch 93/1000\n",
      "----------\n",
      "train Loss: 802.0116\n",
      "val Loss: 1164.2967\n",
      "\n",
      "Epoch 94/1000\n",
      "----------\n",
      "train Loss: 821.2948\n",
      "val Loss: 1432.5052\n",
      "\n",
      "Epoch 95/1000\n",
      "----------\n",
      "train Loss: 806.9783\n",
      "val Loss: 900.1149\n",
      "\n",
      "Epoch 96/1000\n",
      "----------\n",
      "train Loss: 798.2695\n",
      "val Loss: 867.5855\n",
      "\n",
      "Epoch 97/1000\n",
      "----------\n",
      "train Loss: 815.0560\n",
      "val Loss: 3655.6948\n",
      "\n",
      "Epoch 98/1000\n",
      "----------\n",
      "train Loss: 858.8166\n",
      "val Loss: 1040.5490\n",
      "\n",
      "Epoch 99/1000\n",
      "----------\n",
      "train Loss: 838.4882\n",
      "val Loss: 993.2030\n",
      "\n",
      "Epoch 100/1000\n",
      "----------\n",
      "train Loss: 811.6259\n",
      "val Loss: 1094.1851\n",
      "\n",
      "Epoch 101/1000\n",
      "----------\n",
      "train Loss: 805.3520\n",
      "val Loss: 1854.4662\n",
      "\n",
      "Epoch 102/1000\n",
      "----------\n",
      "train Loss: 802.7030\n",
      "val Loss: 1066.6007\n",
      "\n",
      "Epoch 103/1000\n",
      "----------\n",
      "train Loss: 836.2296\n",
      "val Loss: 1274.4961\n",
      "\n",
      "Epoch 104/1000\n",
      "----------\n",
      "train Loss: 825.8669\n",
      "val Loss: 863.6499\n",
      "\n",
      "Epoch 105/1000\n",
      "----------\n",
      "train Loss: 798.4507\n",
      "val Loss: 1393.6641\n",
      "\n",
      "Epoch 106/1000\n",
      "----------\n",
      "train Loss: 824.8137\n",
      "val Loss: 873.2891\n",
      "\n",
      "Epoch 107/1000\n",
      "----------\n",
      "train Loss: 800.2455\n",
      "val Loss: 878.6703\n",
      "\n",
      "Epoch 108/1000\n",
      "----------\n",
      "train Loss: 815.4853\n",
      "val Loss: 1006.2755\n",
      "\n",
      "Epoch 109/1000\n",
      "----------\n",
      "train Loss: 847.0906\n",
      "val Loss: 1919.8236\n",
      "\n",
      "Epoch 110/1000\n",
      "----------\n",
      "train Loss: 822.9576\n",
      "val Loss: 2613.0957\n",
      "\n",
      "Epoch 111/1000\n",
      "----------\n",
      "train Loss: 841.0502\n",
      "val Loss: 1037.2192\n",
      "\n",
      "Epoch 112/1000\n",
      "----------\n",
      "train Loss: 833.9709\n",
      "val Loss: 2394.2170\n",
      "\n",
      "Epoch 113/1000\n",
      "----------\n",
      "train Loss: 823.0726\n",
      "val Loss: 1058.5432\n",
      "\n",
      "Epoch 114/1000\n",
      "----------\n",
      "train Loss: 878.6364\n",
      "val Loss: 1175.5826\n",
      "\n",
      "Epoch 115/1000\n",
      "----------\n",
      "train Loss: 871.9507\n",
      "val Loss: 2108.1498\n",
      "\n",
      "Epoch 116/1000\n",
      "----------\n",
      "train Loss: 819.8724\n",
      "val Loss: 1166.2675\n",
      "\n",
      "Epoch 117/1000\n",
      "----------\n",
      "train Loss: 799.7155\n",
      "val Loss: 998.0848\n",
      "\n",
      "Epoch 118/1000\n",
      "----------\n",
      "train Loss: 815.0313\n",
      "val Loss: 932.2937\n",
      "\n",
      "Epoch 119/1000\n",
      "----------\n",
      "train Loss: 813.2714\n",
      "val Loss: 1587.6937\n",
      "\n",
      "Epoch 120/1000\n",
      "----------\n",
      "train Loss: 827.7062\n",
      "val Loss: 2217.7400\n",
      "\n",
      "Epoch 121/1000\n",
      "----------\n",
      "train Loss: 816.4354\n",
      "val Loss: 908.8335\n",
      "\n",
      "Epoch 122/1000\n",
      "----------\n",
      "train Loss: 796.1534\n",
      "val Loss: 953.3767\n",
      "\n",
      "Epoch 123/1000\n",
      "----------\n",
      "train Loss: 808.3989\n",
      "val Loss: 871.5983\n",
      "\n",
      "Epoch 124/1000\n",
      "----------\n",
      "train Loss: 815.9768\n",
      "val Loss: 1242.5842\n",
      "\n",
      "Epoch 125/1000\n",
      "----------\n",
      "train Loss: 822.0972\n",
      "val Loss: 1367.1126\n",
      "\n",
      "Epoch 126/1000\n",
      "----------\n",
      "train Loss: 825.4935\n",
      "val Loss: 902.4552\n",
      "\n",
      "Epoch 127/1000\n",
      "----------\n",
      "train Loss: 862.2504\n",
      "val Loss: 1507.2067\n",
      "\n",
      "Epoch 128/1000\n",
      "----------\n",
      "train Loss: 838.3483\n",
      "val Loss: 907.5875\n",
      "\n",
      "Epoch 129/1000\n",
      "----------\n",
      "train Loss: 830.9332\n",
      "val Loss: 969.7914\n",
      "\n",
      "Epoch 130/1000\n",
      "----------\n",
      "train Loss: 825.1298\n",
      "val Loss: 1046.5796\n",
      "\n",
      "Epoch 131/1000\n",
      "----------\n",
      "train Loss: 830.2456\n",
      "val Loss: 871.1315\n",
      "\n",
      "Epoch 132/1000\n",
      "----------\n",
      "train Loss: 811.1476\n",
      "val Loss: 1015.1177\n",
      "\n",
      "Epoch 133/1000\n",
      "----------\n",
      "train Loss: 799.5382\n",
      "val Loss: 1105.8934\n",
      "\n",
      "Epoch 134/1000\n",
      "----------\n",
      "train Loss: 837.2197\n",
      "val Loss: 1795.3808\n",
      "\n",
      "Epoch 135/1000\n",
      "----------\n",
      "train Loss: 833.9977\n",
      "val Loss: 1288.7236\n",
      "\n",
      "Epoch 136/1000\n",
      "----------\n",
      "train Loss: 824.7154\n",
      "val Loss: 867.1583\n",
      "\n",
      "Epoch 137/1000\n",
      "----------\n",
      "train Loss: 814.2366\n",
      "val Loss: 1538.4555\n",
      "\n",
      "Epoch 138/1000\n",
      "----------\n",
      "train Loss: 809.3217\n",
      "val Loss: 950.4536\n",
      "\n",
      "Epoch 139/1000\n",
      "----------\n",
      "train Loss: 832.9732\n",
      "val Loss: 962.8975\n",
      "\n",
      "Epoch 140/1000\n",
      "----------\n",
      "train Loss: 819.4155\n",
      "val Loss: 1673.6119\n",
      "\n",
      "Epoch 141/1000\n",
      "----------\n",
      "train Loss: 820.0674\n",
      "val Loss: 924.5266\n",
      "\n",
      "Epoch 142/1000\n",
      "----------\n",
      "train Loss: 811.6203\n",
      "val Loss: 865.3654\n",
      "\n",
      "Epoch 143/1000\n",
      "----------\n",
      "train Loss: 796.3393\n",
      "val Loss: 846.6374\n",
      "\n",
      "Epoch 144/1000\n",
      "----------\n",
      "train Loss: 807.1328\n",
      "val Loss: 934.9046\n",
      "\n",
      "Epoch 145/1000\n",
      "----------\n",
      "train Loss: 826.2494\n",
      "val Loss: 1183.4100\n",
      "\n",
      "Epoch 146/1000\n",
      "----------\n",
      "train Loss: 838.9939\n",
      "val Loss: 1374.7210\n",
      "\n",
      "Epoch 147/1000\n",
      "----------\n",
      "train Loss: 811.4911\n",
      "val Loss: 897.5461\n",
      "\n",
      "Epoch 148/1000\n",
      "----------\n",
      "train Loss: 841.7312\n",
      "val Loss: 1349.5965\n",
      "\n",
      "Epoch 149/1000\n",
      "----------\n",
      "train Loss: 816.5594\n",
      "val Loss: 979.4411\n",
      "\n",
      "Epoch 150/1000\n",
      "----------\n",
      "train Loss: 801.8511\n",
      "val Loss: 1194.7821\n",
      "\n",
      "Epoch 151/1000\n",
      "----------\n",
      "train Loss: 810.9566\n",
      "val Loss: 892.1438\n",
      "\n",
      "Epoch 152/1000\n",
      "----------\n",
      "train Loss: 799.5275\n",
      "val Loss: 870.7784\n",
      "\n",
      "Epoch 153/1000\n",
      "----------\n",
      "train Loss: 802.6547\n",
      "val Loss: 1061.3114\n",
      "\n",
      "Epoch 154/1000\n",
      "----------\n",
      "train Loss: 809.0368\n",
      "val Loss: 953.0722\n",
      "\n",
      "Epoch 155/1000\n",
      "----------\n",
      "train Loss: 816.5654\n",
      "val Loss: 870.3929\n",
      "\n",
      "Epoch 156/1000\n",
      "----------\n",
      "train Loss: 801.2657\n",
      "val Loss: 861.9135\n",
      "\n",
      "Epoch 157/1000\n",
      "----------\n",
      "train Loss: 793.5489\n",
      "val Loss: 932.4411\n",
      "\n",
      "Epoch 158/1000\n",
      "----------\n",
      "train Loss: 825.9134\n",
      "val Loss: 1075.3947\n",
      "\n",
      "Epoch 159/1000\n",
      "----------\n",
      "train Loss: 795.1760\n",
      "val Loss: 920.3508\n",
      "\n",
      "Epoch 160/1000\n",
      "----------\n",
      "train Loss: 809.2793\n",
      "val Loss: 1771.3977\n",
      "\n",
      "Epoch 161/1000\n",
      "----------\n",
      "train Loss: 802.4766\n",
      "val Loss: 898.0057\n",
      "\n",
      "Epoch 162/1000\n",
      "----------\n",
      "train Loss: 797.7900\n",
      "val Loss: 880.2666\n",
      "\n",
      "Epoch 163/1000\n",
      "----------\n",
      "train Loss: 806.2589\n",
      "val Loss: 861.8370\n",
      "\n",
      "Epoch 164/1000\n",
      "----------\n",
      "train Loss: 804.8127\n",
      "val Loss: 938.5041\n",
      "\n",
      "Epoch 165/1000\n",
      "----------\n",
      "train Loss: 809.7111\n",
      "val Loss: 1216.2160\n",
      "\n",
      "Epoch 166/1000\n",
      "----------\n",
      "train Loss: 816.4355\n",
      "val Loss: 1674.7394\n",
      "\n",
      "Epoch 167/1000\n",
      "----------\n",
      "train Loss: 808.1381\n",
      "val Loss: 868.6237\n",
      "\n",
      "Epoch 168/1000\n",
      "----------\n",
      "train Loss: 795.6478\n",
      "val Loss: 1035.4076\n",
      "\n",
      "Epoch 169/1000\n",
      "----------\n",
      "train Loss: 794.5307\n",
      "val Loss: 901.6618\n",
      "\n",
      "Epoch 170/1000\n",
      "----------\n",
      "train Loss: 802.3421\n",
      "val Loss: 931.6187\n",
      "\n",
      "Epoch 171/1000\n",
      "----------\n",
      "train Loss: 799.2809\n",
      "val Loss: 1010.5899\n",
      "\n",
      "Epoch 172/1000\n",
      "----------\n",
      "train Loss: 806.8246\n",
      "val Loss: 1094.5229\n",
      "\n",
      "Epoch 173/1000\n",
      "----------\n",
      "train Loss: 816.8426\n",
      "val Loss: 995.0012\n",
      "\n",
      "Epoch 174/1000\n",
      "----------\n",
      "train Loss: 833.2877\n",
      "val Loss: 1687.5435\n",
      "\n",
      "Epoch 175/1000\n",
      "----------\n",
      "train Loss: 807.4903\n",
      "val Loss: 1157.8968\n",
      "\n",
      "Epoch 176/1000\n",
      "----------\n",
      "train Loss: 811.6603\n",
      "val Loss: 970.6680\n",
      "\n",
      "Epoch 177/1000\n",
      "----------\n",
      "train Loss: 813.8367\n",
      "val Loss: 983.1395\n",
      "\n",
      "Epoch 178/1000\n",
      "----------\n",
      "train Loss: 829.0329\n",
      "val Loss: 1099.3559\n",
      "\n",
      "Epoch 179/1000\n",
      "----------\n",
      "train Loss: 817.6594\n",
      "val Loss: 901.3103\n",
      "\n",
      "Epoch 180/1000\n",
      "----------\n",
      "train Loss: 810.6862\n",
      "val Loss: 853.8325\n",
      "\n",
      "Epoch 181/1000\n",
      "----------\n",
      "train Loss: 822.8143\n",
      "val Loss: 1094.4490\n",
      "\n",
      "Epoch 182/1000\n",
      "----------\n",
      "train Loss: 817.6151\n",
      "val Loss: 871.2152\n",
      "\n",
      "Epoch 183/1000\n",
      "----------\n",
      "train Loss: 826.3586\n",
      "val Loss: 1128.8127\n",
      "\n",
      "Epoch 184/1000\n",
      "----------\n",
      "train Loss: 805.6977\n",
      "val Loss: 940.9221\n",
      "\n",
      "Epoch 185/1000\n",
      "----------\n",
      "train Loss: 819.3641\n",
      "val Loss: 993.9887\n",
      "\n",
      "Epoch 186/1000\n",
      "----------\n",
      "train Loss: 816.1438\n",
      "val Loss: 1186.3397\n",
      "\n",
      "Epoch 187/1000\n",
      "----------\n",
      "train Loss: 833.1586\n",
      "val Loss: 899.7300\n",
      "\n",
      "Epoch 188/1000\n",
      "----------\n",
      "train Loss: 813.7219\n",
      "val Loss: 884.3292\n",
      "\n",
      "Epoch 189/1000\n",
      "----------\n",
      "train Loss: 808.5029\n",
      "val Loss: 971.3024\n",
      "\n",
      "Epoch 190/1000\n",
      "----------\n",
      "train Loss: 858.8722\n",
      "val Loss: 897.7014\n",
      "\n",
      "Epoch 191/1000\n",
      "----------\n",
      "train Loss: 921.2381\n",
      "val Loss: 926.0452\n",
      "\n",
      "Epoch 192/1000\n",
      "----------\n",
      "train Loss: 844.9679\n",
      "val Loss: 878.0083\n",
      "\n",
      "Epoch 193/1000\n",
      "----------\n",
      "train Loss: 838.0821\n",
      "val Loss: 910.7745\n",
      "\n",
      "Epoch 194/1000\n",
      "----------\n",
      "train Loss: 900.5989\n",
      "val Loss: 877.5409\n",
      "\n",
      "Epoch 195/1000\n",
      "----------\n",
      "train Loss: 816.0132\n",
      "val Loss: 1192.2667\n",
      "\n",
      "Epoch 196/1000\n",
      "----------\n",
      "train Loss: 810.1446\n",
      "val Loss: 1351.2510\n",
      "\n",
      "Epoch 197/1000\n",
      "----------\n",
      "train Loss: 805.2047\n",
      "val Loss: 954.2264\n",
      "\n",
      "Epoch 198/1000\n",
      "----------\n",
      "train Loss: 842.6237\n",
      "val Loss: 1353.5360\n",
      "\n",
      "Epoch 199/1000\n",
      "----------\n",
      "train Loss: 799.3891\n",
      "val Loss: 876.3740\n",
      "\n",
      "Epoch 200/1000\n",
      "----------\n",
      "train Loss: 812.2190\n",
      "val Loss: 1768.8804\n",
      "\n",
      "Epoch 201/1000\n",
      "----------\n",
      "train Loss: 811.7165\n",
      "val Loss: 875.5476\n",
      "\n",
      "Epoch 202/1000\n",
      "----------\n",
      "train Loss: 814.6099\n",
      "val Loss: 885.0572\n",
      "\n",
      "Epoch 203/1000\n",
      "----------\n",
      "train Loss: 807.4911\n",
      "val Loss: 870.8451\n",
      "\n",
      "Epoch 204/1000\n",
      "----------\n",
      "train Loss: 816.2278\n",
      "val Loss: 865.4559\n",
      "\n",
      "Epoch 205/1000\n",
      "----------\n",
      "train Loss: 798.4498\n",
      "val Loss: 1339.5493\n",
      "\n",
      "Epoch 206/1000\n",
      "----------\n",
      "train Loss: 820.8179\n",
      "val Loss: 910.1099\n",
      "\n",
      "Epoch 207/1000\n",
      "----------\n",
      "train Loss: 806.7487\n",
      "val Loss: 954.1778\n",
      "\n",
      "Epoch 208/1000\n",
      "----------\n",
      "train Loss: 843.2408\n",
      "val Loss: 1074.4572\n",
      "\n",
      "Epoch 209/1000\n",
      "----------\n",
      "train Loss: 811.9677\n",
      "val Loss: 1935.4038\n",
      "\n",
      "Epoch 210/1000\n",
      "----------\n",
      "train Loss: 804.1957\n",
      "val Loss: 1063.9119\n",
      "\n",
      "Epoch 211/1000\n",
      "----------\n",
      "train Loss: 807.2634\n",
      "val Loss: 925.4387\n",
      "\n",
      "Epoch 212/1000\n",
      "----------\n",
      "train Loss: 809.2345\n",
      "val Loss: 921.2203\n",
      "\n",
      "Epoch 213/1000\n",
      "----------\n",
      "train Loss: 812.8946\n",
      "val Loss: 869.4735\n",
      "\n",
      "Epoch 214/1000\n",
      "----------\n",
      "train Loss: 803.7035\n",
      "val Loss: 1703.1198\n",
      "\n",
      "Epoch 215/1000\n",
      "----------\n",
      "train Loss: 817.9576\n",
      "val Loss: 1146.4216\n",
      "\n",
      "Epoch 216/1000\n",
      "----------\n",
      "train Loss: 823.7799\n",
      "val Loss: 1742.5591\n",
      "\n",
      "Epoch 217/1000\n",
      "----------\n",
      "train Loss: 912.0554\n",
      "val Loss: 1499.8566\n",
      "\n",
      "Epoch 218/1000\n",
      "----------\n",
      "train Loss: 980.4385\n",
      "val Loss: 1341.2174\n",
      "\n",
      "Epoch 219/1000\n",
      "----------\n",
      "train Loss: 894.4640\n",
      "val Loss: 900.1068\n",
      "\n",
      "Epoch 220/1000\n",
      "----------\n",
      "train Loss: 851.5383\n",
      "val Loss: 921.0773\n",
      "\n",
      "Epoch 221/1000\n",
      "----------\n",
      "train Loss: 801.5924\n",
      "val Loss: 872.5807\n",
      "\n",
      "Epoch 222/1000\n",
      "----------\n",
      "train Loss: 801.7178\n",
      "val Loss: 1453.0200\n",
      "\n",
      "Epoch 223/1000\n",
      "----------\n",
      "train Loss: 807.1625\n",
      "val Loss: 938.9872\n",
      "\n",
      "Epoch 224/1000\n",
      "----------\n",
      "train Loss: 806.9462\n",
      "val Loss: 924.9550\n",
      "\n",
      "Epoch 225/1000\n",
      "----------\n",
      "train Loss: 805.8824\n",
      "val Loss: 1033.0670\n",
      "\n",
      "Epoch 226/1000\n",
      "----------\n",
      "train Loss: 819.1182\n",
      "val Loss: 956.2041\n",
      "\n",
      "Epoch 227/1000\n",
      "----------\n",
      "train Loss: 802.5153\n",
      "val Loss: 942.1209\n",
      "\n",
      "Epoch 228/1000\n",
      "----------\n",
      "train Loss: 829.4668\n",
      "val Loss: 899.4699\n",
      "\n",
      "Epoch 229/1000\n",
      "----------\n",
      "train Loss: 835.5206\n",
      "val Loss: 1240.1892\n",
      "\n",
      "Epoch 230/1000\n",
      "----------\n",
      "train Loss: 851.5271\n",
      "val Loss: 864.6265\n",
      "\n",
      "Epoch 231/1000\n",
      "----------\n",
      "train Loss: 793.2587\n",
      "val Loss: 978.4742\n",
      "\n",
      "Epoch 232/1000\n",
      "----------\n",
      "train Loss: 803.7105\n",
      "val Loss: 1548.1124\n",
      "\n",
      "Epoch 233/1000\n",
      "----------\n",
      "train Loss: 816.5376\n",
      "val Loss: 942.7715\n",
      "\n",
      "Epoch 234/1000\n",
      "----------\n",
      "train Loss: 800.5057\n",
      "val Loss: 1155.4192\n",
      "\n",
      "Epoch 235/1000\n",
      "----------\n",
      "train Loss: 808.7822\n",
      "val Loss: 1126.5775\n",
      "\n",
      "Epoch 236/1000\n",
      "----------\n",
      "train Loss: 818.5559\n",
      "val Loss: 868.0347\n",
      "\n",
      "Epoch 237/1000\n",
      "----------\n",
      "train Loss: 805.2232\n",
      "val Loss: 1333.1721\n",
      "\n",
      "Epoch 238/1000\n",
      "----------\n",
      "train Loss: 798.4196\n",
      "val Loss: 889.5807\n",
      "\n",
      "Epoch 239/1000\n",
      "----------\n",
      "train Loss: 805.5421\n",
      "val Loss: 1325.8409\n",
      "\n",
      "Epoch 240/1000\n",
      "----------\n",
      "train Loss: 805.3068\n",
      "val Loss: 1460.8637\n",
      "\n",
      "Epoch 241/1000\n",
      "----------\n",
      "train Loss: 804.9595\n",
      "val Loss: 994.2242\n",
      "\n",
      "Epoch 242/1000\n",
      "----------\n",
      "train Loss: 799.5567\n",
      "val Loss: 895.8502\n",
      "\n",
      "Epoch 243/1000\n",
      "----------\n",
      "train Loss: 802.2937\n",
      "val Loss: 956.3872\n",
      "\n",
      "Epoch 244/1000\n",
      "----------\n",
      "train Loss: 814.1291\n",
      "val Loss: 868.3271\n",
      "\n",
      "Epoch 245/1000\n",
      "----------\n",
      "train Loss: 801.3442\n",
      "val Loss: 993.3428\n",
      "\n",
      "Epoch 246/1000\n",
      "----------\n",
      "train Loss: 851.1215\n",
      "val Loss: 925.2527\n",
      "\n",
      "Epoch 247/1000\n",
      "----------\n",
      "train Loss: 815.0501\n",
      "val Loss: 949.4586\n",
      "\n",
      "Epoch 248/1000\n",
      "----------\n",
      "train Loss: 818.1956\n",
      "val Loss: 1060.1120\n",
      "\n",
      "Epoch 249/1000\n",
      "----------\n",
      "train Loss: 812.1641\n",
      "val Loss: 895.6975\n",
      "\n",
      "Epoch 250/1000\n",
      "----------\n",
      "train Loss: 819.7999\n",
      "val Loss: 1119.0168\n",
      "\n",
      "Epoch 251/1000\n",
      "----------\n",
      "train Loss: 809.5345\n",
      "val Loss: 865.5846\n",
      "\n",
      "Epoch 252/1000\n",
      "----------\n",
      "train Loss: 836.6281\n",
      "val Loss: 889.1788\n",
      "\n",
      "Epoch 253/1000\n",
      "----------\n",
      "train Loss: 831.5602\n",
      "val Loss: 1145.4567\n",
      "\n",
      "Epoch 254/1000\n",
      "----------\n",
      "train Loss: 811.6210\n",
      "val Loss: 873.0986\n",
      "\n",
      "Epoch 255/1000\n",
      "----------\n",
      "train Loss: 800.6153\n",
      "val Loss: 907.2861\n",
      "\n",
      "Epoch 256/1000\n",
      "----------\n",
      "train Loss: 804.6434\n",
      "val Loss: 873.5299\n",
      "\n",
      "Epoch 257/1000\n",
      "----------\n",
      "train Loss: 795.4854\n",
      "val Loss: 919.9619\n",
      "\n",
      "Epoch 258/1000\n",
      "----------\n",
      "train Loss: 790.2754\n",
      "val Loss: 867.4163\n",
      "\n",
      "Epoch 259/1000\n",
      "----------\n",
      "train Loss: 803.6112\n",
      "val Loss: 1573.0318\n",
      "\n",
      "Epoch 260/1000\n",
      "----------\n",
      "train Loss: 826.2068\n",
      "val Loss: 882.3432\n",
      "\n",
      "Epoch 261/1000\n",
      "----------\n",
      "train Loss: 798.4186\n",
      "val Loss: 1055.9318\n",
      "\n",
      "Epoch 262/1000\n",
      "----------\n",
      "train Loss: 863.6686\n",
      "val Loss: 951.9972\n",
      "\n",
      "Epoch 263/1000\n",
      "----------\n",
      "train Loss: 867.7500\n",
      "val Loss: 861.0468\n",
      "\n",
      "Epoch 264/1000\n",
      "----------\n",
      "train Loss: 794.9003\n",
      "val Loss: 898.7496\n",
      "\n",
      "Epoch 265/1000\n",
      "----------\n",
      "train Loss: 796.3437\n",
      "val Loss: 932.2092\n",
      "\n",
      "Epoch 266/1000\n",
      "----------\n",
      "train Loss: 802.8360\n",
      "val Loss: 1323.5189\n",
      "\n",
      "Epoch 267/1000\n",
      "----------\n",
      "train Loss: 816.4228\n",
      "val Loss: 909.9412\n",
      "\n",
      "Epoch 268/1000\n",
      "----------\n",
      "train Loss: 796.2727\n",
      "val Loss: 998.1672\n",
      "\n",
      "Epoch 269/1000\n",
      "----------\n",
      "train Loss: 817.6846\n",
      "val Loss: 891.8800\n",
      "\n",
      "Epoch 270/1000\n",
      "----------\n",
      "train Loss: 802.4907\n",
      "val Loss: 1025.7640\n",
      "\n",
      "Epoch 271/1000\n",
      "----------\n",
      "train Loss: 818.0611\n",
      "val Loss: 846.5064\n",
      "\n",
      "Epoch 272/1000\n",
      "----------\n",
      "train Loss: 793.9658\n",
      "val Loss: 935.3551\n",
      "\n",
      "Epoch 273/1000\n",
      "----------\n",
      "train Loss: 833.7990\n",
      "val Loss: 1437.9156\n",
      "\n",
      "Epoch 274/1000\n",
      "----------\n",
      "train Loss: 816.4896\n",
      "val Loss: 1004.8662\n",
      "\n",
      "Epoch 275/1000\n",
      "----------\n",
      "train Loss: 820.2528\n",
      "val Loss: 920.0418\n",
      "\n",
      "Epoch 276/1000\n",
      "----------\n",
      "train Loss: 791.3186\n",
      "val Loss: 1090.8284\n",
      "\n",
      "Epoch 277/1000\n",
      "----------\n",
      "train Loss: 805.4567\n",
      "val Loss: 868.5439\n",
      "\n",
      "Epoch 278/1000\n",
      "----------\n",
      "train Loss: 808.3173\n",
      "val Loss: 1110.0110\n",
      "\n",
      "Epoch 279/1000\n",
      "----------\n",
      "train Loss: 811.7996\n",
      "val Loss: 1054.2906\n",
      "\n",
      "Epoch 280/1000\n",
      "----------\n",
      "train Loss: 809.9886\n",
      "val Loss: 1319.8934\n",
      "\n",
      "Epoch 281/1000\n",
      "----------\n",
      "train Loss: 806.7242\n",
      "val Loss: 897.0463\n",
      "\n",
      "Epoch 282/1000\n",
      "----------\n",
      "train Loss: 817.5907\n",
      "val Loss: 901.7826\n",
      "\n",
      "Epoch 283/1000\n",
      "----------\n",
      "train Loss: 808.8329\n",
      "val Loss: 1719.2750\n",
      "\n",
      "Epoch 284/1000\n",
      "----------\n",
      "train Loss: 824.4690\n",
      "val Loss: 907.4770\n",
      "\n",
      "Epoch 285/1000\n",
      "----------\n",
      "train Loss: 796.1812\n",
      "val Loss: 861.8829\n",
      "\n",
      "Epoch 286/1000\n",
      "----------\n",
      "train Loss: 813.1495\n",
      "val Loss: 1008.9074\n",
      "\n",
      "Epoch 287/1000\n",
      "----------\n",
      "train Loss: 872.4198\n",
      "val Loss: 1073.5601\n",
      "\n",
      "Epoch 288/1000\n",
      "----------\n",
      "train Loss: 820.0418\n",
      "val Loss: 971.5871\n",
      "\n",
      "Epoch 289/1000\n",
      "----------\n",
      "train Loss: 819.6067\n",
      "val Loss: 1457.1143\n",
      "\n",
      "Epoch 290/1000\n",
      "----------\n",
      "train Loss: 815.8951\n",
      "val Loss: 1533.9912\n",
      "\n",
      "Epoch 291/1000\n",
      "----------\n",
      "train Loss: 791.8942\n",
      "val Loss: 860.8185\n",
      "\n",
      "Epoch 292/1000\n",
      "----------\n",
      "train Loss: 793.9688\n",
      "val Loss: 988.2877\n",
      "\n",
      "Epoch 293/1000\n",
      "----------\n",
      "train Loss: 821.2689\n",
      "val Loss: 1029.9773\n",
      "\n",
      "Epoch 294/1000\n",
      "----------\n",
      "train Loss: 835.1990\n",
      "val Loss: 872.1383\n",
      "\n",
      "Epoch 295/1000\n",
      "----------\n",
      "train Loss: 814.3333\n",
      "val Loss: 1554.1173\n",
      "\n",
      "Epoch 296/1000\n",
      "----------\n",
      "train Loss: 865.7986\n",
      "val Loss: 976.5322\n",
      "\n",
      "Epoch 297/1000\n",
      "----------\n",
      "train Loss: 872.4095\n",
      "val Loss: 867.7888\n",
      "\n",
      "Epoch 298/1000\n",
      "----------\n",
      "train Loss: 831.3818\n",
      "val Loss: 1417.5231\n",
      "\n",
      "Epoch 299/1000\n",
      "----------\n",
      "train Loss: 846.3369\n",
      "val Loss: 930.7064\n",
      "\n",
      "Epoch 300/1000\n",
      "----------\n",
      "train Loss: 830.7247\n",
      "val Loss: 1021.0268\n",
      "\n",
      "Epoch 301/1000\n",
      "----------\n",
      "train Loss: 844.6830\n",
      "val Loss: 872.4936\n",
      "\n",
      "Epoch 302/1000\n",
      "----------\n",
      "train Loss: 795.3620\n",
      "val Loss: 871.1231\n",
      "\n",
      "Epoch 303/1000\n",
      "----------\n",
      "train Loss: 825.1992\n",
      "val Loss: 1081.6708\n",
      "\n",
      "Epoch 304/1000\n",
      "----------\n",
      "train Loss: 825.8088\n",
      "val Loss: 889.1874\n",
      "\n",
      "Epoch 305/1000\n",
      "----------\n",
      "train Loss: 847.5382\n",
      "val Loss: 961.2361\n",
      "\n",
      "Epoch 306/1000\n",
      "----------\n",
      "train Loss: 795.3216\n",
      "val Loss: 863.1906\n",
      "\n",
      "Epoch 307/1000\n",
      "----------\n",
      "train Loss: 788.1453\n",
      "val Loss: 862.3142\n",
      "\n",
      "Epoch 308/1000\n",
      "----------\n",
      "train Loss: 797.1744\n",
      "val Loss: 892.2568\n",
      "\n",
      "Epoch 309/1000\n",
      "----------\n",
      "train Loss: 836.9537\n",
      "val Loss: 1006.9310\n",
      "\n",
      "Epoch 310/1000\n",
      "----------\n",
      "train Loss: 838.6248\n",
      "val Loss: 1032.7085\n",
      "\n",
      "Epoch 311/1000\n",
      "----------\n",
      "train Loss: 805.6587\n",
      "val Loss: 868.9824\n",
      "\n",
      "Epoch 312/1000\n",
      "----------\n",
      "train Loss: 797.0791\n",
      "val Loss: 945.4051\n",
      "\n",
      "Epoch 313/1000\n",
      "----------\n",
      "train Loss: 799.4210\n",
      "val Loss: 1104.4262\n",
      "\n",
      "Epoch 314/1000\n",
      "----------\n",
      "train Loss: 870.3267\n",
      "val Loss: 861.8847\n",
      "\n",
      "Epoch 315/1000\n",
      "----------\n",
      "train Loss: 815.8193\n",
      "val Loss: 884.6988\n",
      "\n",
      "Epoch 316/1000\n",
      "----------\n",
      "train Loss: 798.4203\n",
      "val Loss: 1929.3697\n",
      "\n",
      "Epoch 317/1000\n",
      "----------\n",
      "train Loss: 805.1585\n",
      "val Loss: 1013.5103\n",
      "\n",
      "Epoch 318/1000\n",
      "----------\n",
      "train Loss: 809.0552\n",
      "val Loss: 885.6271\n",
      "\n",
      "Epoch 319/1000\n",
      "----------\n",
      "train Loss: 825.6334\n",
      "val Loss: 1631.6724\n",
      "\n",
      "Epoch 320/1000\n",
      "----------\n",
      "train Loss: 810.5600\n",
      "val Loss: 900.6305\n",
      "\n",
      "Epoch 321/1000\n",
      "----------\n",
      "train Loss: 812.8236\n",
      "val Loss: 902.9205\n",
      "\n",
      "Epoch 322/1000\n",
      "----------\n",
      "train Loss: 831.3068\n",
      "val Loss: 1003.4659\n",
      "\n",
      "Epoch 323/1000\n",
      "----------\n",
      "train Loss: 812.9026\n",
      "val Loss: 900.4639\n",
      "\n",
      "Epoch 324/1000\n",
      "----------\n",
      "train Loss: 807.7920\n",
      "val Loss: 975.0632\n",
      "\n",
      "Epoch 325/1000\n",
      "----------\n",
      "train Loss: 833.3356\n",
      "val Loss: 892.8572\n",
      "\n",
      "Epoch 326/1000\n",
      "----------\n",
      "train Loss: 802.9504\n",
      "val Loss: 918.2487\n",
      "\n",
      "Epoch 327/1000\n",
      "----------\n",
      "train Loss: 807.6405\n",
      "val Loss: 856.3528\n",
      "\n",
      "Epoch 328/1000\n",
      "----------\n",
      "train Loss: 807.6414\n",
      "val Loss: 875.4077\n",
      "\n",
      "Epoch 329/1000\n",
      "----------\n",
      "train Loss: 803.7767\n",
      "val Loss: 935.8861\n",
      "\n",
      "Epoch 330/1000\n",
      "----------\n",
      "train Loss: 810.6806\n",
      "val Loss: 1064.8704\n",
      "\n",
      "Epoch 331/1000\n",
      "----------\n",
      "train Loss: 818.1099\n",
      "val Loss: 1024.8087\n",
      "\n",
      "Epoch 332/1000\n",
      "----------\n",
      "train Loss: 810.0948\n",
      "val Loss: 1377.5930\n",
      "\n",
      "Epoch 333/1000\n",
      "----------\n",
      "train Loss: 811.1246\n",
      "val Loss: 1145.3943\n",
      "\n",
      "Epoch 334/1000\n",
      "----------\n",
      "train Loss: 816.2764\n",
      "val Loss: 1219.2174\n",
      "\n",
      "Epoch 335/1000\n",
      "----------\n",
      "train Loss: 820.1162\n",
      "val Loss: 865.2011\n",
      "\n",
      "Epoch 336/1000\n",
      "----------\n",
      "train Loss: 797.9875\n",
      "val Loss: 931.4090\n",
      "\n",
      "Epoch 337/1000\n",
      "----------\n",
      "train Loss: 804.7922\n",
      "val Loss: 956.3565\n",
      "\n",
      "Epoch 338/1000\n",
      "----------\n",
      "train Loss: 829.3549\n",
      "val Loss: 1788.2154\n",
      "\n",
      "Epoch 339/1000\n",
      "----------\n",
      "train Loss: 826.7628\n",
      "val Loss: 1038.6882\n",
      "\n",
      "Epoch 340/1000\n",
      "----------\n",
      "train Loss: 803.0384\n",
      "val Loss: 964.8868\n",
      "\n",
      "Epoch 341/1000\n",
      "----------\n",
      "train Loss: 801.5880\n",
      "val Loss: 1381.0653\n",
      "\n",
      "Epoch 342/1000\n",
      "----------\n",
      "train Loss: 856.7406\n",
      "val Loss: 1279.8209\n",
      "\n",
      "Epoch 343/1000\n",
      "----------\n",
      "train Loss: 815.1768\n",
      "val Loss: 1167.7028\n",
      "\n",
      "Epoch 344/1000\n",
      "----------\n",
      "train Loss: 792.9175\n",
      "val Loss: 913.6904\n",
      "\n",
      "Epoch 345/1000\n",
      "----------\n",
      "train Loss: 890.0491\n",
      "val Loss: 877.3238\n",
      "\n",
      "Epoch 346/1000\n",
      "----------\n",
      "train Loss: 838.8147\n",
      "val Loss: 923.1263\n",
      "\n",
      "Epoch 347/1000\n",
      "----------\n",
      "train Loss: 828.9199\n",
      "val Loss: 865.3806\n",
      "\n",
      "Epoch 348/1000\n",
      "----------\n",
      "train Loss: 844.0042\n",
      "val Loss: 1003.7690\n",
      "\n",
      "Epoch 349/1000\n",
      "----------\n",
      "train Loss: 806.6091\n",
      "val Loss: 897.5783\n",
      "\n",
      "Epoch 350/1000\n",
      "----------\n",
      "train Loss: 785.9944\n",
      "val Loss: 880.1781\n",
      "\n",
      "Epoch 351/1000\n",
      "----------\n",
      "train Loss: 789.6561\n",
      "val Loss: 876.1568\n",
      "\n",
      "Epoch 352/1000\n",
      "----------\n",
      "train Loss: 835.1715\n",
      "val Loss: 1471.7613\n",
      "\n",
      "Epoch 353/1000\n",
      "----------\n",
      "train Loss: 816.2782\n",
      "val Loss: 973.3582\n",
      "\n",
      "Epoch 354/1000\n",
      "----------\n",
      "train Loss: 789.5861\n",
      "val Loss: 1018.5529\n",
      "\n",
      "Epoch 355/1000\n",
      "----------\n",
      "train Loss: 810.1210\n",
      "val Loss: 1164.1826\n",
      "\n",
      "Epoch 356/1000\n",
      "----------\n",
      "train Loss: 801.8746\n",
      "val Loss: 1007.9118\n",
      "\n",
      "Epoch 357/1000\n",
      "----------\n",
      "train Loss: 786.3267\n",
      "val Loss: 947.5329\n",
      "\n",
      "Epoch 358/1000\n",
      "----------\n",
      "train Loss: 797.2175\n",
      "val Loss: 954.6491\n",
      "\n",
      "Epoch 359/1000\n",
      "----------\n",
      "train Loss: 803.0450\n",
      "val Loss: 935.4728\n",
      "\n",
      "Epoch 360/1000\n",
      "----------\n",
      "train Loss: 795.9549\n",
      "val Loss: 906.5412\n",
      "\n",
      "Epoch 361/1000\n",
      "----------\n",
      "train Loss: 729.0270\n",
      "val Loss: 738.6716\n",
      "\n",
      "Epoch 362/1000\n",
      "----------\n",
      "train Loss: 595.4941\n",
      "val Loss: 493.3060\n",
      "\n",
      "Epoch 363/1000\n",
      "----------\n",
      "train Loss: 371.4713\n",
      "val Loss: 354.8718\n",
      "\n",
      "Epoch 364/1000\n",
      "----------\n",
      "train Loss: 299.3353\n",
      "val Loss: 260.0841\n",
      "\n",
      "Epoch 365/1000\n",
      "----------\n",
      "train Loss: 265.4629\n",
      "val Loss: 3197.6512\n",
      "\n",
      "Epoch 366/1000\n",
      "----------\n",
      "train Loss: 200.4498\n",
      "val Loss: 401.2546\n",
      "\n",
      "Epoch 367/1000\n",
      "----------\n",
      "train Loss: 180.5359\n",
      "val Loss: 235.5229\n",
      "\n",
      "Epoch 368/1000\n",
      "----------\n",
      "train Loss: 174.9892\n",
      "val Loss: 699.9688\n",
      "\n",
      "Epoch 369/1000\n",
      "----------\n",
      "train Loss: 135.9962\n",
      "val Loss: 758.8804\n",
      "\n",
      "Epoch 370/1000\n",
      "----------\n",
      "train Loss: 127.6265\n",
      "val Loss: 3461.3264\n",
      "\n",
      "Epoch 371/1000\n",
      "----------\n",
      "train Loss: 107.3370\n",
      "val Loss: 250.1648\n",
      "\n",
      "Epoch 372/1000\n",
      "----------\n",
      "train Loss: 68.0546\n",
      "val Loss: 1770.8090\n",
      "\n",
      "Epoch 373/1000\n",
      "----------\n",
      "train Loss: 148.1629\n",
      "val Loss: 131.8662\n",
      "\n",
      "Epoch 374/1000\n",
      "----------\n",
      "train Loss: 139.8597\n",
      "val Loss: 39.1955\n",
      "\n",
      "Epoch 375/1000\n",
      "----------\n",
      "train Loss: 43.5776\n",
      "val Loss: 38.9201\n",
      "\n",
      "Epoch 376/1000\n",
      "----------\n",
      "train Loss: 119.2751\n",
      "val Loss: 306.6403\n",
      "\n",
      "Epoch 377/1000\n",
      "----------\n",
      "train Loss: 73.9410\n",
      "val Loss: 112.5534\n",
      "\n",
      "Epoch 378/1000\n",
      "----------\n",
      "train Loss: 25.8913\n",
      "val Loss: 41.0245\n",
      "\n",
      "Epoch 379/1000\n",
      "----------\n",
      "train Loss: 35.9630\n",
      "val Loss: 32.9095\n",
      "\n",
      "Epoch 380/1000\n",
      "----------\n",
      "train Loss: 21.2987\n",
      "val Loss: 20.5926\n",
      "\n",
      "Epoch 381/1000\n",
      "----------\n",
      "train Loss: 22.4026\n",
      "val Loss: 72.7624\n",
      "\n",
      "Epoch 382/1000\n",
      "----------\n",
      "train Loss: 21.2671\n",
      "val Loss: 24.4183\n",
      "\n",
      "Epoch 383/1000\n",
      "----------\n",
      "train Loss: 59.8385\n",
      "val Loss: 21.5028\n",
      "\n",
      "Epoch 384/1000\n",
      "----------\n",
      "train Loss: 28.5457\n",
      "val Loss: 172.6730\n",
      "\n",
      "Epoch 385/1000\n",
      "----------\n",
      "train Loss: 31.0740\n",
      "val Loss: 444.9940\n",
      "\n",
      "Epoch 386/1000\n",
      "----------\n",
      "train Loss: 27.0078\n",
      "val Loss: 376.7846\n",
      "\n",
      "Epoch 387/1000\n",
      "----------\n",
      "train Loss: 35.6795\n",
      "val Loss: 945.8098\n",
      "\n",
      "Epoch 388/1000\n",
      "----------\n",
      "train Loss: 23.8972\n",
      "val Loss: 55.1094\n",
      "\n",
      "Epoch 389/1000\n",
      "----------\n",
      "train Loss: 26.4607\n",
      "val Loss: 334.1214\n",
      "\n",
      "Epoch 390/1000\n",
      "----------\n",
      "train Loss: 28.8702\n",
      "val Loss: 238.7144\n",
      "\n",
      "Epoch 391/1000\n",
      "----------\n",
      "train Loss: 66.4170\n",
      "val Loss: 87.5193\n",
      "\n",
      "Epoch 392/1000\n",
      "----------\n",
      "train Loss: 19.9487\n",
      "val Loss: 51.7890\n",
      "\n",
      "Epoch 393/1000\n",
      "----------\n",
      "train Loss: 36.7246\n",
      "val Loss: 47.3277\n",
      "\n",
      "Epoch 394/1000\n",
      "----------\n",
      "train Loss: 55.0072\n",
      "val Loss: 126.7457\n",
      "\n",
      "Epoch 395/1000\n",
      "----------\n",
      "train Loss: 26.6036\n",
      "val Loss: 376.3149\n",
      "\n",
      "Epoch 396/1000\n",
      "----------\n",
      "train Loss: 107.8715\n",
      "val Loss: 224.3988\n",
      "\n",
      "Epoch 397/1000\n",
      "----------\n",
      "train Loss: 123.5248\n",
      "val Loss: 440.7098\n",
      "\n",
      "Epoch 398/1000\n",
      "----------\n",
      "train Loss: 26.6139\n",
      "val Loss: 6.3102\n",
      "\n",
      "Epoch 399/1000\n",
      "----------\n",
      "train Loss: 41.6189\n",
      "val Loss: 8.0682\n",
      "\n",
      "Epoch 400/1000\n",
      "----------\n",
      "train Loss: 31.5132\n",
      "val Loss: 9.0232\n",
      "\n",
      "Epoch 401/1000\n",
      "----------\n",
      "train Loss: 34.1363\n",
      "val Loss: 192.6179\n",
      "\n",
      "Epoch 402/1000\n",
      "----------\n",
      "train Loss: 20.1133\n",
      "val Loss: 5.1127\n",
      "\n",
      "Epoch 403/1000\n",
      "----------\n",
      "train Loss: 19.7245\n",
      "val Loss: 109.9538\n",
      "\n",
      "Epoch 404/1000\n",
      "----------\n",
      "train Loss: 27.0481\n",
      "val Loss: 296.2224\n",
      "\n",
      "Epoch 405/1000\n",
      "----------\n",
      "train Loss: 69.3375\n",
      "val Loss: 24.5045\n",
      "\n",
      "Epoch 406/1000\n",
      "----------\n",
      "train Loss: 29.0709\n",
      "val Loss: 27.8040\n",
      "\n",
      "Epoch 407/1000\n",
      "----------\n",
      "train Loss: 25.3486\n",
      "val Loss: 262.5855\n",
      "\n",
      "Epoch 408/1000\n",
      "----------\n",
      "train Loss: 62.2730\n",
      "val Loss: 133.2656\n",
      "\n",
      "Epoch 409/1000\n",
      "----------\n",
      "train Loss: 22.8864\n",
      "val Loss: 37.8383\n",
      "\n",
      "Epoch 410/1000\n",
      "----------\n",
      "train Loss: 24.4472\n",
      "val Loss: 19.1881\n",
      "\n",
      "Epoch 411/1000\n",
      "----------\n",
      "train Loss: 46.7570\n",
      "val Loss: 38.0928\n",
      "\n",
      "Epoch 412/1000\n",
      "----------\n",
      "train Loss: 81.0321\n",
      "val Loss: 238.6771\n",
      "\n",
      "Epoch 413/1000\n",
      "----------\n",
      "train Loss: 120.8402\n",
      "val Loss: 65.1061\n",
      "\n",
      "Epoch 414/1000\n",
      "----------\n",
      "train Loss: 54.8352\n",
      "val Loss: 5.6976\n",
      "\n",
      "Epoch 415/1000\n",
      "----------\n",
      "train Loss: 11.1617\n",
      "val Loss: 4.3214\n",
      "\n",
      "Epoch 416/1000\n",
      "----------\n",
      "train Loss: 14.1410\n",
      "val Loss: 19.5819\n",
      "\n",
      "Epoch 417/1000\n",
      "----------\n",
      "train Loss: 21.7064\n",
      "val Loss: 11.1259\n",
      "\n",
      "Epoch 418/1000\n",
      "----------\n",
      "train Loss: 19.4196\n",
      "val Loss: 62.6726\n",
      "\n",
      "Epoch 419/1000\n",
      "----------\n",
      "train Loss: 29.9544\n",
      "val Loss: 9.8031\n",
      "\n",
      "Epoch 420/1000\n",
      "----------\n",
      "train Loss: 26.5327\n",
      "val Loss: 8.3018\n",
      "\n",
      "Epoch 421/1000\n",
      "----------\n",
      "train Loss: 18.3349\n",
      "val Loss: 125.0650\n",
      "\n",
      "Epoch 422/1000\n",
      "----------\n",
      "train Loss: 29.9862\n",
      "val Loss: 42.6119\n",
      "\n",
      "Epoch 423/1000\n",
      "----------\n",
      "train Loss: 15.6289\n",
      "val Loss: 256.7834\n",
      "\n",
      "Epoch 424/1000\n",
      "----------\n",
      "train Loss: 57.7077\n",
      "val Loss: 169.0768\n",
      "\n",
      "Epoch 425/1000\n",
      "----------\n",
      "train Loss: 26.9551\n",
      "val Loss: 26.8355\n",
      "\n",
      "Epoch 426/1000\n",
      "----------\n",
      "train Loss: 40.1748\n",
      "val Loss: 365.7810\n",
      "\n",
      "Epoch 427/1000\n",
      "----------\n",
      "train Loss: 69.9336\n",
      "val Loss: 23.5218\n",
      "\n",
      "Epoch 428/1000\n",
      "----------\n",
      "train Loss: 65.6671\n",
      "val Loss: 94.7011\n",
      "\n",
      "Epoch 429/1000\n",
      "----------\n",
      "train Loss: 68.4029\n",
      "val Loss: 25.0055\n",
      "\n",
      "Epoch 430/1000\n",
      "----------\n",
      "train Loss: 41.7810\n",
      "val Loss: 29.3577\n",
      "\n",
      "Epoch 431/1000\n",
      "----------\n",
      "train Loss: 34.1727\n",
      "val Loss: 79.1030\n",
      "\n",
      "Epoch 432/1000\n",
      "----------\n",
      "train Loss: 34.3353\n",
      "val Loss: 17.4632\n",
      "\n",
      "Epoch 433/1000\n",
      "----------\n",
      "train Loss: 48.7384\n",
      "val Loss: 15.2987\n",
      "\n",
      "Epoch 434/1000\n",
      "----------\n",
      "train Loss: 21.7543\n",
      "val Loss: 155.1146\n",
      "\n",
      "Epoch 435/1000\n",
      "----------\n",
      "train Loss: 56.6835\n",
      "val Loss: 13.6089\n",
      "\n",
      "Epoch 436/1000\n",
      "----------\n",
      "train Loss: 59.6910\n",
      "val Loss: 930.9467\n",
      "\n",
      "Epoch 437/1000\n",
      "----------\n",
      "train Loss: 17.5186\n",
      "val Loss: 15.9911\n",
      "\n",
      "Epoch 438/1000\n",
      "----------\n",
      "train Loss: 31.0618\n",
      "val Loss: 95.5142\n",
      "\n",
      "Epoch 439/1000\n",
      "----------\n",
      "train Loss: 25.1158\n",
      "val Loss: 46.3908\n",
      "\n",
      "Epoch 440/1000\n",
      "----------\n",
      "train Loss: 26.8114\n",
      "val Loss: 31.3334\n",
      "\n",
      "Epoch 441/1000\n",
      "----------\n",
      "train Loss: 35.1924\n",
      "val Loss: 113.1974\n",
      "\n",
      "Epoch 442/1000\n",
      "----------\n",
      "train Loss: 31.1255\n",
      "val Loss: 24.5201\n",
      "\n",
      "Epoch 443/1000\n",
      "----------\n",
      "train Loss: 55.5875\n",
      "val Loss: 285.7947\n",
      "\n",
      "Epoch 444/1000\n",
      "----------\n",
      "train Loss: 18.4782\n",
      "val Loss: 105.8649\n",
      "\n",
      "Epoch 445/1000\n",
      "----------\n",
      "train Loss: 14.7651\n",
      "val Loss: 169.1893\n",
      "\n",
      "Epoch 446/1000\n",
      "----------\n",
      "train Loss: 61.7975\n",
      "val Loss: 289.7193\n",
      "\n",
      "Epoch 447/1000\n",
      "----------\n",
      "train Loss: 77.6638\n",
      "val Loss: 758.1809\n",
      "\n",
      "Epoch 448/1000\n",
      "----------\n",
      "train Loss: 37.5534\n",
      "val Loss: 28.3932\n",
      "\n",
      "Epoch 449/1000\n",
      "----------\n",
      "train Loss: 38.3654\n",
      "val Loss: 52.1701\n",
      "\n",
      "Epoch 450/1000\n",
      "----------\n",
      "train Loss: 34.4712\n",
      "val Loss: 19.4692\n",
      "\n",
      "Epoch 451/1000\n",
      "----------\n",
      "train Loss: 19.4809\n",
      "val Loss: 268.2028\n",
      "\n",
      "Epoch 452/1000\n",
      "----------\n",
      "train Loss: 12.1825\n",
      "val Loss: 78.6347\n",
      "\n",
      "Epoch 453/1000\n",
      "----------\n",
      "train Loss: 19.2975\n",
      "val Loss: 62.8948\n",
      "\n",
      "Epoch 454/1000\n",
      "----------\n",
      "train Loss: 26.9930\n",
      "val Loss: 26.9160\n",
      "\n",
      "Epoch 455/1000\n",
      "----------\n",
      "train Loss: 21.8725\n",
      "val Loss: 69.0240\n",
      "\n",
      "Epoch 456/1000\n",
      "----------\n",
      "train Loss: 44.0206\n",
      "val Loss: 20.7293\n",
      "\n",
      "Epoch 457/1000\n",
      "----------\n",
      "train Loss: 19.5988\n",
      "val Loss: 50.0390\n",
      "\n",
      "Epoch 458/1000\n",
      "----------\n",
      "train Loss: 18.0693\n",
      "val Loss: 143.9800\n",
      "\n",
      "Epoch 459/1000\n",
      "----------\n",
      "train Loss: 14.0019\n",
      "val Loss: 14.2019\n",
      "\n",
      "Epoch 460/1000\n",
      "----------\n",
      "train Loss: 41.6966\n",
      "val Loss: 40.2970\n",
      "\n",
      "Epoch 461/1000\n",
      "----------\n",
      "train Loss: 36.9705\n",
      "val Loss: 437.3108\n",
      "\n",
      "Epoch 462/1000\n",
      "----------\n",
      "train Loss: 38.2709\n",
      "val Loss: 63.0541\n",
      "\n",
      "Epoch 463/1000\n",
      "----------\n",
      "train Loss: 77.2485\n",
      "val Loss: 602.3935\n",
      "\n",
      "Epoch 464/1000\n",
      "----------\n",
      "train Loss: 55.1889\n",
      "val Loss: 117.6131\n",
      "\n",
      "Epoch 465/1000\n",
      "----------\n",
      "train Loss: 23.7237\n",
      "val Loss: 48.7774\n",
      "\n",
      "Epoch 466/1000\n",
      "----------\n",
      "train Loss: 9.8915\n",
      "val Loss: 96.6714\n",
      "\n",
      "Epoch 467/1000\n",
      "----------\n",
      "train Loss: 28.8205\n",
      "val Loss: 5.0236\n",
      "\n",
      "Epoch 468/1000\n",
      "----------\n",
      "train Loss: 18.9816\n",
      "val Loss: 16.6815\n",
      "\n",
      "Epoch 469/1000\n",
      "----------\n",
      "train Loss: 18.7871\n",
      "val Loss: 41.8315\n",
      "\n",
      "Epoch 470/1000\n",
      "----------\n",
      "train Loss: 33.7371\n",
      "val Loss: 31.0895\n",
      "\n",
      "Epoch 471/1000\n",
      "----------\n",
      "train Loss: 38.7875\n",
      "val Loss: 6.6630\n",
      "\n",
      "Epoch 472/1000\n",
      "----------\n",
      "train Loss: 57.7964\n",
      "val Loss: 46.0973\n",
      "\n",
      "Epoch 473/1000\n",
      "----------\n",
      "train Loss: 50.9469\n",
      "val Loss: 188.3253\n",
      "\n",
      "Epoch 474/1000\n",
      "----------\n",
      "train Loss: 49.6087\n",
      "val Loss: 36.7805\n",
      "\n",
      "Epoch 475/1000\n",
      "----------\n",
      "train Loss: 18.6949\n",
      "val Loss: 19.1273\n",
      "\n",
      "Epoch 476/1000\n",
      "----------\n",
      "train Loss: 16.8432\n",
      "val Loss: 27.1282\n",
      "\n",
      "Epoch 477/1000\n",
      "----------\n",
      "train Loss: 73.2071\n",
      "val Loss: 71.8303\n",
      "\n",
      "Epoch 478/1000\n",
      "----------\n",
      "train Loss: 30.8772\n",
      "val Loss: 21.6715\n",
      "\n",
      "Epoch 479/1000\n",
      "----------\n",
      "train Loss: 45.5805\n",
      "val Loss: 207.8445\n",
      "\n",
      "Epoch 480/1000\n",
      "----------\n",
      "train Loss: 10.5591\n",
      "val Loss: 6.1198\n",
      "\n",
      "Epoch 481/1000\n",
      "----------\n",
      "train Loss: 25.3343\n",
      "val Loss: 181.3862\n",
      "\n",
      "Epoch 482/1000\n",
      "----------\n",
      "train Loss: 66.2587\n",
      "val Loss: 55.6000\n",
      "\n",
      "Epoch 483/1000\n",
      "----------\n",
      "train Loss: 14.6927\n",
      "val Loss: 16.4207\n",
      "\n",
      "Epoch 484/1000\n",
      "----------\n",
      "train Loss: 29.3307\n",
      "val Loss: 50.4326\n",
      "\n",
      "Epoch 485/1000\n",
      "----------\n",
      "train Loss: 29.6966\n",
      "val Loss: 7.0411\n",
      "\n",
      "Epoch 486/1000\n",
      "----------\n",
      "train Loss: 15.4420\n",
      "val Loss: 83.3436\n",
      "\n",
      "Epoch 487/1000\n",
      "----------\n",
      "train Loss: 26.6604\n",
      "val Loss: 231.1093\n",
      "\n",
      "Epoch 488/1000\n",
      "----------\n",
      "train Loss: 13.9251\n",
      "val Loss: 16.0954\n",
      "\n",
      "Epoch 489/1000\n",
      "----------\n",
      "train Loss: 24.2696\n",
      "val Loss: 257.3077\n",
      "\n",
      "Epoch 490/1000\n",
      "----------\n",
      "train Loss: 33.4500\n",
      "val Loss: 462.2291\n",
      "\n",
      "Epoch 491/1000\n",
      "----------\n",
      "train Loss: 33.1282\n",
      "val Loss: 139.8100\n",
      "\n",
      "Epoch 492/1000\n",
      "----------\n",
      "train Loss: 35.5527\n",
      "val Loss: 602.2679\n",
      "\n",
      "Epoch 493/1000\n",
      "----------\n",
      "train Loss: 47.6485\n",
      "val Loss: 76.2950\n",
      "\n",
      "Epoch 494/1000\n",
      "----------\n",
      "train Loss: 35.6683\n",
      "val Loss: 115.9044\n",
      "\n",
      "Epoch 495/1000\n",
      "----------\n",
      "train Loss: 32.6482\n",
      "val Loss: 95.8768\n",
      "\n",
      "Epoch 496/1000\n",
      "----------\n",
      "train Loss: 21.6537\n",
      "val Loss: 9.5698\n",
      "\n",
      "Epoch 497/1000\n",
      "----------\n",
      "train Loss: 20.0503\n",
      "val Loss: 121.5203\n",
      "\n",
      "Epoch 498/1000\n",
      "----------\n",
      "train Loss: 49.6634\n",
      "val Loss: 59.6995\n",
      "\n",
      "Epoch 499/1000\n",
      "----------\n",
      "train Loss: 100.8741\n",
      "val Loss: 214.6480\n",
      "\n",
      "Epoch 500/1000\n",
      "----------\n",
      "train Loss: 70.0232\n",
      "val Loss: 28.1639\n",
      "\n",
      "Epoch 501/1000\n",
      "----------\n",
      "train Loss: 44.6404\n",
      "val Loss: 35.9426\n",
      "\n",
      "Epoch 502/1000\n",
      "----------\n",
      "train Loss: 22.3159\n",
      "val Loss: 839.3412\n",
      "\n",
      "Epoch 503/1000\n",
      "----------\n",
      "train Loss: 43.6680\n",
      "val Loss: 49.5727\n",
      "\n",
      "Epoch 504/1000\n",
      "----------\n",
      "train Loss: 10.2496\n",
      "val Loss: 115.8869\n",
      "\n",
      "Epoch 505/1000\n",
      "----------\n",
      "train Loss: 6.1513\n",
      "val Loss: 359.7967\n",
      "\n",
      "Epoch 506/1000\n",
      "----------\n",
      "train Loss: 31.2533\n",
      "val Loss: 416.0948\n",
      "\n",
      "Epoch 507/1000\n",
      "----------\n",
      "train Loss: 47.4789\n",
      "val Loss: 5.3370\n",
      "\n",
      "Epoch 508/1000\n",
      "----------\n",
      "train Loss: 33.7937\n",
      "val Loss: 5.7754\n",
      "\n",
      "Epoch 509/1000\n",
      "----------\n",
      "train Loss: 17.8090\n",
      "val Loss: 67.7162\n",
      "\n",
      "Epoch 510/1000\n",
      "----------\n",
      "train Loss: 40.0709\n",
      "val Loss: 57.5526\n",
      "\n",
      "Epoch 511/1000\n",
      "----------\n",
      "train Loss: 56.0523\n",
      "val Loss: 307.7527\n",
      "\n",
      "Epoch 512/1000\n",
      "----------\n",
      "train Loss: 36.7552\n",
      "val Loss: 211.2629\n",
      "\n",
      "Epoch 513/1000\n",
      "----------\n",
      "train Loss: 22.7774\n",
      "val Loss: 47.1354\n",
      "\n",
      "Epoch 514/1000\n",
      "----------\n",
      "train Loss: 20.3550\n",
      "val Loss: 62.4787\n",
      "\n",
      "Epoch 515/1000\n",
      "----------\n",
      "train Loss: 22.0339\n",
      "val Loss: 138.8124\n",
      "\n",
      "Epoch 516/1000\n",
      "----------\n",
      "train Loss: 19.0751\n",
      "val Loss: 140.1304\n",
      "\n",
      "Epoch 517/1000\n",
      "----------\n",
      "train Loss: 61.0493\n",
      "val Loss: 525.9731\n",
      "\n",
      "Epoch 518/1000\n",
      "----------\n",
      "train Loss: 59.3803\n",
      "val Loss: 220.8761\n",
      "\n",
      "Epoch 519/1000\n",
      "----------\n",
      "train Loss: 39.6418\n",
      "val Loss: 157.7992\n",
      "\n",
      "Epoch 520/1000\n",
      "----------\n",
      "train Loss: 30.4734\n",
      "val Loss: 56.5112\n",
      "\n",
      "Epoch 521/1000\n",
      "----------\n",
      "train Loss: 19.4692\n",
      "val Loss: 67.1457\n",
      "\n",
      "Epoch 522/1000\n",
      "----------\n",
      "train Loss: 61.1955\n",
      "val Loss: 90.6016\n",
      "\n",
      "Epoch 523/1000\n",
      "----------\n",
      "train Loss: 76.0697\n",
      "val Loss: 504.4688\n",
      "\n",
      "Epoch 524/1000\n",
      "----------\n",
      "train Loss: 29.2844\n",
      "val Loss: 37.0303\n",
      "\n",
      "Epoch 525/1000\n",
      "----------\n",
      "train Loss: 58.2393\n",
      "val Loss: 107.4172\n",
      "\n",
      "Epoch 526/1000\n",
      "----------\n",
      "train Loss: 64.7970\n",
      "val Loss: 76.0714\n",
      "\n",
      "Epoch 527/1000\n",
      "----------\n",
      "train Loss: 13.7563\n",
      "val Loss: 35.3149\n",
      "\n",
      "Epoch 528/1000\n",
      "----------\n",
      "train Loss: 15.3236\n",
      "val Loss: 3.6881\n",
      "\n",
      "Epoch 529/1000\n",
      "----------\n",
      "train Loss: 50.2400\n",
      "val Loss: 30.8254\n",
      "\n",
      "Epoch 530/1000\n",
      "----------\n",
      "train Loss: 13.5943\n",
      "val Loss: 190.8316\n",
      "\n",
      "Epoch 531/1000\n",
      "----------\n",
      "train Loss: 16.2421\n",
      "val Loss: 7.9587\n",
      "\n",
      "Epoch 532/1000\n",
      "----------\n",
      "train Loss: 21.9084\n",
      "val Loss: 40.2644\n",
      "\n",
      "Epoch 533/1000\n",
      "----------\n",
      "train Loss: 25.5698\n",
      "val Loss: 134.7375\n",
      "\n",
      "Epoch 534/1000\n",
      "----------\n",
      "train Loss: 33.0928\n",
      "val Loss: 49.1428\n",
      "\n",
      "Epoch 535/1000\n",
      "----------\n",
      "train Loss: 47.6036\n",
      "val Loss: 92.9027\n",
      "\n",
      "Epoch 536/1000\n",
      "----------\n",
      "train Loss: 59.8068\n",
      "val Loss: 201.9353\n",
      "\n",
      "Epoch 537/1000\n",
      "----------\n",
      "train Loss: 77.6239\n",
      "val Loss: 201.8384\n",
      "\n",
      "Epoch 538/1000\n",
      "----------\n",
      "train Loss: 60.3151\n",
      "val Loss: 63.3509\n",
      "\n",
      "Epoch 539/1000\n",
      "----------\n",
      "train Loss: 37.5169\n",
      "val Loss: 25.5345\n",
      "\n",
      "Epoch 540/1000\n",
      "----------\n",
      "train Loss: 39.8103\n",
      "val Loss: 113.6992\n",
      "\n",
      "Epoch 541/1000\n",
      "----------\n",
      "train Loss: 66.5811\n",
      "val Loss: 42.8485\n",
      "\n",
      "Epoch 542/1000\n",
      "----------\n",
      "train Loss: 23.2052\n",
      "val Loss: 17.9918\n",
      "\n",
      "Epoch 543/1000\n",
      "----------\n",
      "train Loss: 33.3956\n",
      "val Loss: 152.8765\n",
      "\n",
      "Epoch 544/1000\n",
      "----------\n",
      "train Loss: 48.8324\n",
      "val Loss: 889.1117\n",
      "\n",
      "Epoch 545/1000\n",
      "----------\n",
      "train Loss: 59.6359\n",
      "val Loss: 361.1446\n",
      "\n",
      "Epoch 546/1000\n",
      "----------\n",
      "train Loss: 38.0548\n",
      "val Loss: 77.1912\n",
      "\n",
      "Epoch 547/1000\n",
      "----------\n",
      "train Loss: 12.8842\n",
      "val Loss: 216.1110\n",
      "\n",
      "Epoch 548/1000\n",
      "----------\n",
      "train Loss: 15.0561\n",
      "val Loss: 3.5647\n",
      "\n",
      "Epoch 549/1000\n",
      "----------\n",
      "train Loss: 9.6925\n",
      "val Loss: 58.6219\n",
      "\n",
      "Epoch 550/1000\n",
      "----------\n",
      "train Loss: 34.4116\n",
      "val Loss: 274.5696\n",
      "\n",
      "Epoch 551/1000\n",
      "----------\n",
      "train Loss: 14.9529\n",
      "val Loss: 132.5135\n",
      "\n",
      "Epoch 552/1000\n",
      "----------\n",
      "train Loss: 19.2010\n",
      "val Loss: 308.9016\n",
      "\n",
      "Epoch 553/1000\n",
      "----------\n",
      "train Loss: 60.2901\n",
      "val Loss: 36.4872\n",
      "\n",
      "Epoch 554/1000\n",
      "----------\n",
      "train Loss: 21.0133\n",
      "val Loss: 439.9436\n",
      "\n",
      "Epoch 555/1000\n",
      "----------\n",
      "train Loss: 18.0579\n",
      "val Loss: 2.7638\n",
      "\n",
      "Epoch 556/1000\n",
      "----------\n",
      "train Loss: 8.4567\n",
      "val Loss: 31.8771\n",
      "\n",
      "Epoch 557/1000\n",
      "----------\n",
      "train Loss: 32.8228\n",
      "val Loss: 31.5793\n",
      "\n",
      "Epoch 558/1000\n",
      "----------\n",
      "train Loss: 13.4244\n",
      "val Loss: 116.0255\n",
      "\n",
      "Epoch 559/1000\n",
      "----------\n",
      "train Loss: 11.9603\n",
      "val Loss: 169.7482\n",
      "\n",
      "Epoch 560/1000\n",
      "----------\n",
      "train Loss: 13.0537\n",
      "val Loss: 8.0654\n",
      "\n",
      "Epoch 561/1000\n",
      "----------\n",
      "train Loss: 16.2414\n",
      "val Loss: 127.6332\n",
      "\n",
      "Epoch 562/1000\n",
      "----------\n",
      "train Loss: 52.1054\n",
      "val Loss: 131.8954\n",
      "\n",
      "Epoch 563/1000\n",
      "----------\n",
      "train Loss: 54.9467\n",
      "val Loss: 1.4828\n",
      "\n",
      "Epoch 564/1000\n",
      "----------\n",
      "train Loss: 35.7647\n",
      "val Loss: 69.7149\n",
      "\n",
      "Epoch 565/1000\n",
      "----------\n",
      "train Loss: 44.0686\n",
      "val Loss: 716.3488\n",
      "\n",
      "Epoch 566/1000\n",
      "----------\n",
      "train Loss: 29.5008\n",
      "val Loss: 19.6602\n",
      "\n",
      "Epoch 567/1000\n",
      "----------\n",
      "train Loss: 22.8110\n",
      "val Loss: 48.6275\n",
      "\n",
      "Epoch 568/1000\n",
      "----------\n",
      "train Loss: 5.0867\n",
      "val Loss: 550.1641\n",
      "\n",
      "Epoch 569/1000\n",
      "----------\n",
      "train Loss: 27.7770\n",
      "val Loss: 137.9467\n",
      "\n",
      "Epoch 570/1000\n",
      "----------\n",
      "train Loss: 21.2004\n",
      "val Loss: 121.4456\n",
      "\n",
      "Epoch 571/1000\n",
      "----------\n",
      "train Loss: 43.9587\n",
      "val Loss: 186.2244\n",
      "\n",
      "Epoch 572/1000\n",
      "----------\n",
      "train Loss: 18.3144\n",
      "val Loss: 65.5340\n",
      "\n",
      "Epoch 573/1000\n",
      "----------\n",
      "train Loss: 19.9918\n",
      "val Loss: 8.0693\n",
      "\n",
      "Epoch 574/1000\n",
      "----------\n",
      "train Loss: 36.4017\n",
      "val Loss: 37.7407\n",
      "\n",
      "Epoch 575/1000\n",
      "----------\n",
      "train Loss: 24.4587\n",
      "val Loss: 297.1268\n",
      "\n",
      "Epoch 576/1000\n",
      "----------\n",
      "train Loss: 26.4942\n",
      "val Loss: 661.7041\n",
      "\n",
      "Epoch 577/1000\n",
      "----------\n",
      "train Loss: 15.5074\n",
      "val Loss: 91.4470\n",
      "\n",
      "Epoch 578/1000\n",
      "----------\n",
      "train Loss: 13.0725\n",
      "val Loss: 89.7426\n",
      "\n",
      "Epoch 579/1000\n",
      "----------\n",
      "train Loss: 11.0721\n",
      "val Loss: 20.3905\n",
      "\n",
      "Epoch 580/1000\n",
      "----------\n",
      "train Loss: 83.5242\n",
      "val Loss: 276.9838\n",
      "\n",
      "Epoch 581/1000\n",
      "----------\n",
      "train Loss: 27.9906\n",
      "val Loss: 15.3798\n",
      "\n",
      "Epoch 582/1000\n",
      "----------\n",
      "train Loss: 28.1091\n",
      "val Loss: 4.8771\n",
      "\n",
      "Epoch 583/1000\n",
      "----------\n",
      "train Loss: 32.4969\n",
      "val Loss: 45.6922\n",
      "\n",
      "Epoch 584/1000\n",
      "----------\n",
      "train Loss: 21.6061\n",
      "val Loss: 23.8551\n",
      "\n",
      "Epoch 585/1000\n",
      "----------\n",
      "train Loss: 40.8352\n",
      "val Loss: 8.8799\n",
      "\n",
      "Epoch 586/1000\n",
      "----------\n",
      "train Loss: 27.8346\n",
      "val Loss: 193.9224\n",
      "\n",
      "Epoch 587/1000\n",
      "----------\n",
      "train Loss: 18.0694\n",
      "val Loss: 153.2482\n",
      "\n",
      "Epoch 588/1000\n",
      "----------\n",
      "train Loss: 12.5866\n",
      "val Loss: 92.3540\n",
      "\n",
      "Epoch 589/1000\n",
      "----------\n",
      "train Loss: 14.4603\n",
      "val Loss: 75.5946\n",
      "\n",
      "Epoch 590/1000\n",
      "----------\n",
      "train Loss: 16.5593\n",
      "val Loss: 9.7218\n",
      "\n",
      "Epoch 591/1000\n",
      "----------\n",
      "train Loss: 15.2442\n",
      "val Loss: 90.8932\n",
      "\n",
      "Epoch 592/1000\n",
      "----------\n",
      "train Loss: 20.3229\n",
      "val Loss: 7.0174\n",
      "\n",
      "Epoch 593/1000\n",
      "----------\n",
      "train Loss: 16.7158\n",
      "val Loss: 63.1382\n",
      "\n",
      "Epoch 594/1000\n",
      "----------\n",
      "train Loss: 27.7904\n",
      "val Loss: 3.1948\n",
      "\n",
      "Epoch 595/1000\n",
      "----------\n",
      "train Loss: 21.0311\n",
      "val Loss: 678.6754\n",
      "\n",
      "Epoch 596/1000\n",
      "----------\n",
      "train Loss: 56.0318\n",
      "val Loss: 138.0559\n",
      "\n",
      "Epoch 597/1000\n",
      "----------\n",
      "train Loss: 68.3463\n",
      "val Loss: 51.7156\n",
      "\n",
      "Epoch 598/1000\n",
      "----------\n",
      "train Loss: 21.5109\n",
      "val Loss: 176.7764\n",
      "\n",
      "Epoch 599/1000\n",
      "----------\n",
      "train Loss: 38.4910\n",
      "val Loss: 314.3049\n",
      "\n",
      "Epoch 600/1000\n",
      "----------\n",
      "train Loss: 61.9546\n",
      "val Loss: 270.5356\n",
      "\n",
      "Epoch 601/1000\n",
      "----------\n",
      "train Loss: 31.6945\n",
      "val Loss: 8.1471\n",
      "\n",
      "Epoch 602/1000\n",
      "----------\n",
      "train Loss: 35.8031\n",
      "val Loss: 28.1722\n",
      "\n",
      "Epoch 603/1000\n",
      "----------\n",
      "train Loss: 14.8283\n",
      "val Loss: 321.3415\n",
      "\n",
      "Epoch 604/1000\n",
      "----------\n",
      "train Loss: 19.9105\n",
      "val Loss: 153.9435\n",
      "\n",
      "Epoch 605/1000\n",
      "----------\n",
      "train Loss: 19.2654\n",
      "val Loss: 16.7975\n",
      "\n",
      "Epoch 606/1000\n",
      "----------\n",
      "train Loss: 4.9441\n",
      "val Loss: 40.6244\n",
      "\n",
      "Epoch 607/1000\n",
      "----------\n",
      "train Loss: 20.7144\n",
      "val Loss: 89.0815\n",
      "\n",
      "Epoch 608/1000\n",
      "----------\n",
      "train Loss: 16.1636\n",
      "val Loss: 8.1239\n",
      "\n",
      "Epoch 609/1000\n",
      "----------\n",
      "train Loss: 13.2084\n",
      "val Loss: 21.0641\n",
      "\n",
      "Epoch 610/1000\n",
      "----------\n",
      "train Loss: 22.3471\n",
      "val Loss: 277.2574\n",
      "\n",
      "Epoch 611/1000\n",
      "----------\n",
      "train Loss: 30.2317\n",
      "val Loss: 12.4192\n",
      "\n",
      "Epoch 612/1000\n",
      "----------\n",
      "train Loss: 60.2819\n",
      "val Loss: 66.7346\n",
      "\n",
      "Epoch 613/1000\n",
      "----------\n",
      "train Loss: 32.3615\n",
      "val Loss: 360.3607\n",
      "\n",
      "Epoch 614/1000\n",
      "----------\n",
      "train Loss: 26.7918\n",
      "val Loss: 420.0159\n",
      "\n",
      "Epoch 615/1000\n",
      "----------\n",
      "train Loss: 22.2999\n",
      "val Loss: 805.1636\n",
      "\n",
      "Epoch 616/1000\n",
      "----------\n",
      "train Loss: 17.8684\n",
      "val Loss: 16.5364\n",
      "\n",
      "Epoch 617/1000\n",
      "----------\n",
      "train Loss: 12.8765\n",
      "val Loss: 19.1274\n",
      "\n",
      "Epoch 618/1000\n",
      "----------\n",
      "train Loss: 15.0949\n",
      "val Loss: 16.1687\n",
      "\n",
      "Epoch 619/1000\n",
      "----------\n",
      "train Loss: 44.1999\n",
      "val Loss: 14.4774\n",
      "\n",
      "Epoch 620/1000\n",
      "----------\n",
      "train Loss: 115.7503\n",
      "val Loss: 780.5800\n",
      "\n",
      "Epoch 621/1000\n",
      "----------\n",
      "train Loss: 54.6836\n",
      "val Loss: 11.6724\n",
      "\n",
      "Epoch 622/1000\n",
      "----------\n",
      "train Loss: 29.5311\n",
      "val Loss: 14.0511\n",
      "\n",
      "Epoch 623/1000\n",
      "----------\n",
      "train Loss: 54.0725\n",
      "val Loss: 151.5606\n",
      "\n",
      "Epoch 624/1000\n",
      "----------\n",
      "train Loss: 21.8953\n",
      "val Loss: 62.4588\n",
      "\n",
      "Epoch 625/1000\n",
      "----------\n",
      "train Loss: 31.7113\n",
      "val Loss: 225.6524\n",
      "\n",
      "Epoch 626/1000\n",
      "----------\n",
      "train Loss: 38.9393\n",
      "val Loss: 71.1192\n",
      "\n",
      "Epoch 627/1000\n",
      "----------\n",
      "train Loss: 15.3342\n",
      "val Loss: 2.0587\n",
      "\n",
      "Epoch 628/1000\n",
      "----------\n",
      "train Loss: 26.2713\n",
      "val Loss: 262.6449\n",
      "\n",
      "Epoch 629/1000\n",
      "----------\n",
      "train Loss: 47.5385\n",
      "val Loss: 215.5746\n",
      "\n",
      "Epoch 630/1000\n",
      "----------\n",
      "train Loss: 33.9647\n",
      "val Loss: 14.9963\n",
      "\n",
      "Epoch 631/1000\n",
      "----------\n",
      "train Loss: 36.8418\n",
      "val Loss: 23.8667\n",
      "\n",
      "Epoch 632/1000\n",
      "----------\n",
      "train Loss: 11.0631\n",
      "val Loss: 743.3006\n",
      "\n",
      "Epoch 633/1000\n",
      "----------\n",
      "train Loss: 19.1040\n",
      "val Loss: 148.7974\n",
      "\n",
      "Epoch 634/1000\n",
      "----------\n",
      "train Loss: 11.7652\n",
      "val Loss: 40.2097\n",
      "\n",
      "Epoch 635/1000\n",
      "----------\n",
      "train Loss: 31.6914\n",
      "val Loss: 17.6170\n",
      "\n",
      "Epoch 636/1000\n",
      "----------\n",
      "train Loss: 43.5637\n",
      "val Loss: 119.3895\n",
      "\n",
      "Epoch 637/1000\n",
      "----------\n",
      "train Loss: 25.9233\n",
      "val Loss: 32.3884\n",
      "\n",
      "Epoch 638/1000\n",
      "----------\n",
      "train Loss: 9.2652\n",
      "val Loss: 41.2607\n",
      "\n",
      "Epoch 639/1000\n",
      "----------\n",
      "train Loss: 26.3231\n",
      "val Loss: 59.7143\n",
      "\n",
      "Epoch 640/1000\n",
      "----------\n",
      "train Loss: 35.9347\n",
      "val Loss: 83.9525\n",
      "\n",
      "Epoch 641/1000\n",
      "----------\n",
      "train Loss: 12.3206\n",
      "val Loss: 32.2492\n",
      "\n",
      "Epoch 642/1000\n",
      "----------\n",
      "train Loss: 50.7102\n",
      "val Loss: 45.6365\n",
      "\n",
      "Epoch 643/1000\n",
      "----------\n",
      "train Loss: 63.4902\n",
      "val Loss: 239.1533\n",
      "\n",
      "Epoch 644/1000\n",
      "----------\n",
      "train Loss: 108.5477\n",
      "val Loss: 171.7969\n",
      "\n",
      "Epoch 645/1000\n",
      "----------\n",
      "train Loss: 61.1910\n",
      "val Loss: 19.8120\n",
      "\n",
      "Epoch 646/1000\n",
      "----------\n",
      "train Loss: 36.9995\n",
      "val Loss: 230.0950\n",
      "\n",
      "Epoch 647/1000\n",
      "----------\n",
      "train Loss: 20.0459\n",
      "val Loss: 44.6206\n",
      "\n",
      "Epoch 648/1000\n",
      "----------\n",
      "train Loss: 24.0452\n",
      "val Loss: 16.2098\n",
      "\n",
      "Epoch 649/1000\n",
      "----------\n",
      "train Loss: 53.0821\n",
      "val Loss: 84.7112\n",
      "\n",
      "Epoch 650/1000\n",
      "----------\n",
      "train Loss: 33.9427\n",
      "val Loss: 336.6378\n",
      "\n",
      "Epoch 651/1000\n",
      "----------\n",
      "train Loss: 65.2911\n",
      "val Loss: 777.0102\n",
      "\n",
      "Epoch 652/1000\n",
      "----------\n",
      "train Loss: 28.1089\n",
      "val Loss: 136.2973\n",
      "\n",
      "Epoch 653/1000\n",
      "----------\n",
      "train Loss: 14.6874\n",
      "val Loss: 543.4266\n",
      "\n",
      "Epoch 654/1000\n",
      "----------\n",
      "train Loss: 43.5511\n",
      "val Loss: 64.0928\n",
      "\n",
      "Epoch 655/1000\n",
      "----------\n",
      "train Loss: 16.2302\n",
      "val Loss: 1.5231\n",
      "\n",
      "Epoch 656/1000\n",
      "----------\n",
      "train Loss: 13.2992\n",
      "val Loss: 39.4077\n",
      "\n",
      "Epoch 657/1000\n",
      "----------\n",
      "train Loss: 20.9196\n",
      "val Loss: 89.9463\n",
      "\n",
      "Epoch 658/1000\n",
      "----------\n",
      "train Loss: 34.8098\n",
      "val Loss: 87.0038\n",
      "\n",
      "Epoch 659/1000\n",
      "----------\n",
      "train Loss: 54.0902\n",
      "val Loss: 265.4795\n",
      "\n",
      "Epoch 660/1000\n",
      "----------\n",
      "train Loss: 38.6010\n",
      "val Loss: 189.7321\n",
      "\n",
      "Epoch 661/1000\n",
      "----------\n",
      "train Loss: 19.7221\n",
      "val Loss: 101.3178\n",
      "\n",
      "Epoch 662/1000\n",
      "----------\n",
      "train Loss: 25.8044\n",
      "val Loss: 100.2432\n",
      "\n",
      "Epoch 663/1000\n",
      "----------\n",
      "train Loss: 25.2171\n",
      "val Loss: 46.2823\n",
      "\n",
      "Epoch 664/1000\n",
      "----------\n",
      "train Loss: 31.1989\n",
      "val Loss: 70.9053\n",
      "\n",
      "Epoch 665/1000\n",
      "----------\n",
      "train Loss: 12.6871\n",
      "val Loss: 2.4315\n",
      "\n",
      "Epoch 666/1000\n",
      "----------\n",
      "train Loss: 25.6940\n",
      "val Loss: 585.7080\n",
      "\n",
      "Epoch 667/1000\n",
      "----------\n",
      "train Loss: 13.9898\n",
      "val Loss: 24.0019\n",
      "\n",
      "Epoch 668/1000\n",
      "----------\n",
      "train Loss: 9.3959\n",
      "val Loss: 313.3161\n",
      "\n",
      "Epoch 669/1000\n",
      "----------\n",
      "train Loss: 10.7035\n",
      "val Loss: 84.5255\n",
      "\n",
      "Epoch 670/1000\n",
      "----------\n",
      "train Loss: 17.4394\n",
      "val Loss: 233.1533\n",
      "\n",
      "Epoch 671/1000\n",
      "----------\n",
      "train Loss: 9.8704\n",
      "val Loss: 152.1055\n",
      "\n",
      "Epoch 672/1000\n",
      "----------\n",
      "train Loss: 19.0019\n",
      "val Loss: 570.8544\n",
      "\n",
      "Epoch 673/1000\n",
      "----------\n",
      "train Loss: 84.0031\n",
      "val Loss: 73.5547\n",
      "\n",
      "Epoch 674/1000\n",
      "----------\n",
      "train Loss: 44.6885\n",
      "val Loss: 37.1858\n",
      "\n",
      "Epoch 675/1000\n",
      "----------\n",
      "train Loss: 13.8342\n",
      "val Loss: 51.6487\n",
      "\n",
      "Epoch 676/1000\n",
      "----------\n",
      "train Loss: 28.4642\n",
      "val Loss: 4.7705\n",
      "\n",
      "Epoch 677/1000\n",
      "----------\n",
      "train Loss: 23.6569\n",
      "val Loss: 5.9196\n",
      "\n",
      "Epoch 678/1000\n",
      "----------\n",
      "train Loss: 27.6565\n",
      "val Loss: 744.5156\n",
      "\n",
      "Epoch 679/1000\n",
      "----------\n",
      "train Loss: 7.3722\n",
      "val Loss: 145.0436\n",
      "\n",
      "Epoch 680/1000\n",
      "----------\n",
      "train Loss: 9.6431\n",
      "val Loss: 10.6942\n",
      "\n",
      "Epoch 681/1000\n",
      "----------\n",
      "train Loss: 22.1243\n",
      "val Loss: 2.9365\n",
      "\n",
      "Epoch 682/1000\n",
      "----------\n",
      "train Loss: 20.3858\n",
      "val Loss: 208.4678\n",
      "\n",
      "Epoch 683/1000\n",
      "----------\n",
      "train Loss: 61.3772\n",
      "val Loss: 83.9784\n",
      "\n",
      "Epoch 684/1000\n",
      "----------\n",
      "train Loss: 11.5029\n",
      "val Loss: 106.7611\n",
      "\n",
      "Epoch 685/1000\n",
      "----------\n",
      "train Loss: 12.5060\n",
      "val Loss: 60.3169\n",
      "\n",
      "Epoch 686/1000\n",
      "----------\n",
      "train Loss: 35.3427\n",
      "val Loss: 154.9545\n",
      "\n",
      "Epoch 687/1000\n",
      "----------\n",
      "train Loss: 37.8550\n",
      "val Loss: 97.1621\n",
      "\n",
      "Epoch 688/1000\n",
      "----------\n",
      "train Loss: 21.4805\n",
      "val Loss: 32.9503\n",
      "\n",
      "Epoch 689/1000\n",
      "----------\n",
      "train Loss: 15.0229\n",
      "val Loss: 78.5123\n",
      "\n",
      "Epoch 690/1000\n",
      "----------\n",
      "train Loss: 22.0214\n",
      "val Loss: 28.9246\n",
      "\n",
      "Epoch 691/1000\n",
      "----------\n",
      "train Loss: 34.4321\n",
      "val Loss: 16.0745\n",
      "\n",
      "Epoch 692/1000\n",
      "----------\n",
      "train Loss: 28.0370\n",
      "val Loss: 148.4924\n",
      "\n",
      "Epoch 693/1000\n",
      "----------\n",
      "train Loss: 28.3088\n",
      "val Loss: 284.9464\n",
      "\n",
      "Epoch 694/1000\n",
      "----------\n",
      "train Loss: 62.0153\n",
      "val Loss: 163.5723\n",
      "\n",
      "Epoch 695/1000\n",
      "----------\n",
      "train Loss: 14.8465\n",
      "val Loss: 108.1007\n",
      "\n",
      "Epoch 696/1000\n",
      "----------\n",
      "train Loss: 17.6758\n",
      "val Loss: 19.9837\n",
      "\n",
      "Epoch 697/1000\n",
      "----------\n",
      "train Loss: 26.1221\n",
      "val Loss: 262.9769\n",
      "\n",
      "Epoch 698/1000\n",
      "----------\n",
      "train Loss: 30.0710\n",
      "val Loss: 30.5118\n",
      "\n",
      "Epoch 699/1000\n",
      "----------\n",
      "train Loss: 35.1103\n",
      "val Loss: 8.1732\n",
      "\n",
      "Epoch 700/1000\n",
      "----------\n",
      "train Loss: 32.0824\n",
      "val Loss: 5.5989\n",
      "\n",
      "Epoch 701/1000\n",
      "----------\n",
      "train Loss: 37.1369\n",
      "val Loss: 551.9308\n",
      "\n",
      "Epoch 702/1000\n",
      "----------\n",
      "train Loss: 76.7802\n",
      "val Loss: 55.7993\n",
      "\n",
      "Epoch 703/1000\n",
      "----------\n",
      "train Loss: 67.8497\n",
      "val Loss: 17.4269\n",
      "\n",
      "Epoch 704/1000\n",
      "----------\n",
      "train Loss: 24.5506\n",
      "val Loss: 207.7055\n",
      "\n",
      "Epoch 705/1000\n",
      "----------\n",
      "train Loss: 43.9876\n",
      "val Loss: 91.2810\n",
      "\n",
      "Epoch 706/1000\n",
      "----------\n",
      "train Loss: 39.9997\n",
      "val Loss: 12.8945\n",
      "\n",
      "Epoch 707/1000\n",
      "----------\n",
      "train Loss: 33.8620\n",
      "val Loss: 271.5240\n",
      "\n",
      "Epoch 708/1000\n",
      "----------\n",
      "train Loss: 10.6232\n",
      "val Loss: 16.1927\n",
      "\n",
      "Epoch 709/1000\n",
      "----------\n",
      "train Loss: 29.0156\n",
      "val Loss: 23.6250\n",
      "\n",
      "Epoch 710/1000\n",
      "----------\n",
      "train Loss: 49.2899\n",
      "val Loss: 25.1102\n",
      "\n",
      "Epoch 711/1000\n",
      "----------\n",
      "train Loss: 15.9446\n",
      "val Loss: 8.6975\n",
      "\n",
      "Epoch 712/1000\n",
      "----------\n",
      "train Loss: 21.7616\n",
      "val Loss: 2.4738\n",
      "\n",
      "Epoch 713/1000\n",
      "----------\n",
      "train Loss: 34.1613\n",
      "val Loss: 30.1271\n",
      "\n",
      "Epoch 714/1000\n",
      "----------\n",
      "train Loss: 57.6727\n",
      "val Loss: 7.9869\n",
      "\n",
      "Epoch 715/1000\n",
      "----------\n",
      "train Loss: 62.5653\n",
      "val Loss: 126.3980\n",
      "\n",
      "Epoch 716/1000\n",
      "----------\n",
      "train Loss: 39.3232\n",
      "val Loss: 13.0441\n",
      "\n",
      "Epoch 717/1000\n",
      "----------\n",
      "train Loss: 10.7217\n",
      "val Loss: 293.0970\n",
      "\n",
      "Epoch 718/1000\n",
      "----------\n",
      "train Loss: 52.9575\n",
      "val Loss: 190.5864\n",
      "\n",
      "Epoch 719/1000\n",
      "----------\n",
      "train Loss: 41.1504\n",
      "val Loss: 44.3492\n",
      "\n",
      "Epoch 720/1000\n",
      "----------\n",
      "train Loss: 37.9973\n",
      "val Loss: 40.2576\n",
      "\n",
      "Epoch 721/1000\n",
      "----------\n",
      "train Loss: 67.3010\n",
      "val Loss: 46.0709\n",
      "\n",
      "Epoch 722/1000\n",
      "----------\n",
      "train Loss: 58.9692\n",
      "val Loss: 200.3524\n",
      "\n",
      "Epoch 723/1000\n",
      "----------\n",
      "train Loss: 45.3353\n",
      "val Loss: 37.0525\n",
      "\n",
      "Epoch 724/1000\n",
      "----------\n",
      "train Loss: 24.3398\n",
      "val Loss: 48.1185\n",
      "\n",
      "Epoch 725/1000\n",
      "----------\n",
      "train Loss: 14.2954\n",
      "val Loss: 194.4104\n",
      "\n",
      "Epoch 726/1000\n",
      "----------\n",
      "train Loss: 14.9817\n",
      "val Loss: 14.0716\n",
      "\n",
      "Epoch 727/1000\n",
      "----------\n",
      "train Loss: 15.3707\n",
      "val Loss: 13.0613\n",
      "\n",
      "Epoch 728/1000\n",
      "----------\n",
      "train Loss: 15.0350\n",
      "val Loss: 35.3074\n",
      "\n",
      "Epoch 729/1000\n",
      "----------\n",
      "train Loss: 27.0887\n",
      "val Loss: 9.5326\n",
      "\n",
      "Epoch 730/1000\n",
      "----------\n",
      "train Loss: 32.8560\n",
      "val Loss: 11.0369\n",
      "\n",
      "Epoch 731/1000\n",
      "----------\n",
      "train Loss: 36.3493\n",
      "val Loss: 118.6301\n",
      "\n",
      "Epoch 732/1000\n",
      "----------\n",
      "train Loss: 35.0713\n",
      "val Loss: 217.6219\n",
      "\n",
      "Epoch 733/1000\n",
      "----------\n",
      "train Loss: 25.4980\n",
      "val Loss: 21.3656\n",
      "\n",
      "Epoch 734/1000\n",
      "----------\n",
      "train Loss: 20.3767\n",
      "val Loss: 10.0144\n",
      "\n",
      "Epoch 735/1000\n",
      "----------\n",
      "train Loss: 10.1785\n",
      "val Loss: 29.2961\n",
      "\n",
      "Epoch 736/1000\n",
      "----------\n",
      "train Loss: 11.1121\n",
      "val Loss: 14.3803\n",
      "\n",
      "Epoch 737/1000\n",
      "----------\n",
      "train Loss: 29.6865\n",
      "val Loss: 239.5914\n",
      "\n",
      "Epoch 738/1000\n",
      "----------\n",
      "train Loss: 19.8818\n",
      "val Loss: 132.8590\n",
      "\n",
      "Epoch 739/1000\n",
      "----------\n",
      "train Loss: 24.7576\n",
      "val Loss: 498.7143\n",
      "\n",
      "Epoch 740/1000\n",
      "----------\n",
      "train Loss: 18.5409\n",
      "val Loss: 145.6892\n",
      "\n",
      "Epoch 741/1000\n",
      "----------\n",
      "train Loss: 15.8589\n",
      "val Loss: 8.9446\n",
      "\n",
      "Epoch 742/1000\n",
      "----------\n",
      "train Loss: 42.5428\n",
      "val Loss: 440.6441\n",
      "\n",
      "Epoch 743/1000\n",
      "----------\n",
      "train Loss: 68.0578\n",
      "val Loss: 54.1160\n",
      "\n",
      "Epoch 744/1000\n",
      "----------\n",
      "train Loss: 24.9133\n",
      "val Loss: 94.0347\n",
      "\n",
      "Epoch 745/1000\n",
      "----------\n",
      "train Loss: 16.1985\n",
      "val Loss: 8.2405\n",
      "\n",
      "Epoch 746/1000\n",
      "----------\n",
      "train Loss: 34.5866\n",
      "val Loss: 267.2425\n",
      "\n",
      "Epoch 747/1000\n",
      "----------\n",
      "train Loss: 26.3400\n",
      "val Loss: 337.1709\n",
      "\n",
      "Epoch 748/1000\n",
      "----------\n",
      "train Loss: 17.1702\n",
      "val Loss: 1.0660\n",
      "\n",
      "Epoch 749/1000\n",
      "----------\n",
      "train Loss: 6.5009\n",
      "val Loss: 203.0833\n",
      "\n",
      "Epoch 750/1000\n",
      "----------\n",
      "train Loss: 18.1006\n",
      "val Loss: 460.8705\n",
      "\n",
      "Epoch 751/1000\n",
      "----------\n",
      "train Loss: 14.6289\n",
      "val Loss: 167.3025\n",
      "\n",
      "Epoch 752/1000\n",
      "----------\n",
      "train Loss: 25.6671\n",
      "val Loss: 132.3002\n",
      "\n",
      "Epoch 753/1000\n",
      "----------\n",
      "train Loss: 9.2034\n",
      "val Loss: 3.7537\n",
      "\n",
      "Epoch 754/1000\n",
      "----------\n",
      "train Loss: 12.9540\n",
      "val Loss: 145.4219\n",
      "\n",
      "Epoch 755/1000\n",
      "----------\n",
      "train Loss: 41.3399\n",
      "val Loss: 4.5234\n",
      "\n",
      "Epoch 756/1000\n",
      "----------\n",
      "train Loss: 10.7270\n",
      "val Loss: 17.6607\n",
      "\n",
      "Epoch 757/1000\n",
      "----------\n",
      "train Loss: 28.4844\n",
      "val Loss: 2.5286\n",
      "\n",
      "Epoch 758/1000\n",
      "----------\n",
      "train Loss: 28.0170\n",
      "val Loss: 66.2434\n",
      "\n",
      "Epoch 759/1000\n",
      "----------\n",
      "train Loss: 25.9557\n",
      "val Loss: 122.1221\n",
      "\n",
      "Epoch 760/1000\n",
      "----------\n",
      "train Loss: 12.0623\n",
      "val Loss: 349.6906\n",
      "\n",
      "Epoch 761/1000\n",
      "----------\n",
      "train Loss: 31.4141\n",
      "val Loss: 75.7291\n",
      "\n",
      "Epoch 762/1000\n",
      "----------\n",
      "train Loss: 30.3559\n",
      "val Loss: 477.9534\n",
      "\n",
      "Epoch 763/1000\n",
      "----------\n",
      "train Loss: 36.0635\n",
      "val Loss: 73.1998\n",
      "\n",
      "Epoch 764/1000\n",
      "----------\n",
      "train Loss: 17.0580\n",
      "val Loss: 74.7353\n",
      "\n",
      "Epoch 765/1000\n",
      "----------\n",
      "train Loss: 23.9637\n",
      "val Loss: 23.7219\n",
      "\n",
      "Epoch 766/1000\n",
      "----------\n",
      "train Loss: 48.4690\n",
      "val Loss: 732.1733\n",
      "\n",
      "Epoch 767/1000\n",
      "----------\n",
      "train Loss: 16.5343\n",
      "val Loss: 58.6788\n",
      "\n",
      "Epoch 768/1000\n",
      "----------\n",
      "train Loss: 16.9991\n",
      "val Loss: 421.1683\n",
      "\n",
      "Epoch 769/1000\n",
      "----------\n",
      "train Loss: 33.5254\n",
      "val Loss: 156.8304\n",
      "\n",
      "Epoch 770/1000\n",
      "----------\n",
      "train Loss: 53.9405\n",
      "val Loss: 196.4300\n",
      "\n",
      "Epoch 771/1000\n",
      "----------\n",
      "train Loss: 47.5001\n",
      "val Loss: 337.9768\n",
      "\n",
      "Epoch 772/1000\n",
      "----------\n",
      "train Loss: 23.8070\n",
      "val Loss: 384.2884\n",
      "\n",
      "Epoch 773/1000\n",
      "----------\n",
      "train Loss: 19.5235\n",
      "val Loss: 1212.0621\n",
      "\n",
      "Epoch 774/1000\n",
      "----------\n",
      "train Loss: 26.0326\n",
      "val Loss: 39.2670\n",
      "\n",
      "Epoch 775/1000\n",
      "----------\n",
      "train Loss: 17.4545\n",
      "val Loss: 10.6399\n",
      "\n",
      "Epoch 776/1000\n",
      "----------\n",
      "train Loss: 11.2839\n",
      "val Loss: 41.6549\n",
      "\n",
      "Epoch 777/1000\n",
      "----------\n",
      "train Loss: 11.7682\n",
      "val Loss: 102.1821\n",
      "\n",
      "Epoch 778/1000\n",
      "----------\n",
      "train Loss: 13.9017\n",
      "val Loss: 100.2702\n",
      "\n",
      "Epoch 779/1000\n",
      "----------\n",
      "train Loss: 27.1438\n",
      "val Loss: 4.6811\n",
      "\n",
      "Epoch 780/1000\n",
      "----------\n",
      "train Loss: 43.8311\n",
      "val Loss: 85.0672\n",
      "\n",
      "Epoch 781/1000\n",
      "----------\n",
      "train Loss: 20.7293\n",
      "val Loss: 4.3689\n",
      "\n",
      "Epoch 782/1000\n",
      "----------\n",
      "train Loss: 11.7815\n",
      "val Loss: 33.2410\n",
      "\n",
      "Epoch 783/1000\n",
      "----------\n",
      "train Loss: 21.8973\n",
      "val Loss: 231.4734\n",
      "\n",
      "Epoch 784/1000\n",
      "----------\n",
      "train Loss: 48.7020\n",
      "val Loss: 45.0447\n",
      "\n",
      "Epoch 785/1000\n",
      "----------\n",
      "train Loss: 10.3717\n",
      "val Loss: 12.5513\n",
      "\n",
      "Epoch 786/1000\n",
      "----------\n",
      "train Loss: 19.8950\n",
      "val Loss: 46.2588\n",
      "\n",
      "Epoch 787/1000\n",
      "----------\n",
      "train Loss: 66.7782\n",
      "val Loss: 28.0909\n",
      "\n",
      "Epoch 788/1000\n",
      "----------\n",
      "train Loss: 39.6259\n",
      "val Loss: 92.5046\n",
      "\n",
      "Epoch 789/1000\n",
      "----------\n",
      "train Loss: 18.0805\n",
      "val Loss: 33.5734\n",
      "\n",
      "Epoch 790/1000\n",
      "----------\n",
      "train Loss: 16.9984\n",
      "val Loss: 13.6541\n",
      "\n",
      "Epoch 791/1000\n",
      "----------\n",
      "train Loss: 46.7292\n",
      "val Loss: 15.8327\n",
      "\n",
      "Epoch 792/1000\n",
      "----------\n",
      "train Loss: 32.6191\n",
      "val Loss: 6.6459\n",
      "\n",
      "Epoch 793/1000\n",
      "----------\n",
      "train Loss: 80.2076\n",
      "val Loss: 15.1559\n",
      "\n",
      "Epoch 794/1000\n",
      "----------\n",
      "train Loss: 88.3008\n",
      "val Loss: 638.9846\n",
      "\n",
      "Epoch 795/1000\n",
      "----------\n",
      "train Loss: 73.4619\n",
      "val Loss: 179.2638\n",
      "\n",
      "Epoch 796/1000\n",
      "----------\n",
      "train Loss: 59.4415\n",
      "val Loss: 12.7504\n",
      "\n",
      "Epoch 797/1000\n",
      "----------\n",
      "train Loss: 19.1841\n",
      "val Loss: 14.9585\n",
      "\n",
      "Epoch 798/1000\n",
      "----------\n",
      "train Loss: 38.6432\n",
      "val Loss: 14.5901\n",
      "\n",
      "Epoch 799/1000\n",
      "----------\n",
      "train Loss: 20.2900\n",
      "val Loss: 94.8006\n",
      "\n",
      "Epoch 800/1000\n",
      "----------\n",
      "train Loss: 41.2362\n",
      "val Loss: 65.0701\n",
      "\n",
      "Epoch 801/1000\n",
      "----------\n",
      "train Loss: 9.5551\n",
      "val Loss: 84.6687\n",
      "\n",
      "Epoch 802/1000\n",
      "----------\n",
      "train Loss: 15.2323\n",
      "val Loss: 37.3773\n",
      "\n",
      "Epoch 803/1000\n",
      "----------\n",
      "train Loss: 14.4750\n",
      "val Loss: 53.8792\n",
      "\n",
      "Epoch 804/1000\n",
      "----------\n",
      "train Loss: 8.7787\n",
      "val Loss: 5.5101\n",
      "\n",
      "Epoch 805/1000\n",
      "----------\n",
      "train Loss: 12.7542\n",
      "val Loss: 261.3831\n",
      "\n",
      "Epoch 806/1000\n",
      "----------\n",
      "train Loss: 23.1493\n",
      "val Loss: 13.5611\n",
      "\n",
      "Epoch 807/1000\n",
      "----------\n",
      "train Loss: 45.3806\n",
      "val Loss: 314.7895\n",
      "\n",
      "Epoch 808/1000\n",
      "----------\n",
      "train Loss: 80.8590\n",
      "val Loss: 88.5802\n",
      "\n",
      "Epoch 809/1000\n",
      "----------\n",
      "train Loss: 34.5162\n",
      "val Loss: 325.1340\n",
      "\n",
      "Epoch 810/1000\n",
      "----------\n",
      "train Loss: 36.5361\n",
      "val Loss: 210.7335\n",
      "\n",
      "Epoch 811/1000\n",
      "----------\n",
      "train Loss: 24.8415\n",
      "val Loss: 38.1390\n",
      "\n",
      "Epoch 812/1000\n",
      "----------\n",
      "train Loss: 23.9848\n",
      "val Loss: 84.0583\n",
      "\n",
      "Epoch 813/1000\n",
      "----------\n",
      "train Loss: 33.9783\n",
      "val Loss: 185.6008\n",
      "\n",
      "Epoch 814/1000\n",
      "----------\n",
      "train Loss: 33.7177\n",
      "val Loss: 9.6688\n",
      "\n",
      "Epoch 815/1000\n",
      "----------\n",
      "train Loss: 10.9584\n",
      "val Loss: 9.1819\n",
      "\n",
      "Epoch 816/1000\n",
      "----------\n",
      "train Loss: 16.0726\n",
      "val Loss: 24.0379\n",
      "\n",
      "Epoch 817/1000\n",
      "----------\n",
      "train Loss: 17.9283\n",
      "val Loss: 31.8129\n",
      "\n",
      "Epoch 818/1000\n",
      "----------\n",
      "train Loss: 25.9014\n",
      "val Loss: 86.1822\n",
      "\n",
      "Epoch 819/1000\n",
      "----------\n",
      "train Loss: 64.9942\n",
      "val Loss: 89.2341\n",
      "\n",
      "Epoch 820/1000\n",
      "----------\n",
      "train Loss: 37.8335\n",
      "val Loss: 3.6472\n",
      "\n",
      "Epoch 821/1000\n",
      "----------\n",
      "train Loss: 45.4413\n",
      "val Loss: 54.6906\n",
      "\n",
      "Epoch 822/1000\n",
      "----------\n",
      "train Loss: 16.6581\n",
      "val Loss: 497.8008\n",
      "\n",
      "Epoch 823/1000\n",
      "----------\n",
      "train Loss: 38.7268\n",
      "val Loss: 37.7166\n",
      "\n",
      "Epoch 824/1000\n",
      "----------\n",
      "train Loss: 27.5494\n",
      "val Loss: 12.0481\n",
      "\n",
      "Epoch 825/1000\n",
      "----------\n",
      "train Loss: 15.8100\n",
      "val Loss: 154.0193\n",
      "\n",
      "Epoch 826/1000\n",
      "----------\n",
      "train Loss: 28.2360\n",
      "val Loss: 9.8750\n",
      "\n",
      "Epoch 827/1000\n",
      "----------\n",
      "train Loss: 24.6764\n",
      "val Loss: 264.0542\n",
      "\n",
      "Epoch 828/1000\n",
      "----------\n",
      "train Loss: 10.4447\n",
      "val Loss: 49.0995\n",
      "\n",
      "Epoch 829/1000\n",
      "----------\n",
      "train Loss: 16.4232\n",
      "val Loss: 439.7929\n",
      "\n",
      "Epoch 830/1000\n",
      "----------\n",
      "train Loss: 14.1504\n",
      "val Loss: 5.6083\n",
      "\n",
      "Epoch 831/1000\n",
      "----------\n",
      "train Loss: 22.3691\n",
      "val Loss: 172.2225\n",
      "\n",
      "Epoch 832/1000\n",
      "----------\n",
      "train Loss: 13.4867\n",
      "val Loss: 38.6479\n",
      "\n",
      "Epoch 833/1000\n",
      "----------\n",
      "train Loss: 45.7271\n",
      "val Loss: 16.3662\n",
      "\n",
      "Epoch 834/1000\n",
      "----------\n",
      "train Loss: 55.7268\n",
      "val Loss: 44.2528\n",
      "\n",
      "Epoch 835/1000\n",
      "----------\n",
      "train Loss: 17.6716\n",
      "val Loss: 6.8851\n",
      "\n",
      "Epoch 836/1000\n",
      "----------\n",
      "train Loss: 27.1918\n",
      "val Loss: 133.2634\n",
      "\n",
      "Epoch 837/1000\n",
      "----------\n",
      "train Loss: 13.9146\n",
      "val Loss: 155.5219\n",
      "\n",
      "Epoch 838/1000\n",
      "----------\n",
      "train Loss: 59.6646\n",
      "val Loss: 18.7178\n",
      "\n",
      "Epoch 839/1000\n",
      "----------\n",
      "train Loss: 22.6858\n",
      "val Loss: 3.6947\n",
      "\n",
      "Epoch 840/1000\n",
      "----------\n",
      "train Loss: 29.8833\n",
      "val Loss: 232.5970\n",
      "\n",
      "Epoch 841/1000\n",
      "----------\n",
      "train Loss: 70.3941\n",
      "val Loss: 66.0166\n",
      "\n",
      "Epoch 842/1000\n",
      "----------\n",
      "train Loss: 92.8939\n",
      "val Loss: 104.8238\n",
      "\n",
      "Epoch 843/1000\n",
      "----------\n",
      "train Loss: 30.8390\n",
      "val Loss: 186.0597\n",
      "\n",
      "Epoch 844/1000\n",
      "----------\n",
      "train Loss: 23.1226\n",
      "val Loss: 2.0254\n",
      "\n",
      "Epoch 845/1000\n",
      "----------\n",
      "train Loss: 25.2577\n",
      "val Loss: 141.1335\n",
      "\n",
      "Epoch 846/1000\n",
      "----------\n",
      "train Loss: 28.4357\n",
      "val Loss: 36.5939\n",
      "\n",
      "Epoch 847/1000\n",
      "----------\n",
      "train Loss: 19.8066\n",
      "val Loss: 61.3459\n",
      "\n",
      "Epoch 848/1000\n",
      "----------\n",
      "train Loss: 26.3370\n",
      "val Loss: 616.5631\n",
      "\n",
      "Epoch 849/1000\n",
      "----------\n",
      "train Loss: 19.1837\n",
      "val Loss: 22.9088\n",
      "\n",
      "Epoch 850/1000\n",
      "----------\n",
      "train Loss: 18.6689\n",
      "val Loss: 96.0279\n",
      "\n",
      "Epoch 851/1000\n",
      "----------\n",
      "train Loss: 21.6926\n",
      "val Loss: 93.2278\n",
      "\n",
      "Epoch 852/1000\n",
      "----------\n",
      "train Loss: 28.9978\n",
      "val Loss: 2.8922\n",
      "\n",
      "Epoch 853/1000\n",
      "----------\n",
      "train Loss: 12.4319\n",
      "val Loss: 21.6254\n",
      "\n",
      "Epoch 854/1000\n",
      "----------\n",
      "train Loss: 19.0074\n",
      "val Loss: 14.2256\n",
      "\n",
      "Epoch 855/1000\n",
      "----------\n",
      "train Loss: 18.0063\n",
      "val Loss: 200.3946\n",
      "\n",
      "Epoch 856/1000\n",
      "----------\n",
      "train Loss: 19.6401\n",
      "val Loss: 29.5086\n",
      "\n",
      "Epoch 857/1000\n",
      "----------\n",
      "train Loss: 28.2254\n",
      "val Loss: 89.0674\n",
      "\n",
      "Epoch 858/1000\n",
      "----------\n",
      "train Loss: 26.1391\n",
      "val Loss: 90.3648\n",
      "\n",
      "Epoch 859/1000\n",
      "----------\n",
      "train Loss: 49.7341\n",
      "val Loss: 130.9957\n",
      "\n",
      "Epoch 860/1000\n",
      "----------\n",
      "train Loss: 59.2846\n",
      "val Loss: 140.7346\n",
      "\n",
      "Epoch 861/1000\n",
      "----------\n",
      "train Loss: 55.7809\n",
      "val Loss: 370.7126\n",
      "\n",
      "Epoch 862/1000\n",
      "----------\n",
      "train Loss: 45.6294\n",
      "val Loss: 9.4490\n",
      "\n",
      "Epoch 863/1000\n",
      "----------\n",
      "train Loss: 25.1923\n",
      "val Loss: 8.7636\n",
      "\n",
      "Epoch 864/1000\n",
      "----------\n",
      "train Loss: 59.9590\n",
      "val Loss: 15.7821\n",
      "\n",
      "Epoch 865/1000\n",
      "----------\n",
      "train Loss: 20.0919\n",
      "val Loss: 9.6229\n",
      "\n",
      "Epoch 866/1000\n",
      "----------\n",
      "train Loss: 37.4694\n",
      "val Loss: 28.6127\n",
      "\n",
      "Epoch 867/1000\n",
      "----------\n",
      "train Loss: 44.7105\n",
      "val Loss: 10.4209\n",
      "\n",
      "Epoch 868/1000\n",
      "----------\n",
      "train Loss: 13.4110\n",
      "val Loss: 2.4170\n",
      "\n",
      "Epoch 869/1000\n",
      "----------\n",
      "train Loss: 65.4967\n",
      "val Loss: 17.2356\n",
      "\n",
      "Epoch 870/1000\n",
      "----------\n",
      "train Loss: 61.4204\n",
      "val Loss: 3.8885\n",
      "\n",
      "Epoch 871/1000\n",
      "----------\n",
      "train Loss: 25.9991\n",
      "val Loss: 25.3306\n",
      "\n",
      "Epoch 872/1000\n",
      "----------\n",
      "train Loss: 43.0346\n",
      "val Loss: 4.1875\n",
      "\n",
      "Epoch 873/1000\n",
      "----------\n",
      "train Loss: 22.3704\n",
      "val Loss: 20.0247\n",
      "\n",
      "Epoch 874/1000\n",
      "----------\n",
      "train Loss: 46.2901\n",
      "val Loss: 25.7074\n",
      "\n",
      "Epoch 875/1000\n",
      "----------\n",
      "train Loss: 16.9835\n",
      "val Loss: 372.0748\n",
      "\n",
      "Epoch 876/1000\n",
      "----------\n",
      "train Loss: 21.8264\n",
      "val Loss: 2.0663\n",
      "\n",
      "Epoch 877/1000\n",
      "----------\n",
      "train Loss: 32.6461\n",
      "val Loss: 9.0907\n",
      "\n",
      "Epoch 878/1000\n",
      "----------\n",
      "train Loss: 6.1299\n",
      "val Loss: 26.3089\n",
      "\n",
      "Epoch 879/1000\n",
      "----------\n",
      "train Loss: 7.6782\n",
      "val Loss: 93.3941\n",
      "\n",
      "Epoch 880/1000\n",
      "----------\n",
      "train Loss: 18.8064\n",
      "val Loss: 6.9966\n",
      "\n",
      "Epoch 881/1000\n",
      "----------\n",
      "train Loss: 3.9038\n",
      "val Loss: 7.2997\n",
      "\n",
      "Epoch 882/1000\n",
      "----------\n",
      "train Loss: 12.1409\n",
      "val Loss: 10.9879\n",
      "\n",
      "Epoch 883/1000\n",
      "----------\n",
      "train Loss: 8.1826\n",
      "val Loss: 17.9820\n",
      "\n",
      "Epoch 884/1000\n",
      "----------\n",
      "train Loss: 29.3383\n",
      "val Loss: 24.4711\n",
      "\n",
      "Epoch 885/1000\n",
      "----------\n",
      "train Loss: 42.8356\n",
      "val Loss: 261.8379\n",
      "\n",
      "Epoch 886/1000\n",
      "----------\n",
      "train Loss: 40.8939\n",
      "val Loss: 73.9339\n",
      "\n",
      "Epoch 887/1000\n",
      "----------\n",
      "train Loss: 60.4693\n",
      "val Loss: 22.0891\n",
      "\n",
      "Epoch 888/1000\n",
      "----------\n",
      "train Loss: 36.9315\n",
      "val Loss: 10.8452\n",
      "\n",
      "Epoch 889/1000\n",
      "----------\n",
      "train Loss: 22.3855\n",
      "val Loss: 11.1725\n",
      "\n",
      "Epoch 890/1000\n",
      "----------\n",
      "train Loss: 21.9366\n",
      "val Loss: 300.8866\n",
      "\n",
      "Epoch 891/1000\n",
      "----------\n",
      "train Loss: 12.4408\n",
      "val Loss: 4.3494\n",
      "\n",
      "Epoch 892/1000\n",
      "----------\n",
      "train Loss: 18.3423\n",
      "val Loss: 154.3127\n",
      "\n",
      "Epoch 893/1000\n",
      "----------\n",
      "train Loss: 44.9525\n",
      "val Loss: 7.3503\n",
      "\n",
      "Epoch 894/1000\n",
      "----------\n",
      "train Loss: 55.4500\n",
      "val Loss: 48.6196\n",
      "\n",
      "Epoch 895/1000\n",
      "----------\n",
      "train Loss: 30.0189\n",
      "val Loss: 38.2958\n",
      "\n",
      "Epoch 896/1000\n",
      "----------\n",
      "train Loss: 20.1191\n",
      "val Loss: 49.1075\n",
      "\n",
      "Epoch 897/1000\n",
      "----------\n",
      "train Loss: 15.6578\n",
      "val Loss: 109.5480\n",
      "\n",
      "Epoch 898/1000\n",
      "----------\n",
      "train Loss: 32.8361\n",
      "val Loss: 8.2646\n",
      "\n",
      "Epoch 899/1000\n",
      "----------\n",
      "train Loss: 50.2421\n",
      "val Loss: 13.9985\n",
      "\n",
      "Epoch 900/1000\n",
      "----------\n",
      "train Loss: 47.7435\n",
      "val Loss: 11.2936\n",
      "\n",
      "Epoch 901/1000\n",
      "----------\n",
      "train Loss: 23.6863\n",
      "val Loss: 14.5831\n",
      "\n",
      "Epoch 902/1000\n",
      "----------\n",
      "train Loss: 14.5679\n",
      "val Loss: 157.8386\n",
      "\n",
      "Epoch 903/1000\n",
      "----------\n",
      "train Loss: 14.3319\n",
      "val Loss: 80.3325\n",
      "\n",
      "Epoch 904/1000\n",
      "----------\n",
      "train Loss: 22.2829\n",
      "val Loss: 6.4914\n",
      "\n",
      "Epoch 905/1000\n",
      "----------\n",
      "train Loss: 7.1547\n",
      "val Loss: 167.5432\n",
      "\n",
      "Epoch 906/1000\n",
      "----------\n",
      "train Loss: 10.8636\n",
      "val Loss: 4.3542\n",
      "\n",
      "Epoch 907/1000\n",
      "----------\n",
      "train Loss: 20.2446\n",
      "val Loss: 24.0965\n",
      "\n",
      "Epoch 908/1000\n",
      "----------\n",
      "train Loss: 6.7727\n",
      "val Loss: 163.9454\n",
      "\n",
      "Epoch 909/1000\n",
      "----------\n",
      "train Loss: 24.4614\n",
      "val Loss: 0.9006\n",
      "\n",
      "Epoch 910/1000\n",
      "----------\n",
      "train Loss: 39.0574\n",
      "val Loss: 20.7789\n",
      "\n",
      "Epoch 911/1000\n",
      "----------\n",
      "train Loss: 39.6323\n",
      "val Loss: 21.5039\n",
      "\n",
      "Epoch 912/1000\n",
      "----------\n",
      "train Loss: 20.6857\n",
      "val Loss: 3.1475\n",
      "\n",
      "Epoch 913/1000\n",
      "----------\n",
      "train Loss: 42.7005\n",
      "val Loss: 11.8504\n",
      "\n",
      "Epoch 914/1000\n",
      "----------\n",
      "train Loss: 12.0768\n",
      "val Loss: 41.4267\n",
      "\n",
      "Epoch 915/1000\n",
      "----------\n",
      "train Loss: 9.4416\n",
      "val Loss: 1.6961\n",
      "\n",
      "Epoch 916/1000\n",
      "----------\n",
      "train Loss: 27.9800\n",
      "val Loss: 24.4266\n",
      "\n",
      "Epoch 917/1000\n",
      "----------\n",
      "train Loss: 22.7718\n",
      "val Loss: 29.5016\n",
      "\n",
      "Epoch 918/1000\n",
      "----------\n",
      "train Loss: 18.7278\n",
      "val Loss: 79.1168\n",
      "\n",
      "Epoch 919/1000\n",
      "----------\n",
      "train Loss: 15.6715\n",
      "val Loss: 1.5668\n",
      "\n",
      "Epoch 920/1000\n",
      "----------\n",
      "train Loss: 24.3792\n",
      "val Loss: 31.1849\n",
      "\n",
      "Epoch 921/1000\n",
      "----------\n",
      "train Loss: 17.6592\n",
      "val Loss: 1.7137\n",
      "\n",
      "Epoch 922/1000\n",
      "----------\n",
      "train Loss: 29.3018\n",
      "val Loss: 55.2979\n",
      "\n",
      "Epoch 923/1000\n",
      "----------\n",
      "train Loss: 19.8373\n",
      "val Loss: 139.4291\n",
      "\n",
      "Epoch 924/1000\n",
      "----------\n",
      "train Loss: 10.9744\n",
      "val Loss: 16.1985\n",
      "\n",
      "Epoch 925/1000\n",
      "----------\n",
      "train Loss: 22.9352\n",
      "val Loss: 72.0348\n",
      "\n",
      "Epoch 926/1000\n",
      "----------\n",
      "train Loss: 45.2627\n",
      "val Loss: 19.4496\n",
      "\n",
      "Epoch 927/1000\n",
      "----------\n",
      "train Loss: 22.3495\n",
      "val Loss: 10.1829\n",
      "\n",
      "Epoch 928/1000\n",
      "----------\n",
      "train Loss: 4.8280\n",
      "val Loss: 8.5686\n",
      "\n",
      "Epoch 929/1000\n",
      "----------\n",
      "train Loss: 29.6058\n",
      "val Loss: 8.7675\n",
      "\n",
      "Epoch 930/1000\n",
      "----------\n",
      "train Loss: 16.3095\n",
      "val Loss: 77.3357\n",
      "\n",
      "Epoch 931/1000\n",
      "----------\n",
      "train Loss: 30.3802\n",
      "val Loss: 308.9458\n",
      "\n",
      "Epoch 932/1000\n",
      "----------\n",
      "train Loss: 39.5977\n",
      "val Loss: 2.5657\n",
      "\n",
      "Epoch 933/1000\n",
      "----------\n",
      "train Loss: 26.7571\n",
      "val Loss: 67.7914\n",
      "\n",
      "Epoch 934/1000\n",
      "----------\n",
      "train Loss: 26.1504\n",
      "val Loss: 83.4499\n",
      "\n",
      "Epoch 935/1000\n",
      "----------\n",
      "train Loss: 10.5196\n",
      "val Loss: 3.2723\n",
      "\n",
      "Epoch 936/1000\n",
      "----------\n",
      "train Loss: 11.1944\n",
      "val Loss: 260.1447\n",
      "\n",
      "Epoch 937/1000\n",
      "----------\n",
      "train Loss: 33.5590\n",
      "val Loss: 36.6317\n",
      "\n",
      "Epoch 938/1000\n",
      "----------\n",
      "train Loss: 9.2766\n",
      "val Loss: 3.5990\n",
      "\n",
      "Epoch 939/1000\n",
      "----------\n",
      "train Loss: 18.2172\n",
      "val Loss: 12.6308\n",
      "\n",
      "Epoch 940/1000\n",
      "----------\n",
      "train Loss: 8.3009\n",
      "val Loss: 2.8443\n",
      "\n",
      "Epoch 941/1000\n",
      "----------\n",
      "train Loss: 11.3432\n",
      "val Loss: 2.7276\n",
      "\n",
      "Epoch 942/1000\n",
      "----------\n",
      "train Loss: 44.6120\n",
      "val Loss: 231.0062\n",
      "\n",
      "Epoch 943/1000\n",
      "----------\n",
      "train Loss: 56.0492\n",
      "val Loss: 111.3708\n",
      "\n",
      "Epoch 944/1000\n",
      "----------\n",
      "train Loss: 18.5973\n",
      "val Loss: 24.8840\n",
      "\n",
      "Epoch 945/1000\n",
      "----------\n",
      "train Loss: 34.9206\n",
      "val Loss: 35.2125\n",
      "\n",
      "Epoch 946/1000\n",
      "----------\n",
      "train Loss: 18.3491\n",
      "val Loss: 243.9581\n",
      "\n",
      "Epoch 947/1000\n",
      "----------\n",
      "train Loss: 10.8841\n",
      "val Loss: 112.6876\n",
      "\n",
      "Epoch 948/1000\n",
      "----------\n",
      "train Loss: 25.1818\n",
      "val Loss: 1.6098\n",
      "\n",
      "Epoch 949/1000\n",
      "----------\n",
      "train Loss: 18.2639\n",
      "val Loss: 20.9572\n",
      "\n",
      "Epoch 950/1000\n",
      "----------\n",
      "train Loss: 7.3684\n",
      "val Loss: 2.9126\n",
      "\n",
      "Epoch 951/1000\n",
      "----------\n",
      "train Loss: 7.4022\n",
      "val Loss: 11.7487\n",
      "\n",
      "Epoch 952/1000\n",
      "----------\n",
      "train Loss: 10.0972\n",
      "val Loss: 119.0140\n",
      "\n",
      "Epoch 953/1000\n",
      "----------\n",
      "train Loss: 20.5915\n",
      "val Loss: 57.1505\n",
      "\n",
      "Epoch 954/1000\n",
      "----------\n",
      "train Loss: 34.0080\n",
      "val Loss: 162.9485\n",
      "\n",
      "Epoch 955/1000\n",
      "----------\n",
      "train Loss: 42.5569\n",
      "val Loss: 17.6045\n",
      "\n",
      "Epoch 956/1000\n",
      "----------\n",
      "train Loss: 8.1874\n",
      "val Loss: 60.7795\n",
      "\n",
      "Epoch 957/1000\n",
      "----------\n",
      "train Loss: 11.0780\n",
      "val Loss: 18.8543\n",
      "\n",
      "Epoch 958/1000\n",
      "----------\n",
      "train Loss: 17.0973\n",
      "val Loss: 20.1882\n",
      "\n",
      "Epoch 959/1000\n",
      "----------\n",
      "train Loss: 39.2337\n",
      "val Loss: 16.4329\n",
      "\n",
      "Epoch 960/1000\n",
      "----------\n",
      "train Loss: 22.8569\n",
      "val Loss: 53.2326\n",
      "\n",
      "Epoch 961/1000\n",
      "----------\n",
      "train Loss: 8.5695\n",
      "val Loss: 0.7597\n",
      "\n",
      "Epoch 962/1000\n",
      "----------\n",
      "train Loss: 24.2224\n",
      "val Loss: 20.3379\n",
      "\n",
      "Epoch 963/1000\n",
      "----------\n",
      "train Loss: 10.8522\n",
      "val Loss: 54.9276\n",
      "\n",
      "Epoch 964/1000\n",
      "----------\n",
      "train Loss: 10.8029\n",
      "val Loss: 29.7238\n",
      "\n",
      "Epoch 965/1000\n",
      "----------\n",
      "train Loss: 8.8919\n",
      "val Loss: 113.5520\n",
      "\n",
      "Epoch 966/1000\n",
      "----------\n",
      "train Loss: 22.8987\n",
      "val Loss: 23.7709\n",
      "\n",
      "Epoch 967/1000\n",
      "----------\n",
      "train Loss: 13.3329\n",
      "val Loss: 86.5813\n",
      "\n",
      "Epoch 968/1000\n",
      "----------\n",
      "train Loss: 31.0461\n",
      "val Loss: 202.9158\n",
      "\n",
      "Epoch 969/1000\n",
      "----------\n",
      "train Loss: 17.4734\n",
      "val Loss: 62.5818\n",
      "\n",
      "Epoch 970/1000\n",
      "----------\n",
      "train Loss: 16.1813\n",
      "val Loss: 123.8767\n",
      "\n",
      "Epoch 971/1000\n",
      "----------\n",
      "train Loss: 7.6414\n",
      "val Loss: 1.9757\n",
      "\n",
      "Epoch 972/1000\n",
      "----------\n",
      "train Loss: 17.5657\n",
      "val Loss: 14.4622\n",
      "\n",
      "Epoch 973/1000\n",
      "----------\n",
      "train Loss: 15.0246\n",
      "val Loss: 17.7270\n",
      "\n",
      "Epoch 974/1000\n",
      "----------\n",
      "train Loss: 39.5003\n",
      "val Loss: 77.6881\n",
      "\n",
      "Epoch 975/1000\n",
      "----------\n",
      "train Loss: 18.8912\n",
      "val Loss: 4.5191\n",
      "\n",
      "Epoch 976/1000\n",
      "----------\n",
      "train Loss: 20.5813\n",
      "val Loss: 9.5415\n",
      "\n",
      "Epoch 977/1000\n",
      "----------\n",
      "train Loss: 25.9186\n",
      "val Loss: 22.6744\n",
      "\n",
      "Epoch 978/1000\n",
      "----------\n",
      "train Loss: 20.2324\n",
      "val Loss: 232.8055\n",
      "\n",
      "Epoch 979/1000\n",
      "----------\n",
      "train Loss: 13.8453\n",
      "val Loss: 12.5176\n",
      "\n",
      "Epoch 980/1000\n",
      "----------\n",
      "train Loss: 11.2694\n",
      "val Loss: 9.7409\n",
      "\n",
      "Epoch 981/1000\n",
      "----------\n",
      "train Loss: 7.8107\n",
      "val Loss: 4.1193\n",
      "\n",
      "Epoch 982/1000\n",
      "----------\n",
      "train Loss: 56.0654\n",
      "val Loss: 4.6715\n",
      "\n",
      "Epoch 983/1000\n",
      "----------\n",
      "train Loss: 23.7801\n",
      "val Loss: 81.9342\n",
      "\n",
      "Epoch 984/1000\n",
      "----------\n",
      "train Loss: 9.2632\n",
      "val Loss: 44.7083\n",
      "\n",
      "Epoch 985/1000\n",
      "----------\n",
      "train Loss: 19.8593\n",
      "val Loss: 14.5768\n",
      "\n",
      "Epoch 986/1000\n",
      "----------\n",
      "train Loss: 25.0635\n",
      "val Loss: 185.7879\n",
      "\n",
      "Epoch 987/1000\n",
      "----------\n",
      "train Loss: 66.6009\n",
      "val Loss: 300.4352\n",
      "\n",
      "Epoch 988/1000\n",
      "----------\n",
      "train Loss: 106.3448\n",
      "val Loss: 184.6707\n",
      "\n",
      "Epoch 989/1000\n",
      "----------\n",
      "train Loss: 44.7233\n",
      "val Loss: 95.9955\n",
      "\n",
      "Epoch 990/1000\n",
      "----------\n",
      "train Loss: 18.3353\n",
      "val Loss: 3.4007\n",
      "\n",
      "Epoch 991/1000\n",
      "----------\n",
      "train Loss: 20.6356\n",
      "val Loss: 2.8681\n",
      "\n",
      "Epoch 992/1000\n",
      "----------\n",
      "train Loss: 12.2926\n",
      "val Loss: 90.4658\n",
      "\n",
      "Epoch 993/1000\n",
      "----------\n",
      "train Loss: 9.1447\n",
      "val Loss: 208.5887\n",
      "\n",
      "Epoch 994/1000\n",
      "----------\n",
      "train Loss: 29.5372\n",
      "val Loss: 5.6370\n",
      "\n",
      "Epoch 995/1000\n",
      "----------\n",
      "train Loss: 25.7652\n",
      "val Loss: 75.3381\n",
      "\n",
      "Epoch 996/1000\n",
      "----------\n",
      "train Loss: 59.4878\n",
      "val Loss: 593.4522\n",
      "\n",
      "Epoch 997/1000\n",
      "----------\n",
      "train Loss: 93.7324\n",
      "val Loss: 225.3258\n",
      "\n",
      "Epoch 998/1000\n",
      "----------\n",
      "train Loss: 55.0480\n",
      "val Loss: 85.6707\n",
      "\n",
      "Epoch 999/1000\n",
      "----------\n",
      "train Loss: 23.1330\n",
      "val Loss: 10.6048\n",
      "\n",
      "Epoch 1000/1000\n",
      "----------\n",
      "train Loss: 28.8689\n",
      "val Loss: 8.0172\n",
      "\n",
      "Training complete in 16m 26s\n",
      "Best val Loss: 0.759693\n",
      "model saved to ./checkpoints/A_mixed_model\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "----------\n",
      "train Loss: 120837.4471\n",
      "val Loss: 118681.8200\n",
      "\n",
      "Epoch 2/1000\n",
      "----------\n",
      "train Loss: 58308.4567\n",
      "val Loss: 45345.5575\n",
      "\n",
      "Epoch 3/1000\n",
      "----------\n",
      "train Loss: 8263.0048\n",
      "val Loss: 32409.0547\n",
      "\n",
      "Epoch 4/1000\n",
      "----------\n",
      "train Loss: 2000.3817\n",
      "val Loss: 2967.8262\n",
      "\n",
      "Epoch 5/1000\n",
      "----------\n",
      "train Loss: 1375.8473\n",
      "val Loss: 1519.9564\n",
      "\n",
      "Epoch 6/1000\n",
      "----------\n",
      "train Loss: 1233.0832\n",
      "val Loss: 1436.7313\n",
      "\n",
      "Epoch 7/1000\n",
      "----------\n",
      "train Loss: 1182.1460\n",
      "val Loss: 1375.8347\n",
      "\n",
      "Epoch 8/1000\n",
      "----------\n",
      "train Loss: 1146.0200\n",
      "val Loss: 1325.2719\n",
      "\n",
      "Epoch 9/1000\n",
      "----------\n",
      "train Loss: 1104.2931\n",
      "val Loss: 1274.2921\n",
      "\n",
      "Epoch 10/1000\n",
      "----------\n",
      "train Loss: 1067.7480\n",
      "val Loss: 1255.6732\n",
      "\n",
      "Epoch 11/1000\n",
      "----------\n",
      "train Loss: 1031.1618\n",
      "val Loss: 1217.6536\n",
      "\n",
      "Epoch 12/1000\n",
      "----------\n",
      "train Loss: 996.6762\n",
      "val Loss: 1177.2705\n",
      "\n",
      "Epoch 13/1000\n",
      "----------\n",
      "train Loss: 964.7393\n",
      "val Loss: 1144.5152\n",
      "\n",
      "Epoch 14/1000\n",
      "----------\n",
      "train Loss: 930.8439\n",
      "val Loss: 1092.2935\n",
      "\n",
      "Epoch 15/1000\n",
      "----------\n",
      "train Loss: 900.4624\n",
      "val Loss: 1075.4863\n",
      "\n",
      "Epoch 16/1000\n",
      "----------\n",
      "train Loss: 871.5306\n",
      "val Loss: 1069.7879\n",
      "\n",
      "Epoch 17/1000\n",
      "----------\n",
      "train Loss: 851.6344\n",
      "val Loss: 1036.5479\n",
      "\n",
      "Epoch 18/1000\n",
      "----------\n",
      "train Loss: 830.4760\n",
      "val Loss: 998.8530\n",
      "\n",
      "Epoch 19/1000\n",
      "----------\n",
      "train Loss: 819.3803\n",
      "val Loss: 1053.3216\n",
      "\n",
      "Epoch 20/1000\n",
      "----------\n",
      "train Loss: 802.6371\n",
      "val Loss: 998.5733\n",
      "\n",
      "Epoch 21/1000\n",
      "----------\n",
      "train Loss: 784.5822\n",
      "val Loss: 944.5154\n",
      "\n",
      "Epoch 22/1000\n",
      "----------\n",
      "train Loss: 774.1428\n",
      "val Loss: 940.3433\n",
      "\n",
      "Epoch 23/1000\n",
      "----------\n",
      "train Loss: 763.4989\n",
      "val Loss: 971.9819\n",
      "\n",
      "Epoch 24/1000\n",
      "----------\n",
      "train Loss: 760.9745\n",
      "val Loss: 991.4603\n",
      "\n",
      "Epoch 25/1000\n",
      "----------\n",
      "train Loss: 765.7909\n",
      "val Loss: 919.1159\n",
      "\n",
      "Epoch 26/1000\n",
      "----------\n",
      "train Loss: 746.2451\n",
      "val Loss: 938.0821\n",
      "\n",
      "Epoch 27/1000\n",
      "----------\n",
      "train Loss: 745.8031\n",
      "val Loss: 933.2841\n",
      "\n",
      "Epoch 28/1000\n",
      "----------\n",
      "train Loss: 756.1799\n",
      "val Loss: 947.0717\n",
      "\n",
      "Epoch 29/1000\n",
      "----------\n",
      "train Loss: 739.1789\n",
      "val Loss: 915.5576\n",
      "\n",
      "Epoch 30/1000\n",
      "----------\n",
      "train Loss: 733.6665\n",
      "val Loss: 931.6412\n",
      "\n",
      "Epoch 31/1000\n",
      "----------\n",
      "train Loss: 741.7934\n",
      "val Loss: 916.0738\n",
      "\n",
      "Epoch 32/1000\n",
      "----------\n",
      "train Loss: 726.6428\n",
      "val Loss: 911.3925\n",
      "\n",
      "Epoch 33/1000\n",
      "----------\n",
      "train Loss: 725.5371\n",
      "val Loss: 924.7648\n",
      "\n",
      "Epoch 34/1000\n",
      "----------\n",
      "train Loss: 743.6442\n",
      "val Loss: 931.8243\n",
      "\n",
      "Epoch 35/1000\n",
      "----------\n",
      "train Loss: 732.9349\n",
      "val Loss: 898.2791\n",
      "\n",
      "Epoch 36/1000\n",
      "----------\n",
      "train Loss: 728.8145\n",
      "val Loss: 922.8896\n",
      "\n",
      "Epoch 37/1000\n",
      "----------\n",
      "train Loss: 729.2775\n",
      "val Loss: 902.9331\n",
      "\n",
      "Epoch 38/1000\n",
      "----------\n",
      "train Loss: 719.2028\n",
      "val Loss: 902.7125\n",
      "\n",
      "Epoch 39/1000\n",
      "----------\n",
      "train Loss: 717.1886\n",
      "val Loss: 943.0928\n",
      "\n",
      "Epoch 40/1000\n",
      "----------\n",
      "train Loss: 731.4548\n",
      "val Loss: 897.7090\n",
      "\n",
      "Epoch 41/1000\n",
      "----------\n",
      "train Loss: 722.3043\n",
      "val Loss: 897.1620\n",
      "\n",
      "Epoch 42/1000\n",
      "----------\n",
      "train Loss: 714.3112\n",
      "val Loss: 893.1261\n",
      "\n",
      "Epoch 43/1000\n",
      "----------\n",
      "train Loss: 718.5383\n",
      "val Loss: 912.7120\n",
      "\n",
      "Epoch 44/1000\n",
      "----------\n",
      "train Loss: 712.4683\n",
      "val Loss: 884.4847\n",
      "\n",
      "Epoch 45/1000\n",
      "----------\n",
      "train Loss: 707.5000\n",
      "val Loss: 868.3957\n",
      "\n",
      "Epoch 46/1000\n",
      "----------\n",
      "train Loss: 712.3639\n",
      "val Loss: 887.4453\n",
      "\n",
      "Epoch 47/1000\n",
      "----------\n",
      "train Loss: 719.9222\n",
      "val Loss: 866.5699\n",
      "\n",
      "Epoch 48/1000\n",
      "----------\n",
      "train Loss: 708.0834\n",
      "val Loss: 881.4671\n",
      "\n",
      "Epoch 49/1000\n",
      "----------\n",
      "train Loss: 715.9696\n",
      "val Loss: 909.3106\n",
      "\n",
      "Epoch 50/1000\n",
      "----------\n",
      "train Loss: 707.7709\n",
      "val Loss: 918.1529\n",
      "\n",
      "Epoch 51/1000\n",
      "----------\n",
      "train Loss: 700.2862\n",
      "val Loss: 902.2622\n",
      "\n",
      "Epoch 52/1000\n",
      "----------\n",
      "train Loss: 703.1025\n",
      "val Loss: 943.7412\n",
      "\n",
      "Epoch 53/1000\n",
      "----------\n",
      "train Loss: 704.4685\n",
      "val Loss: 874.6015\n",
      "\n",
      "Epoch 54/1000\n",
      "----------\n",
      "train Loss: 690.4851\n",
      "val Loss: 869.4105\n",
      "\n",
      "Epoch 55/1000\n",
      "----------\n",
      "train Loss: 692.6158\n",
      "val Loss: 852.7847\n",
      "\n",
      "Epoch 56/1000\n",
      "----------\n",
      "train Loss: 698.1249\n",
      "val Loss: 836.5944\n",
      "\n",
      "Epoch 57/1000\n",
      "----------\n",
      "train Loss: 669.9080\n",
      "val Loss: 816.5480\n",
      "\n",
      "Epoch 58/1000\n",
      "----------\n",
      "train Loss: 654.0933\n",
      "val Loss: 812.3601\n",
      "\n",
      "Epoch 59/1000\n",
      "----------\n",
      "train Loss: 617.4922\n",
      "val Loss: 696.6544\n",
      "\n",
      "Epoch 60/1000\n",
      "----------\n",
      "train Loss: 496.3836\n",
      "val Loss: 580.7578\n",
      "\n",
      "Epoch 61/1000\n",
      "----------\n",
      "train Loss: 333.9203\n",
      "val Loss: 553.0792\n",
      "\n",
      "Epoch 62/1000\n",
      "----------\n",
      "train Loss: 262.7262\n",
      "val Loss: 313.6476\n",
      "\n",
      "Epoch 63/1000\n",
      "----------\n",
      "train Loss: 212.1063\n",
      "val Loss: 496.5633\n",
      "\n",
      "Epoch 64/1000\n",
      "----------\n",
      "train Loss: 158.6142\n",
      "val Loss: 186.8110\n",
      "\n",
      "Epoch 65/1000\n",
      "----------\n",
      "train Loss: 101.8933\n",
      "val Loss: 129.7054\n",
      "\n",
      "Epoch 66/1000\n",
      "----------\n",
      "train Loss: 81.3756\n",
      "val Loss: 118.8619\n",
      "\n",
      "Epoch 67/1000\n",
      "----------\n",
      "train Loss: 59.3624\n",
      "val Loss: 141.7182\n",
      "\n",
      "Epoch 68/1000\n",
      "----------\n",
      "train Loss: 43.4982\n",
      "val Loss: 64.6237\n",
      "\n",
      "Epoch 69/1000\n",
      "----------\n",
      "train Loss: 31.5849\n",
      "val Loss: 51.7615\n",
      "\n",
      "Epoch 70/1000\n",
      "----------\n",
      "train Loss: 25.9398\n",
      "val Loss: 45.1834\n",
      "\n",
      "Epoch 71/1000\n",
      "----------\n",
      "train Loss: 22.3168\n",
      "val Loss: 35.3708\n",
      "\n",
      "Epoch 72/1000\n",
      "----------\n",
      "train Loss: 17.6322\n",
      "val Loss: 39.1116\n",
      "\n",
      "Epoch 73/1000\n",
      "----------\n",
      "train Loss: 28.6514\n",
      "val Loss: 74.2880\n",
      "\n",
      "Epoch 74/1000\n",
      "----------\n",
      "train Loss: 25.1884\n",
      "val Loss: 29.0633\n",
      "\n",
      "Epoch 75/1000\n",
      "----------\n",
      "train Loss: 74.7811\n",
      "val Loss: 96.0597\n",
      "\n",
      "Epoch 76/1000\n",
      "----------\n",
      "train Loss: 93.4202\n",
      "val Loss: 124.8212\n",
      "\n",
      "Epoch 77/1000\n",
      "----------\n",
      "train Loss: 38.3238\n",
      "val Loss: 67.8383\n",
      "\n",
      "Epoch 78/1000\n",
      "----------\n",
      "train Loss: 25.8692\n",
      "val Loss: 39.3670\n",
      "\n",
      "Epoch 79/1000\n",
      "----------\n",
      "train Loss: 31.5116\n",
      "val Loss: 20.1305\n",
      "\n",
      "Epoch 80/1000\n",
      "----------\n",
      "train Loss: 14.5435\n",
      "val Loss: 57.7346\n",
      "\n",
      "Epoch 81/1000\n",
      "----------\n",
      "train Loss: 29.7124\n",
      "val Loss: 27.7654\n",
      "\n",
      "Epoch 82/1000\n",
      "----------\n",
      "train Loss: 21.9272\n",
      "val Loss: 36.6250\n",
      "\n",
      "Epoch 83/1000\n",
      "----------\n",
      "train Loss: 20.1006\n",
      "val Loss: 137.6372\n",
      "\n",
      "Epoch 84/1000\n",
      "----------\n",
      "train Loss: 30.1697\n",
      "val Loss: 20.8434\n",
      "\n",
      "Epoch 85/1000\n",
      "----------\n",
      "train Loss: 14.9732\n",
      "val Loss: 44.6636\n",
      "\n",
      "Epoch 86/1000\n",
      "----------\n",
      "train Loss: 42.1169\n",
      "val Loss: 57.1491\n",
      "\n",
      "Epoch 87/1000\n",
      "----------\n",
      "train Loss: 24.6206\n",
      "val Loss: 96.1344\n",
      "\n",
      "Epoch 88/1000\n",
      "----------\n",
      "train Loss: 24.4798\n",
      "val Loss: 22.1657\n",
      "\n",
      "Epoch 89/1000\n",
      "----------\n",
      "train Loss: 34.0407\n",
      "val Loss: 138.2490\n",
      "\n",
      "Epoch 90/1000\n",
      "----------\n",
      "train Loss: 31.5627\n",
      "val Loss: 20.6710\n",
      "\n",
      "Epoch 91/1000\n",
      "----------\n",
      "train Loss: 10.9297\n",
      "val Loss: 15.6761\n",
      "\n",
      "Epoch 92/1000\n",
      "----------\n",
      "train Loss: 43.0242\n",
      "val Loss: 20.3343\n",
      "\n",
      "Epoch 93/1000\n",
      "----------\n",
      "train Loss: 17.5893\n",
      "val Loss: 14.5946\n",
      "\n",
      "Epoch 94/1000\n",
      "----------\n",
      "train Loss: 11.3749\n",
      "val Loss: 14.5377\n",
      "\n",
      "Epoch 95/1000\n",
      "----------\n",
      "train Loss: 17.5653\n",
      "val Loss: 11.8685\n",
      "\n",
      "Epoch 96/1000\n",
      "----------\n",
      "train Loss: 15.6208\n",
      "val Loss: 10.6582\n",
      "\n",
      "Epoch 97/1000\n",
      "----------\n",
      "train Loss: 22.5933\n",
      "val Loss: 31.7464\n",
      "\n",
      "Epoch 98/1000\n",
      "----------\n",
      "train Loss: 12.4199\n",
      "val Loss: 23.8346\n",
      "\n",
      "Epoch 99/1000\n",
      "----------\n",
      "train Loss: 9.5448\n",
      "val Loss: 13.6403\n",
      "\n",
      "Epoch 100/1000\n",
      "----------\n",
      "train Loss: 9.4220\n",
      "val Loss: 29.2315\n",
      "\n",
      "Epoch 101/1000\n",
      "----------\n",
      "train Loss: 15.8249\n",
      "val Loss: 34.1031\n",
      "\n",
      "Epoch 102/1000\n",
      "----------\n",
      "train Loss: 34.3786\n",
      "val Loss: 12.3050\n",
      "\n",
      "Epoch 103/1000\n",
      "----------\n",
      "train Loss: 39.7674\n",
      "val Loss: 100.5094\n",
      "\n",
      "Epoch 104/1000\n",
      "----------\n",
      "train Loss: 19.6988\n",
      "val Loss: 9.4816\n",
      "\n",
      "Epoch 105/1000\n",
      "----------\n",
      "train Loss: 9.6760\n",
      "val Loss: 21.9347\n",
      "\n",
      "Epoch 106/1000\n",
      "----------\n",
      "train Loss: 13.7066\n",
      "val Loss: 12.2104\n",
      "\n",
      "Epoch 107/1000\n",
      "----------\n",
      "train Loss: 14.8847\n",
      "val Loss: 35.0221\n",
      "\n",
      "Epoch 108/1000\n",
      "----------\n",
      "train Loss: 27.6907\n",
      "val Loss: 22.0102\n",
      "\n",
      "Epoch 109/1000\n",
      "----------\n",
      "train Loss: 13.1955\n",
      "val Loss: 13.1527\n",
      "\n",
      "Epoch 110/1000\n",
      "----------\n",
      "train Loss: 15.5043\n",
      "val Loss: 10.0771\n",
      "\n",
      "Epoch 111/1000\n",
      "----------\n",
      "train Loss: 22.7551\n",
      "val Loss: 31.4735\n",
      "\n",
      "Epoch 112/1000\n",
      "----------\n",
      "train Loss: 7.0597\n",
      "val Loss: 12.4050\n",
      "\n",
      "Epoch 113/1000\n",
      "----------\n",
      "train Loss: 8.6684\n",
      "val Loss: 14.2206\n",
      "\n",
      "Epoch 114/1000\n",
      "----------\n",
      "train Loss: 22.1337\n",
      "val Loss: 13.3670\n",
      "\n",
      "Epoch 115/1000\n",
      "----------\n",
      "train Loss: 35.8133\n",
      "val Loss: 68.0191\n",
      "\n",
      "Epoch 116/1000\n",
      "----------\n",
      "train Loss: 44.4438\n",
      "val Loss: 23.0258\n",
      "\n",
      "Epoch 117/1000\n",
      "----------\n",
      "train Loss: 29.7125\n",
      "val Loss: 143.5780\n",
      "\n",
      "Epoch 118/1000\n",
      "----------\n",
      "train Loss: 31.9086\n",
      "val Loss: 8.2305\n",
      "\n",
      "Epoch 119/1000\n",
      "----------\n",
      "train Loss: 12.3446\n",
      "val Loss: 22.5434\n",
      "\n",
      "Epoch 120/1000\n",
      "----------\n",
      "train Loss: 20.6978\n",
      "val Loss: 27.0236\n",
      "\n",
      "Epoch 121/1000\n",
      "----------\n",
      "train Loss: 8.1901\n",
      "val Loss: 9.4900\n",
      "\n",
      "Epoch 122/1000\n",
      "----------\n",
      "train Loss: 12.3622\n",
      "val Loss: 41.2543\n",
      "\n",
      "Epoch 123/1000\n",
      "----------\n",
      "train Loss: 25.1688\n",
      "val Loss: 80.8882\n",
      "\n",
      "Epoch 124/1000\n",
      "----------\n",
      "train Loss: 12.8071\n",
      "val Loss: 8.7883\n",
      "\n",
      "Epoch 125/1000\n",
      "----------\n",
      "train Loss: 30.3294\n",
      "val Loss: 21.2796\n",
      "\n",
      "Epoch 126/1000\n",
      "----------\n",
      "train Loss: 14.1859\n",
      "val Loss: 13.6007\n",
      "\n",
      "Epoch 127/1000\n",
      "----------\n",
      "train Loss: 16.4513\n",
      "val Loss: 31.5169\n",
      "\n",
      "Epoch 128/1000\n",
      "----------\n",
      "train Loss: 14.8906\n",
      "val Loss: 43.1976\n",
      "\n",
      "Epoch 129/1000\n",
      "----------\n",
      "train Loss: 24.7958\n",
      "val Loss: 23.4728\n",
      "\n",
      "Epoch 130/1000\n",
      "----------\n",
      "train Loss: 35.1060\n",
      "val Loss: 83.1067\n",
      "\n",
      "Epoch 131/1000\n",
      "----------\n",
      "train Loss: 87.2649\n",
      "val Loss: 180.5733\n",
      "\n",
      "Epoch 132/1000\n",
      "----------\n",
      "train Loss: 50.1741\n",
      "val Loss: 8.4922\n",
      "\n",
      "Epoch 133/1000\n",
      "----------\n",
      "train Loss: 19.4715\n",
      "val Loss: 11.9938\n",
      "\n",
      "Epoch 134/1000\n",
      "----------\n",
      "train Loss: 16.7767\n",
      "val Loss: 11.4344\n",
      "\n",
      "Epoch 135/1000\n",
      "----------\n",
      "train Loss: 20.7997\n",
      "val Loss: 10.1842\n",
      "\n",
      "Epoch 136/1000\n",
      "----------\n",
      "train Loss: 28.9073\n",
      "val Loss: 33.7693\n",
      "\n",
      "Epoch 137/1000\n",
      "----------\n",
      "train Loss: 8.4428\n",
      "val Loss: 13.9619\n",
      "\n",
      "Epoch 138/1000\n",
      "----------\n",
      "train Loss: 18.8981\n",
      "val Loss: 11.3983\n",
      "\n",
      "Epoch 139/1000\n",
      "----------\n",
      "train Loss: 5.4336\n",
      "val Loss: 39.1858\n",
      "\n",
      "Epoch 140/1000\n",
      "----------\n",
      "train Loss: 8.2526\n",
      "val Loss: 8.8463\n",
      "\n",
      "Epoch 141/1000\n",
      "----------\n",
      "train Loss: 14.5397\n",
      "val Loss: 11.1918\n",
      "\n",
      "Epoch 142/1000\n",
      "----------\n",
      "train Loss: 7.6930\n",
      "val Loss: 14.4554\n",
      "\n",
      "Epoch 143/1000\n",
      "----------\n",
      "train Loss: 19.5749\n",
      "val Loss: 14.8743\n",
      "\n",
      "Epoch 144/1000\n",
      "----------\n",
      "train Loss: 37.1109\n",
      "val Loss: 47.6698\n",
      "\n",
      "Epoch 145/1000\n",
      "----------\n",
      "train Loss: 7.8668\n",
      "val Loss: 28.5971\n",
      "\n",
      "Epoch 146/1000\n",
      "----------\n",
      "train Loss: 9.3703\n",
      "val Loss: 8.1417\n",
      "\n",
      "Epoch 147/1000\n",
      "----------\n",
      "train Loss: 25.3042\n",
      "val Loss: 83.1653\n",
      "\n",
      "Epoch 148/1000\n",
      "----------\n",
      "train Loss: 20.8160\n",
      "val Loss: 36.4304\n",
      "\n",
      "Epoch 149/1000\n",
      "----------\n",
      "train Loss: 13.7534\n",
      "val Loss: 51.4908\n",
      "\n",
      "Epoch 150/1000\n",
      "----------\n",
      "train Loss: 23.3406\n",
      "val Loss: 8.3179\n",
      "\n",
      "Epoch 151/1000\n",
      "----------\n",
      "train Loss: 9.4518\n",
      "val Loss: 81.5875\n",
      "\n",
      "Epoch 152/1000\n",
      "----------\n",
      "train Loss: 74.8484\n",
      "val Loss: 171.7672\n",
      "\n",
      "Epoch 153/1000\n",
      "----------\n",
      "train Loss: 52.9815\n",
      "val Loss: 21.7185\n",
      "\n",
      "Epoch 154/1000\n",
      "----------\n",
      "train Loss: 9.6413\n",
      "val Loss: 47.2834\n",
      "\n",
      "Epoch 155/1000\n",
      "----------\n",
      "train Loss: 16.3298\n",
      "val Loss: 9.2263\n",
      "\n",
      "Epoch 156/1000\n",
      "----------\n",
      "train Loss: 35.5905\n",
      "val Loss: 20.9180\n",
      "\n",
      "Epoch 157/1000\n",
      "----------\n",
      "train Loss: 12.7577\n",
      "val Loss: 9.7439\n",
      "\n",
      "Epoch 158/1000\n",
      "----------\n",
      "train Loss: 17.0945\n",
      "val Loss: 63.2008\n",
      "\n",
      "Epoch 159/1000\n",
      "----------\n",
      "train Loss: 12.9186\n",
      "val Loss: 7.7667\n",
      "\n",
      "Epoch 160/1000\n",
      "----------\n",
      "train Loss: 20.2232\n",
      "val Loss: 10.2560\n",
      "\n",
      "Epoch 161/1000\n",
      "----------\n",
      "train Loss: 16.7366\n",
      "val Loss: 39.6508\n",
      "\n",
      "Epoch 162/1000\n",
      "----------\n",
      "train Loss: 11.1387\n",
      "val Loss: 25.2897\n",
      "\n",
      "Epoch 163/1000\n",
      "----------\n",
      "train Loss: 51.4362\n",
      "val Loss: 32.4556\n",
      "\n",
      "Epoch 164/1000\n",
      "----------\n",
      "train Loss: 13.1238\n",
      "val Loss: 22.2613\n",
      "\n",
      "Epoch 165/1000\n",
      "----------\n",
      "train Loss: 23.5655\n",
      "val Loss: 15.4129\n",
      "\n",
      "Epoch 166/1000\n",
      "----------\n",
      "train Loss: 53.8737\n",
      "val Loss: 93.2502\n",
      "\n",
      "Epoch 167/1000\n",
      "----------\n",
      "train Loss: 25.7262\n",
      "val Loss: 14.7282\n",
      "\n",
      "Epoch 168/1000\n",
      "----------\n",
      "train Loss: 12.7436\n",
      "val Loss: 13.1481\n",
      "\n",
      "Epoch 169/1000\n",
      "----------\n",
      "train Loss: 7.3483\n",
      "val Loss: 12.1572\n",
      "\n",
      "Epoch 170/1000\n",
      "----------\n",
      "train Loss: 8.5362\n",
      "val Loss: 11.7078\n",
      "\n",
      "Epoch 171/1000\n",
      "----------\n",
      "train Loss: 21.6031\n",
      "val Loss: 7.7527\n",
      "\n",
      "Epoch 172/1000\n",
      "----------\n",
      "train Loss: 29.9670\n",
      "val Loss: 145.3611\n",
      "\n",
      "Epoch 173/1000\n",
      "----------\n",
      "train Loss: 29.6044\n",
      "val Loss: 114.4768\n",
      "\n",
      "Epoch 174/1000\n",
      "----------\n",
      "train Loss: 17.8266\n",
      "val Loss: 12.7313\n",
      "\n",
      "Epoch 175/1000\n",
      "----------\n",
      "train Loss: 34.7541\n",
      "val Loss: 16.9654\n",
      "\n",
      "Epoch 176/1000\n",
      "----------\n",
      "train Loss: 19.1280\n",
      "val Loss: 17.9365\n",
      "\n",
      "Epoch 177/1000\n",
      "----------\n",
      "train Loss: 41.9445\n",
      "val Loss: 40.7727\n",
      "\n",
      "Epoch 178/1000\n",
      "----------\n",
      "train Loss: 25.7365\n",
      "val Loss: 63.4203\n",
      "\n",
      "Epoch 179/1000\n",
      "----------\n",
      "train Loss: 38.4205\n",
      "val Loss: 39.4754\n",
      "\n",
      "Epoch 180/1000\n",
      "----------\n",
      "train Loss: 36.6064\n",
      "val Loss: 11.9310\n",
      "\n",
      "Epoch 181/1000\n",
      "----------\n",
      "train Loss: 15.2080\n",
      "val Loss: 7.4785\n",
      "\n",
      "Epoch 182/1000\n",
      "----------\n",
      "train Loss: 19.7060\n",
      "val Loss: 15.9657\n",
      "\n",
      "Epoch 183/1000\n",
      "----------\n",
      "train Loss: 18.4854\n",
      "val Loss: 18.0878\n",
      "\n",
      "Epoch 184/1000\n",
      "----------\n",
      "train Loss: 18.8532\n",
      "val Loss: 18.2873\n",
      "\n",
      "Epoch 185/1000\n",
      "----------\n",
      "train Loss: 6.1631\n",
      "val Loss: 8.4417\n",
      "\n",
      "Epoch 186/1000\n",
      "----------\n",
      "train Loss: 28.5193\n",
      "val Loss: 88.6983\n",
      "\n",
      "Epoch 187/1000\n",
      "----------\n",
      "train Loss: 23.9204\n",
      "val Loss: 46.2350\n",
      "\n",
      "Epoch 188/1000\n",
      "----------\n",
      "train Loss: 8.0621\n",
      "val Loss: 10.3002\n",
      "\n",
      "Epoch 189/1000\n",
      "----------\n",
      "train Loss: 8.7820\n",
      "val Loss: 7.1194\n",
      "\n",
      "Epoch 190/1000\n",
      "----------\n",
      "train Loss: 11.7657\n",
      "val Loss: 17.4046\n",
      "\n",
      "Epoch 191/1000\n",
      "----------\n",
      "train Loss: 8.3604\n",
      "val Loss: 21.0632\n",
      "\n",
      "Epoch 192/1000\n",
      "----------\n",
      "train Loss: 7.8170\n",
      "val Loss: 67.7179\n",
      "\n",
      "Epoch 193/1000\n",
      "----------\n",
      "train Loss: 37.0636\n",
      "val Loss: 55.1449\n",
      "\n",
      "Epoch 194/1000\n",
      "----------\n",
      "train Loss: 13.6341\n",
      "val Loss: 82.3803\n",
      "\n",
      "Epoch 195/1000\n",
      "----------\n",
      "train Loss: 14.0787\n",
      "val Loss: 7.0655\n",
      "\n",
      "Epoch 196/1000\n",
      "----------\n",
      "train Loss: 10.1962\n",
      "val Loss: 6.1578\n",
      "\n",
      "Epoch 197/1000\n",
      "----------\n",
      "train Loss: 29.6651\n",
      "val Loss: 8.2865\n",
      "\n",
      "Epoch 198/1000\n",
      "----------\n",
      "train Loss: 13.7584\n",
      "val Loss: 6.6914\n",
      "\n",
      "Epoch 199/1000\n",
      "----------\n",
      "train Loss: 5.5413\n",
      "val Loss: 17.8605\n",
      "\n",
      "Epoch 200/1000\n",
      "----------\n",
      "train Loss: 13.3304\n",
      "val Loss: 8.9952\n",
      "\n",
      "Epoch 201/1000\n",
      "----------\n",
      "train Loss: 19.1327\n",
      "val Loss: 37.9871\n",
      "\n",
      "Epoch 202/1000\n",
      "----------\n",
      "train Loss: 24.2419\n",
      "val Loss: 17.8854\n",
      "\n",
      "Epoch 203/1000\n",
      "----------\n",
      "train Loss: 31.8996\n",
      "val Loss: 70.2238\n",
      "\n",
      "Epoch 204/1000\n",
      "----------\n",
      "train Loss: 46.2644\n",
      "val Loss: 42.8477\n",
      "\n",
      "Epoch 205/1000\n",
      "----------\n",
      "train Loss: 20.9286\n",
      "val Loss: 22.9816\n",
      "\n",
      "Epoch 206/1000\n",
      "----------\n",
      "train Loss: 33.2632\n",
      "val Loss: 10.1652\n",
      "\n",
      "Epoch 207/1000\n",
      "----------\n",
      "train Loss: 55.3649\n",
      "val Loss: 49.9405\n",
      "\n",
      "Epoch 208/1000\n",
      "----------\n",
      "train Loss: 32.3170\n",
      "val Loss: 36.4597\n",
      "\n",
      "Epoch 209/1000\n",
      "----------\n",
      "train Loss: 11.2167\n",
      "val Loss: 8.3367\n",
      "\n",
      "Epoch 210/1000\n",
      "----------\n",
      "train Loss: 14.6002\n",
      "val Loss: 31.6634\n",
      "\n",
      "Epoch 211/1000\n",
      "----------\n",
      "train Loss: 14.1012\n",
      "val Loss: 178.9611\n",
      "\n",
      "Epoch 212/1000\n",
      "----------\n",
      "train Loss: 35.5965\n",
      "val Loss: 8.4891\n",
      "\n",
      "Epoch 213/1000\n",
      "----------\n",
      "train Loss: 33.5325\n",
      "val Loss: 13.7173\n",
      "\n",
      "Epoch 214/1000\n",
      "----------\n",
      "train Loss: 24.3665\n",
      "val Loss: 21.4708\n",
      "\n",
      "Epoch 215/1000\n",
      "----------\n",
      "train Loss: 8.3873\n",
      "val Loss: 10.1387\n",
      "\n",
      "Epoch 216/1000\n",
      "----------\n",
      "train Loss: 5.6527\n",
      "val Loss: 6.4298\n",
      "\n",
      "Epoch 217/1000\n",
      "----------\n",
      "train Loss: 13.6302\n",
      "val Loss: 6.5734\n",
      "\n",
      "Epoch 218/1000\n",
      "----------\n",
      "train Loss: 17.9196\n",
      "val Loss: 7.8677\n",
      "\n",
      "Epoch 219/1000\n",
      "----------\n",
      "train Loss: 17.7993\n",
      "val Loss: 25.7043\n",
      "\n",
      "Epoch 220/1000\n",
      "----------\n",
      "train Loss: 47.6256\n",
      "val Loss: 179.3899\n",
      "\n",
      "Epoch 221/1000\n",
      "----------\n",
      "train Loss: 46.4394\n",
      "val Loss: 8.9186\n",
      "\n",
      "Epoch 222/1000\n",
      "----------\n",
      "train Loss: 14.2830\n",
      "val Loss: 30.0962\n",
      "\n",
      "Epoch 223/1000\n",
      "----------\n",
      "train Loss: 7.8182\n",
      "val Loss: 8.3754\n",
      "\n",
      "Epoch 224/1000\n",
      "----------\n",
      "train Loss: 10.2282\n",
      "val Loss: 7.5953\n",
      "\n",
      "Epoch 225/1000\n",
      "----------\n",
      "train Loss: 12.2536\n",
      "val Loss: 35.8758\n",
      "\n",
      "Epoch 226/1000\n",
      "----------\n",
      "train Loss: 30.3671\n",
      "val Loss: 21.9933\n",
      "\n",
      "Epoch 227/1000\n",
      "----------\n",
      "train Loss: 40.0791\n",
      "val Loss: 7.5105\n",
      "\n",
      "Epoch 228/1000\n",
      "----------\n",
      "train Loss: 38.5124\n",
      "val Loss: 6.5252\n",
      "\n",
      "Epoch 229/1000\n",
      "----------\n",
      "train Loss: 53.8844\n",
      "val Loss: 9.5299\n",
      "\n",
      "Epoch 230/1000\n",
      "----------\n",
      "train Loss: 46.3851\n",
      "val Loss: 35.4442\n",
      "\n",
      "Epoch 231/1000\n",
      "----------\n",
      "train Loss: 10.4854\n",
      "val Loss: 11.2631\n",
      "\n",
      "Epoch 232/1000\n",
      "----------\n",
      "train Loss: 9.1883\n",
      "val Loss: 18.2557\n",
      "\n",
      "Epoch 233/1000\n",
      "----------\n",
      "train Loss: 7.1324\n",
      "val Loss: 5.3749\n",
      "\n",
      "Epoch 234/1000\n",
      "----------\n",
      "train Loss: 10.5594\n",
      "val Loss: 14.0041\n",
      "\n",
      "Epoch 235/1000\n",
      "----------\n",
      "train Loss: 22.3208\n",
      "val Loss: 7.8760\n",
      "\n",
      "Epoch 236/1000\n",
      "----------\n",
      "train Loss: 23.4025\n",
      "val Loss: 14.0423\n",
      "\n",
      "Epoch 237/1000\n",
      "----------\n",
      "train Loss: 36.3151\n",
      "val Loss: 6.9595\n",
      "\n",
      "Epoch 238/1000\n",
      "----------\n",
      "train Loss: 53.2727\n",
      "val Loss: 11.4399\n",
      "\n",
      "Epoch 239/1000\n",
      "----------\n",
      "train Loss: 30.8429\n",
      "val Loss: 13.1101\n",
      "\n",
      "Epoch 240/1000\n",
      "----------\n",
      "train Loss: 11.8402\n",
      "val Loss: 9.7379\n",
      "\n",
      "Epoch 241/1000\n",
      "----------\n",
      "train Loss: 19.1047\n",
      "val Loss: 23.4132\n",
      "\n",
      "Epoch 242/1000\n",
      "----------\n",
      "train Loss: 18.8833\n",
      "val Loss: 6.7666\n",
      "\n",
      "Epoch 243/1000\n",
      "----------\n",
      "train Loss: 10.8131\n",
      "val Loss: 8.9576\n",
      "\n",
      "Epoch 244/1000\n",
      "----------\n",
      "train Loss: 6.0755\n",
      "val Loss: 34.1223\n",
      "\n",
      "Epoch 245/1000\n",
      "----------\n",
      "train Loss: 75.2079\n",
      "val Loss: 131.4710\n",
      "\n",
      "Epoch 246/1000\n",
      "----------\n",
      "train Loss: 20.0080\n",
      "val Loss: 8.9789\n",
      "\n",
      "Epoch 247/1000\n",
      "----------\n",
      "train Loss: 13.8568\n",
      "val Loss: 71.0452\n",
      "\n",
      "Epoch 248/1000\n",
      "----------\n",
      "train Loss: 9.4033\n",
      "val Loss: 14.0705\n",
      "\n",
      "Epoch 249/1000\n",
      "----------\n",
      "train Loss: 4.6448\n",
      "val Loss: 25.4086\n",
      "\n",
      "Epoch 250/1000\n",
      "----------\n",
      "train Loss: 12.0330\n",
      "val Loss: 26.2340\n",
      "\n",
      "Epoch 251/1000\n",
      "----------\n",
      "train Loss: 90.9037\n",
      "val Loss: 86.3427\n",
      "\n",
      "Epoch 252/1000\n",
      "----------\n",
      "train Loss: 56.2477\n",
      "val Loss: 15.9787\n",
      "\n",
      "Epoch 253/1000\n",
      "----------\n",
      "train Loss: 23.1798\n",
      "val Loss: 31.2276\n",
      "\n",
      "Epoch 254/1000\n",
      "----------\n",
      "train Loss: 13.3004\n",
      "val Loss: 13.1399\n",
      "\n",
      "Epoch 255/1000\n",
      "----------\n",
      "train Loss: 19.0376\n",
      "val Loss: 51.5058\n",
      "\n",
      "Epoch 256/1000\n",
      "----------\n",
      "train Loss: 11.2841\n",
      "val Loss: 152.1858\n",
      "\n",
      "Epoch 257/1000\n",
      "----------\n",
      "train Loss: 29.8195\n",
      "val Loss: 5.5913\n",
      "\n",
      "Epoch 258/1000\n",
      "----------\n",
      "train Loss: 11.5706\n",
      "val Loss: 33.5380\n",
      "\n",
      "Epoch 259/1000\n",
      "----------\n",
      "train Loss: 21.5659\n",
      "val Loss: 26.0744\n",
      "\n",
      "Epoch 260/1000\n",
      "----------\n",
      "train Loss: 17.4333\n",
      "val Loss: 43.1515\n",
      "\n",
      "Epoch 261/1000\n",
      "----------\n",
      "train Loss: 10.6790\n",
      "val Loss: 5.2003\n",
      "\n",
      "Epoch 262/1000\n",
      "----------\n",
      "train Loss: 7.9718\n",
      "val Loss: 10.1226\n",
      "\n",
      "Epoch 263/1000\n",
      "----------\n",
      "train Loss: 15.9606\n",
      "val Loss: 6.8890\n",
      "\n",
      "Epoch 264/1000\n",
      "----------\n",
      "train Loss: 40.4775\n",
      "val Loss: 7.3270\n",
      "\n",
      "Epoch 265/1000\n",
      "----------\n",
      "train Loss: 19.3373\n",
      "val Loss: 35.3969\n",
      "\n",
      "Epoch 266/1000\n",
      "----------\n",
      "train Loss: 72.6711\n",
      "val Loss: 22.2595\n",
      "\n",
      "Epoch 267/1000\n",
      "----------\n",
      "train Loss: 20.8121\n",
      "val Loss: 11.8929\n",
      "\n",
      "Epoch 268/1000\n",
      "----------\n",
      "train Loss: 14.8463\n",
      "val Loss: 14.7147\n",
      "\n",
      "Epoch 269/1000\n",
      "----------\n",
      "train Loss: 7.1864\n",
      "val Loss: 38.0396\n",
      "\n",
      "Epoch 270/1000\n",
      "----------\n",
      "train Loss: 14.0375\n",
      "val Loss: 16.1205\n",
      "\n",
      "Epoch 271/1000\n",
      "----------\n",
      "train Loss: 21.5297\n",
      "val Loss: 48.2355\n",
      "\n",
      "Epoch 272/1000\n",
      "----------\n",
      "train Loss: 8.0947\n",
      "val Loss: 9.1352\n",
      "\n",
      "Epoch 273/1000\n",
      "----------\n",
      "train Loss: 13.3176\n",
      "val Loss: 73.3771\n",
      "\n",
      "Epoch 274/1000\n",
      "----------\n",
      "train Loss: 52.6690\n",
      "val Loss: 94.1715\n",
      "\n",
      "Epoch 275/1000\n",
      "----------\n",
      "train Loss: 43.5482\n",
      "val Loss: 163.7021\n",
      "\n",
      "Epoch 276/1000\n",
      "----------\n",
      "train Loss: 54.9362\n",
      "val Loss: 126.8212\n",
      "\n",
      "Epoch 277/1000\n",
      "----------\n",
      "train Loss: 12.9101\n",
      "val Loss: 17.7225\n",
      "\n",
      "Epoch 278/1000\n",
      "----------\n",
      "train Loss: 12.6753\n",
      "val Loss: 10.8811\n",
      "\n",
      "Epoch 279/1000\n",
      "----------\n",
      "train Loss: 15.5585\n",
      "val Loss: 74.2517\n",
      "\n",
      "Epoch 280/1000\n",
      "----------\n",
      "train Loss: 11.5985\n",
      "val Loss: 5.9639\n",
      "\n",
      "Epoch 281/1000\n",
      "----------\n",
      "train Loss: 11.9848\n",
      "val Loss: 42.1972\n",
      "\n",
      "Epoch 282/1000\n",
      "----------\n",
      "train Loss: 10.4102\n",
      "val Loss: 9.9735\n",
      "\n",
      "Epoch 283/1000\n",
      "----------\n",
      "train Loss: 19.4576\n",
      "val Loss: 77.4440\n",
      "\n",
      "Epoch 284/1000\n",
      "----------\n",
      "train Loss: 25.5321\n",
      "val Loss: 16.9077\n",
      "\n",
      "Epoch 285/1000\n",
      "----------\n",
      "train Loss: 19.3329\n",
      "val Loss: 6.8800\n",
      "\n",
      "Epoch 286/1000\n",
      "----------\n",
      "train Loss: 17.7282\n",
      "val Loss: 9.8977\n",
      "\n",
      "Epoch 287/1000\n",
      "----------\n",
      "train Loss: 30.3349\n",
      "val Loss: 7.3759\n",
      "\n",
      "Epoch 288/1000\n",
      "----------\n",
      "train Loss: 13.0719\n",
      "val Loss: 12.9521\n",
      "\n",
      "Epoch 289/1000\n",
      "----------\n",
      "train Loss: 12.6061\n",
      "val Loss: 50.2708\n",
      "\n",
      "Epoch 290/1000\n",
      "----------\n",
      "train Loss: 16.0681\n",
      "val Loss: 61.4594\n",
      "\n",
      "Epoch 291/1000\n",
      "----------\n",
      "train Loss: 17.2315\n",
      "val Loss: 16.9034\n",
      "\n",
      "Epoch 292/1000\n",
      "----------\n",
      "train Loss: 6.4466\n",
      "val Loss: 7.9162\n",
      "\n",
      "Epoch 293/1000\n",
      "----------\n",
      "train Loss: 15.4460\n",
      "val Loss: 45.5339\n",
      "\n",
      "Epoch 294/1000\n",
      "----------\n",
      "train Loss: 12.9934\n",
      "val Loss: 154.7785\n",
      "\n",
      "Epoch 295/1000\n",
      "----------\n",
      "train Loss: 53.5005\n",
      "val Loss: 123.2657\n",
      "\n",
      "Epoch 296/1000\n",
      "----------\n",
      "train Loss: 60.4573\n",
      "val Loss: 75.7176\n",
      "\n",
      "Epoch 297/1000\n",
      "----------\n",
      "train Loss: 16.5551\n",
      "val Loss: 21.9325\n",
      "\n",
      "Epoch 298/1000\n",
      "----------\n",
      "train Loss: 31.6284\n",
      "val Loss: 19.3082\n",
      "\n",
      "Epoch 299/1000\n",
      "----------\n",
      "train Loss: 36.1822\n",
      "val Loss: 15.0569\n",
      "\n",
      "Epoch 300/1000\n",
      "----------\n",
      "train Loss: 17.1013\n",
      "val Loss: 10.7902\n",
      "\n",
      "Epoch 301/1000\n",
      "----------\n",
      "train Loss: 8.9630\n",
      "val Loss: 76.6403\n",
      "\n",
      "Epoch 302/1000\n",
      "----------\n",
      "train Loss: 67.3756\n",
      "val Loss: 104.2205\n",
      "\n",
      "Epoch 303/1000\n",
      "----------\n",
      "train Loss: 52.8512\n",
      "val Loss: 8.4535\n",
      "\n",
      "Epoch 304/1000\n",
      "----------\n",
      "train Loss: 26.2367\n",
      "val Loss: 26.1107\n",
      "\n",
      "Epoch 305/1000\n",
      "----------\n",
      "train Loss: 30.7012\n",
      "val Loss: 45.0885\n",
      "\n",
      "Epoch 306/1000\n",
      "----------\n",
      "train Loss: 19.6417\n",
      "val Loss: 39.6415\n",
      "\n",
      "Epoch 307/1000\n",
      "----------\n",
      "train Loss: 21.4898\n",
      "val Loss: 13.0119\n",
      "\n",
      "Epoch 308/1000\n",
      "----------\n",
      "train Loss: 24.5600\n",
      "val Loss: 7.9006\n",
      "\n",
      "Epoch 309/1000\n",
      "----------\n",
      "train Loss: 35.2786\n",
      "val Loss: 47.5292\n",
      "\n",
      "Epoch 310/1000\n",
      "----------\n",
      "train Loss: 20.7853\n",
      "val Loss: 25.9236\n",
      "\n",
      "Epoch 311/1000\n",
      "----------\n",
      "train Loss: 11.7182\n",
      "val Loss: 15.4113\n",
      "\n",
      "Epoch 312/1000\n",
      "----------\n",
      "train Loss: 33.1413\n",
      "val Loss: 9.5254\n",
      "\n",
      "Epoch 313/1000\n",
      "----------\n",
      "train Loss: 33.9717\n",
      "val Loss: 20.1593\n",
      "\n",
      "Epoch 314/1000\n",
      "----------\n",
      "train Loss: 25.7632\n",
      "val Loss: 32.0952\n",
      "\n",
      "Epoch 315/1000\n",
      "----------\n",
      "train Loss: 14.8012\n",
      "val Loss: 15.1005\n",
      "\n",
      "Epoch 316/1000\n",
      "----------\n",
      "train Loss: 12.9081\n",
      "val Loss: 11.8416\n",
      "\n",
      "Epoch 317/1000\n",
      "----------\n",
      "train Loss: 18.1844\n",
      "val Loss: 32.5999\n",
      "\n",
      "Epoch 318/1000\n",
      "----------\n",
      "train Loss: 8.0427\n",
      "val Loss: 10.6417\n",
      "\n",
      "Epoch 319/1000\n",
      "----------\n",
      "train Loss: 10.7629\n",
      "val Loss: 7.5252\n",
      "\n",
      "Epoch 320/1000\n",
      "----------\n",
      "train Loss: 12.6267\n",
      "val Loss: 25.4773\n",
      "\n",
      "Epoch 321/1000\n",
      "----------\n",
      "train Loss: 8.1860\n",
      "val Loss: 5.4216\n",
      "\n",
      "Epoch 322/1000\n",
      "----------\n",
      "train Loss: 5.5110\n",
      "val Loss: 18.6881\n",
      "\n",
      "Epoch 323/1000\n",
      "----------\n",
      "train Loss: 8.2357\n",
      "val Loss: 46.7737\n",
      "\n",
      "Epoch 324/1000\n",
      "----------\n",
      "train Loss: 14.4496\n",
      "val Loss: 17.6734\n",
      "\n",
      "Epoch 325/1000\n",
      "----------\n",
      "train Loss: 13.0851\n",
      "val Loss: 7.5655\n",
      "\n",
      "Epoch 326/1000\n",
      "----------\n",
      "train Loss: 18.2519\n",
      "val Loss: 106.9742\n",
      "\n",
      "Epoch 327/1000\n",
      "----------\n",
      "train Loss: 37.8125\n",
      "val Loss: 151.9430\n",
      "\n",
      "Epoch 328/1000\n",
      "----------\n",
      "train Loss: 31.3414\n",
      "val Loss: 46.1588\n",
      "\n",
      "Epoch 329/1000\n",
      "----------\n",
      "train Loss: 27.8949\n",
      "val Loss: 26.3203\n",
      "\n",
      "Epoch 330/1000\n",
      "----------\n",
      "train Loss: 41.2609\n",
      "val Loss: 47.7658\n",
      "\n",
      "Epoch 331/1000\n",
      "----------\n",
      "train Loss: 11.4632\n",
      "val Loss: 7.9313\n",
      "\n",
      "Epoch 332/1000\n",
      "----------\n",
      "train Loss: 10.1301\n",
      "val Loss: 87.5483\n",
      "\n",
      "Epoch 333/1000\n",
      "----------\n",
      "train Loss: 13.4357\n",
      "val Loss: 5.8484\n",
      "\n",
      "Epoch 334/1000\n",
      "----------\n",
      "train Loss: 17.6595\n",
      "val Loss: 31.4784\n",
      "\n",
      "Epoch 335/1000\n",
      "----------\n",
      "train Loss: 58.1435\n",
      "val Loss: 112.9094\n",
      "\n",
      "Epoch 336/1000\n",
      "----------\n",
      "train Loss: 26.3257\n",
      "val Loss: 6.8590\n",
      "\n",
      "Epoch 337/1000\n",
      "----------\n",
      "train Loss: 3.7184\n",
      "val Loss: 40.4320\n",
      "\n",
      "Epoch 338/1000\n",
      "----------\n",
      "train Loss: 6.0647\n",
      "val Loss: 10.0335\n",
      "\n",
      "Epoch 339/1000\n",
      "----------\n",
      "train Loss: 23.9756\n",
      "val Loss: 203.2883\n",
      "\n",
      "Epoch 340/1000\n",
      "----------\n",
      "train Loss: 47.7827\n",
      "val Loss: 6.7471\n",
      "\n",
      "Epoch 341/1000\n",
      "----------\n",
      "train Loss: 39.7296\n",
      "val Loss: 77.8154\n",
      "\n",
      "Epoch 342/1000\n",
      "----------\n",
      "train Loss: 61.6150\n",
      "val Loss: 126.6883\n",
      "\n",
      "Epoch 343/1000\n",
      "----------\n",
      "train Loss: 29.1526\n",
      "val Loss: 15.8337\n",
      "\n",
      "Epoch 344/1000\n",
      "----------\n",
      "train Loss: 42.8672\n",
      "val Loss: 111.0982\n",
      "\n",
      "Epoch 345/1000\n",
      "----------\n",
      "train Loss: 14.8461\n",
      "val Loss: 7.3947\n",
      "\n",
      "Epoch 346/1000\n",
      "----------\n",
      "train Loss: 2.7288\n",
      "val Loss: 12.0170\n",
      "\n",
      "Epoch 347/1000\n",
      "----------\n",
      "train Loss: 10.3607\n",
      "val Loss: 34.4667\n",
      "\n",
      "Epoch 348/1000\n",
      "----------\n",
      "train Loss: 16.6666\n",
      "val Loss: 12.4181\n",
      "\n",
      "Epoch 349/1000\n",
      "----------\n",
      "train Loss: 20.5292\n",
      "val Loss: 11.9261\n",
      "\n",
      "Epoch 350/1000\n",
      "----------\n",
      "train Loss: 25.3423\n",
      "val Loss: 66.1200\n",
      "\n",
      "Epoch 351/1000\n",
      "----------\n",
      "train Loss: 37.7908\n",
      "val Loss: 67.9215\n",
      "\n",
      "Epoch 352/1000\n",
      "----------\n",
      "train Loss: 23.4641\n",
      "val Loss: 20.6970\n",
      "\n",
      "Epoch 353/1000\n",
      "----------\n",
      "train Loss: 11.6363\n",
      "val Loss: 9.0069\n",
      "\n",
      "Epoch 354/1000\n",
      "----------\n",
      "train Loss: 10.7330\n",
      "val Loss: 112.4525\n",
      "\n",
      "Epoch 355/1000\n",
      "----------\n",
      "train Loss: 42.7689\n",
      "val Loss: 69.5826\n",
      "\n",
      "Epoch 356/1000\n",
      "----------\n",
      "train Loss: 38.5696\n",
      "val Loss: 15.6843\n",
      "\n",
      "Epoch 357/1000\n",
      "----------\n",
      "train Loss: 15.1742\n",
      "val Loss: 18.9826\n",
      "\n",
      "Epoch 358/1000\n",
      "----------\n",
      "train Loss: 8.9653\n",
      "val Loss: 16.5638\n",
      "\n",
      "Epoch 359/1000\n",
      "----------\n",
      "train Loss: 7.6587\n",
      "val Loss: 8.7688\n",
      "\n",
      "Epoch 360/1000\n",
      "----------\n",
      "train Loss: 12.8757\n",
      "val Loss: 105.1563\n",
      "\n",
      "Epoch 361/1000\n",
      "----------\n",
      "train Loss: 21.1942\n",
      "val Loss: 18.9061\n",
      "\n",
      "Epoch 362/1000\n",
      "----------\n",
      "train Loss: 22.8088\n",
      "val Loss: 41.8288\n",
      "\n",
      "Epoch 363/1000\n",
      "----------\n",
      "train Loss: 12.3966\n",
      "val Loss: 6.5000\n",
      "\n",
      "Epoch 364/1000\n",
      "----------\n",
      "train Loss: 18.0686\n",
      "val Loss: 56.1048\n",
      "\n",
      "Epoch 365/1000\n",
      "----------\n",
      "train Loss: 20.5395\n",
      "val Loss: 114.3709\n",
      "\n",
      "Epoch 366/1000\n",
      "----------\n",
      "train Loss: 30.0390\n",
      "val Loss: 54.5333\n",
      "\n",
      "Epoch 367/1000\n",
      "----------\n",
      "train Loss: 9.6423\n",
      "val Loss: 5.8040\n",
      "\n",
      "Epoch 368/1000\n",
      "----------\n",
      "train Loss: 8.2235\n",
      "val Loss: 17.4940\n",
      "\n",
      "Epoch 369/1000\n",
      "----------\n",
      "train Loss: 26.0364\n",
      "val Loss: 71.1745\n",
      "\n",
      "Epoch 370/1000\n",
      "----------\n",
      "train Loss: 24.5546\n",
      "val Loss: 81.5927\n",
      "\n",
      "Epoch 371/1000\n",
      "----------\n",
      "train Loss: 24.0523\n",
      "val Loss: 31.7035\n",
      "\n",
      "Epoch 372/1000\n",
      "----------\n",
      "train Loss: 14.2852\n",
      "val Loss: 5.8530\n",
      "\n",
      "Epoch 373/1000\n",
      "----------\n",
      "train Loss: 22.3937\n",
      "val Loss: 31.0943\n",
      "\n",
      "Epoch 374/1000\n",
      "----------\n",
      "train Loss: 15.8673\n",
      "val Loss: 50.5601\n",
      "\n",
      "Epoch 375/1000\n",
      "----------\n",
      "train Loss: 8.8367\n",
      "val Loss: 8.8000\n",
      "\n",
      "Epoch 376/1000\n",
      "----------\n",
      "train Loss: 19.1814\n",
      "val Loss: 38.2610\n",
      "\n",
      "Epoch 377/1000\n",
      "----------\n",
      "train Loss: 10.9706\n",
      "val Loss: 43.5439\n",
      "\n",
      "Epoch 378/1000\n",
      "----------\n",
      "train Loss: 16.7006\n",
      "val Loss: 41.2532\n",
      "\n",
      "Epoch 379/1000\n",
      "----------\n",
      "train Loss: 8.9159\n",
      "val Loss: 9.8032\n",
      "\n",
      "Epoch 380/1000\n",
      "----------\n",
      "train Loss: 13.3138\n",
      "val Loss: 16.1503\n",
      "\n",
      "Epoch 381/1000\n",
      "----------\n",
      "train Loss: 11.9515\n",
      "val Loss: 5.8991\n",
      "\n",
      "Epoch 382/1000\n",
      "----------\n",
      "train Loss: 10.8694\n",
      "val Loss: 34.9449\n",
      "\n",
      "Epoch 383/1000\n",
      "----------\n",
      "train Loss: 11.4489\n",
      "val Loss: 9.3431\n",
      "\n",
      "Epoch 384/1000\n",
      "----------\n",
      "train Loss: 17.3917\n",
      "val Loss: 7.3186\n",
      "\n",
      "Epoch 385/1000\n",
      "----------\n",
      "train Loss: 9.0199\n",
      "val Loss: 35.8619\n",
      "\n",
      "Epoch 386/1000\n",
      "----------\n",
      "train Loss: 9.8748\n",
      "val Loss: 41.3644\n",
      "\n",
      "Epoch 387/1000\n",
      "----------\n",
      "train Loss: 22.6950\n",
      "val Loss: 54.0502\n",
      "\n",
      "Epoch 388/1000\n",
      "----------\n",
      "train Loss: 18.0733\n",
      "val Loss: 16.6943\n",
      "\n",
      "Epoch 389/1000\n",
      "----------\n",
      "train Loss: 15.8866\n",
      "val Loss: 133.3535\n",
      "\n",
      "Epoch 390/1000\n",
      "----------\n",
      "train Loss: 33.7591\n",
      "val Loss: 14.2469\n",
      "\n",
      "Epoch 391/1000\n",
      "----------\n",
      "train Loss: 11.4844\n",
      "val Loss: 21.0743\n",
      "\n",
      "Epoch 392/1000\n",
      "----------\n",
      "train Loss: 18.8556\n",
      "val Loss: 45.5229\n",
      "\n",
      "Epoch 393/1000\n",
      "----------\n",
      "train Loss: 23.7282\n",
      "val Loss: 116.8498\n",
      "\n",
      "Epoch 394/1000\n",
      "----------\n",
      "train Loss: 77.8457\n",
      "val Loss: 193.3330\n",
      "\n",
      "Epoch 395/1000\n",
      "----------\n",
      "train Loss: 36.6955\n",
      "val Loss: 5.4781\n",
      "\n",
      "Epoch 396/1000\n",
      "----------\n",
      "train Loss: 4.8701\n",
      "val Loss: 10.5480\n",
      "\n",
      "Epoch 397/1000\n",
      "----------\n",
      "train Loss: 14.6126\n",
      "val Loss: 8.8962\n",
      "\n",
      "Epoch 398/1000\n",
      "----------\n",
      "train Loss: 16.1309\n",
      "val Loss: 26.7691\n",
      "\n",
      "Epoch 399/1000\n",
      "----------\n",
      "train Loss: 6.9892\n",
      "val Loss: 13.6785\n",
      "\n",
      "Epoch 400/1000\n",
      "----------\n",
      "train Loss: 12.0374\n",
      "val Loss: 5.1465\n",
      "\n",
      "Epoch 401/1000\n",
      "----------\n",
      "train Loss: 9.7363\n",
      "val Loss: 7.0463\n",
      "\n",
      "Epoch 402/1000\n",
      "----------\n",
      "train Loss: 36.8576\n",
      "val Loss: 7.7581\n",
      "\n",
      "Epoch 403/1000\n",
      "----------\n",
      "train Loss: 17.0159\n",
      "val Loss: 28.9310\n",
      "\n",
      "Epoch 404/1000\n",
      "----------\n",
      "train Loss: 17.9083\n",
      "val Loss: 14.5102\n",
      "\n",
      "Epoch 405/1000\n",
      "----------\n",
      "train Loss: 27.7479\n",
      "val Loss: 222.9348\n",
      "\n",
      "Epoch 406/1000\n",
      "----------\n",
      "train Loss: 45.0970\n",
      "val Loss: 37.5452\n",
      "\n",
      "Epoch 407/1000\n",
      "----------\n",
      "train Loss: 78.1246\n",
      "val Loss: 27.7422\n",
      "\n",
      "Epoch 408/1000\n",
      "----------\n",
      "train Loss: 9.4394\n",
      "val Loss: 47.3201\n",
      "\n",
      "Epoch 409/1000\n",
      "----------\n",
      "train Loss: 22.1769\n",
      "val Loss: 36.6303\n",
      "\n",
      "Epoch 410/1000\n",
      "----------\n",
      "train Loss: 27.5237\n",
      "val Loss: 9.0982\n",
      "\n",
      "Epoch 411/1000\n",
      "----------\n",
      "train Loss: 20.8293\n",
      "val Loss: 18.4655\n",
      "\n",
      "Epoch 412/1000\n",
      "----------\n",
      "train Loss: 24.9682\n",
      "val Loss: 49.4567\n",
      "\n",
      "Epoch 413/1000\n",
      "----------\n",
      "train Loss: 20.6691\n",
      "val Loss: 25.1882\n",
      "\n",
      "Epoch 414/1000\n",
      "----------\n",
      "train Loss: 17.9265\n",
      "val Loss: 31.3953\n",
      "\n",
      "Epoch 415/1000\n",
      "----------\n",
      "train Loss: 11.6234\n",
      "val Loss: 17.9548\n",
      "\n",
      "Epoch 416/1000\n",
      "----------\n",
      "train Loss: 45.5815\n",
      "val Loss: 105.5660\n",
      "\n",
      "Epoch 417/1000\n",
      "----------\n",
      "train Loss: 35.2335\n",
      "val Loss: 33.6532\n",
      "\n",
      "Epoch 418/1000\n",
      "----------\n",
      "train Loss: 33.1327\n",
      "val Loss: 10.5913\n",
      "\n",
      "Epoch 419/1000\n",
      "----------\n",
      "train Loss: 25.5766\n",
      "val Loss: 12.7025\n",
      "\n",
      "Epoch 420/1000\n",
      "----------\n",
      "train Loss: 7.3906\n",
      "val Loss: 12.4095\n",
      "\n",
      "Epoch 421/1000\n",
      "----------\n",
      "train Loss: 17.5917\n",
      "val Loss: 25.3019\n",
      "\n",
      "Epoch 422/1000\n",
      "----------\n",
      "train Loss: 55.3505\n",
      "val Loss: 177.9680\n",
      "\n",
      "Epoch 423/1000\n",
      "----------\n",
      "train Loss: 49.5336\n",
      "val Loss: 7.9250\n",
      "\n",
      "Epoch 424/1000\n",
      "----------\n",
      "train Loss: 11.5660\n",
      "val Loss: 19.1750\n",
      "\n",
      "Epoch 425/1000\n",
      "----------\n",
      "train Loss: 3.9807\n",
      "val Loss: 15.4762\n",
      "\n",
      "Epoch 426/1000\n",
      "----------\n",
      "train Loss: 7.4662\n",
      "val Loss: 8.8637\n",
      "\n",
      "Epoch 427/1000\n",
      "----------\n",
      "train Loss: 9.2595\n",
      "val Loss: 57.4612\n",
      "\n",
      "Epoch 428/1000\n",
      "----------\n",
      "train Loss: 17.2188\n",
      "val Loss: 5.2282\n",
      "\n",
      "Epoch 429/1000\n",
      "----------\n",
      "train Loss: 6.0316\n",
      "val Loss: 133.5139\n",
      "\n",
      "Epoch 430/1000\n",
      "----------\n",
      "train Loss: 35.0502\n",
      "val Loss: 52.4887\n",
      "\n",
      "Epoch 431/1000\n",
      "----------\n",
      "train Loss: 10.4420\n",
      "val Loss: 8.4566\n",
      "\n",
      "Epoch 432/1000\n",
      "----------\n",
      "train Loss: 12.3746\n",
      "val Loss: 13.2259\n",
      "\n",
      "Epoch 433/1000\n",
      "----------\n",
      "train Loss: 13.6456\n",
      "val Loss: 22.5923\n",
      "\n",
      "Epoch 434/1000\n",
      "----------\n",
      "train Loss: 34.3543\n",
      "val Loss: 12.3837\n",
      "\n",
      "Epoch 435/1000\n",
      "----------\n",
      "train Loss: 9.9600\n",
      "val Loss: 29.8714\n",
      "\n",
      "Epoch 436/1000\n",
      "----------\n",
      "train Loss: 14.2046\n",
      "val Loss: 6.3190\n",
      "\n",
      "Epoch 437/1000\n",
      "----------\n",
      "train Loss: 9.3613\n",
      "val Loss: 33.7874\n",
      "\n",
      "Epoch 438/1000\n",
      "----------\n",
      "train Loss: 8.9861\n",
      "val Loss: 18.8875\n",
      "\n",
      "Epoch 439/1000\n",
      "----------\n",
      "train Loss: 18.9605\n",
      "val Loss: 14.0082\n",
      "\n",
      "Epoch 440/1000\n",
      "----------\n",
      "train Loss: 28.8351\n",
      "val Loss: 7.6376\n",
      "\n",
      "Epoch 441/1000\n",
      "----------\n",
      "train Loss: 10.8639\n",
      "val Loss: 9.7850\n",
      "\n",
      "Epoch 442/1000\n",
      "----------\n",
      "train Loss: 7.5220\n",
      "val Loss: 8.6711\n",
      "\n",
      "Epoch 443/1000\n",
      "----------\n",
      "train Loss: 14.9550\n",
      "val Loss: 21.5297\n",
      "\n",
      "Epoch 444/1000\n",
      "----------\n",
      "train Loss: 20.4590\n",
      "val Loss: 15.0834\n",
      "\n",
      "Epoch 445/1000\n",
      "----------\n",
      "train Loss: 12.6314\n",
      "val Loss: 6.0940\n",
      "\n",
      "Epoch 446/1000\n",
      "----------\n",
      "train Loss: 6.0477\n",
      "val Loss: 10.3207\n",
      "\n",
      "Epoch 447/1000\n",
      "----------\n",
      "train Loss: 25.2339\n",
      "val Loss: 5.8403\n",
      "\n",
      "Epoch 448/1000\n",
      "----------\n",
      "train Loss: 33.9234\n",
      "val Loss: 6.5256\n",
      "\n",
      "Epoch 449/1000\n",
      "----------\n",
      "train Loss: 7.1442\n",
      "val Loss: 28.8660\n",
      "\n",
      "Epoch 450/1000\n",
      "----------\n",
      "train Loss: 12.1087\n",
      "val Loss: 6.7808\n",
      "\n",
      "Epoch 451/1000\n",
      "----------\n",
      "train Loss: 20.7733\n",
      "val Loss: 40.7455\n",
      "\n",
      "Epoch 452/1000\n",
      "----------\n",
      "train Loss: 22.9560\n",
      "val Loss: 27.5997\n",
      "\n",
      "Epoch 453/1000\n",
      "----------\n",
      "train Loss: 11.1646\n",
      "val Loss: 40.9347\n",
      "\n",
      "Epoch 454/1000\n",
      "----------\n",
      "train Loss: 7.7812\n",
      "val Loss: 12.2029\n",
      "\n",
      "Epoch 455/1000\n",
      "----------\n",
      "train Loss: 43.6603\n",
      "val Loss: 86.0949\n",
      "\n",
      "Epoch 456/1000\n",
      "----------\n",
      "train Loss: 40.1420\n",
      "val Loss: 7.7006\n",
      "\n",
      "Epoch 457/1000\n",
      "----------\n",
      "train Loss: 13.4142\n",
      "val Loss: 24.5041\n",
      "\n",
      "Epoch 458/1000\n",
      "----------\n",
      "train Loss: 12.9095\n",
      "val Loss: 5.8075\n",
      "\n",
      "Epoch 459/1000\n",
      "----------\n",
      "train Loss: 10.1172\n",
      "val Loss: 25.8500\n",
      "\n",
      "Epoch 460/1000\n",
      "----------\n",
      "train Loss: 25.5913\n",
      "val Loss: 8.3190\n",
      "\n",
      "Epoch 461/1000\n",
      "----------\n",
      "train Loss: 10.4199\n",
      "val Loss: 7.8824\n",
      "\n",
      "Epoch 462/1000\n",
      "----------\n",
      "train Loss: 10.1570\n",
      "val Loss: 24.3783\n",
      "\n",
      "Epoch 463/1000\n",
      "----------\n",
      "train Loss: 18.0695\n",
      "val Loss: 8.9934\n",
      "\n",
      "Epoch 464/1000\n",
      "----------\n",
      "train Loss: 12.7230\n",
      "val Loss: 38.7675\n",
      "\n",
      "Epoch 465/1000\n",
      "----------\n",
      "train Loss: 14.8043\n",
      "val Loss: 14.7126\n",
      "\n",
      "Epoch 466/1000\n",
      "----------\n",
      "train Loss: 40.5984\n",
      "val Loss: 79.8961\n",
      "\n",
      "Epoch 467/1000\n",
      "----------\n",
      "train Loss: 43.9082\n",
      "val Loss: 27.6617\n",
      "\n",
      "Epoch 468/1000\n",
      "----------\n",
      "train Loss: 15.4778\n",
      "val Loss: 7.0539\n",
      "\n",
      "Epoch 469/1000\n",
      "----------\n",
      "train Loss: 7.1917\n",
      "val Loss: 12.1245\n",
      "\n",
      "Epoch 470/1000\n",
      "----------\n",
      "train Loss: 20.8509\n",
      "val Loss: 9.3701\n",
      "\n",
      "Epoch 471/1000\n",
      "----------\n",
      "train Loss: 13.2391\n",
      "val Loss: 8.2704\n",
      "\n",
      "Epoch 472/1000\n",
      "----------\n",
      "train Loss: 12.4218\n",
      "val Loss: 91.1937\n",
      "\n",
      "Epoch 473/1000\n",
      "----------\n",
      "train Loss: 45.8169\n",
      "val Loss: 44.3982\n",
      "\n",
      "Epoch 474/1000\n",
      "----------\n",
      "train Loss: 8.0260\n",
      "val Loss: 6.1216\n",
      "\n",
      "Epoch 475/1000\n",
      "----------\n",
      "train Loss: 10.9668\n",
      "val Loss: 9.2247\n",
      "\n",
      "Epoch 476/1000\n",
      "----------\n",
      "train Loss: 9.2233\n",
      "val Loss: 22.8794\n",
      "\n",
      "Epoch 477/1000\n",
      "----------\n",
      "train Loss: 11.0023\n",
      "val Loss: 22.7797\n",
      "\n",
      "Epoch 478/1000\n",
      "----------\n",
      "train Loss: 14.3408\n",
      "val Loss: 11.8424\n",
      "\n",
      "Epoch 479/1000\n",
      "----------\n",
      "train Loss: 6.2922\n",
      "val Loss: 13.6468\n",
      "\n",
      "Epoch 480/1000\n",
      "----------\n",
      "train Loss: 6.8933\n",
      "val Loss: 9.5019\n",
      "\n",
      "Epoch 481/1000\n",
      "----------\n",
      "train Loss: 9.8015\n",
      "val Loss: 9.0200\n",
      "\n",
      "Epoch 482/1000\n",
      "----------\n",
      "train Loss: 32.0536\n",
      "val Loss: 51.3048\n",
      "\n",
      "Epoch 483/1000\n",
      "----------\n",
      "train Loss: 50.5492\n",
      "val Loss: 184.8895\n",
      "\n",
      "Epoch 484/1000\n",
      "----------\n",
      "train Loss: 35.9634\n",
      "val Loss: 19.5911\n",
      "\n",
      "Epoch 485/1000\n",
      "----------\n",
      "train Loss: 10.8128\n",
      "val Loss: 7.5407\n",
      "\n",
      "Epoch 486/1000\n",
      "----------\n",
      "train Loss: 15.6329\n",
      "val Loss: 28.7891\n",
      "\n",
      "Epoch 487/1000\n",
      "----------\n",
      "train Loss: 9.4147\n",
      "val Loss: 11.6305\n",
      "\n",
      "Epoch 488/1000\n",
      "----------\n",
      "train Loss: 14.8908\n",
      "val Loss: 60.5859\n",
      "\n",
      "Epoch 489/1000\n",
      "----------\n",
      "train Loss: 21.7308\n",
      "val Loss: 124.7083\n",
      "\n",
      "Epoch 490/1000\n",
      "----------\n",
      "train Loss: 26.5254\n",
      "val Loss: 39.5441\n",
      "\n",
      "Epoch 491/1000\n",
      "----------\n",
      "train Loss: 15.7502\n",
      "val Loss: 7.3287\n",
      "\n",
      "Epoch 492/1000\n",
      "----------\n",
      "train Loss: 35.2684\n",
      "val Loss: 145.1477\n",
      "\n",
      "Epoch 493/1000\n",
      "----------\n",
      "train Loss: 51.5716\n",
      "val Loss: 6.1912\n",
      "\n",
      "Epoch 494/1000\n",
      "----------\n",
      "train Loss: 14.4684\n",
      "val Loss: 8.8111\n",
      "\n",
      "Epoch 495/1000\n",
      "----------\n",
      "train Loss: 20.3380\n",
      "val Loss: 5.9601\n",
      "\n",
      "Epoch 496/1000\n",
      "----------\n",
      "train Loss: 12.7600\n",
      "val Loss: 99.2721\n",
      "\n",
      "Epoch 497/1000\n",
      "----------\n",
      "train Loss: 21.7998\n",
      "val Loss: 23.5539\n",
      "\n",
      "Epoch 498/1000\n",
      "----------\n",
      "train Loss: 13.8059\n",
      "val Loss: 11.0402\n",
      "\n",
      "Epoch 499/1000\n",
      "----------\n",
      "train Loss: 33.0778\n",
      "val Loss: 12.3724\n",
      "\n",
      "Epoch 500/1000\n",
      "----------\n",
      "train Loss: 25.8174\n",
      "val Loss: 46.7808\n",
      "\n",
      "Epoch 501/1000\n",
      "----------\n",
      "train Loss: 34.7761\n",
      "val Loss: 193.0900\n",
      "\n",
      "Epoch 502/1000\n",
      "----------\n",
      "train Loss: 22.3063\n",
      "val Loss: 13.3432\n",
      "\n",
      "Epoch 503/1000\n",
      "----------\n",
      "train Loss: 13.6208\n",
      "val Loss: 36.1628\n",
      "\n",
      "Epoch 504/1000\n",
      "----------\n",
      "train Loss: 7.6132\n",
      "val Loss: 50.7089\n",
      "\n",
      "Epoch 505/1000\n",
      "----------\n",
      "train Loss: 3.4932\n",
      "val Loss: 13.2140\n",
      "\n",
      "Epoch 506/1000\n",
      "----------\n",
      "train Loss: 7.3388\n",
      "val Loss: 7.1770\n",
      "\n",
      "Epoch 507/1000\n",
      "----------\n",
      "train Loss: 14.1383\n",
      "val Loss: 7.8548\n",
      "\n",
      "Epoch 508/1000\n",
      "----------\n",
      "train Loss: 20.0582\n",
      "val Loss: 93.7251\n",
      "\n",
      "Epoch 509/1000\n",
      "----------\n",
      "train Loss: 35.9077\n",
      "val Loss: 7.3699\n",
      "\n",
      "Epoch 510/1000\n",
      "----------\n",
      "train Loss: 39.3813\n",
      "val Loss: 90.2320\n",
      "\n",
      "Epoch 511/1000\n",
      "----------\n",
      "train Loss: 19.0886\n",
      "val Loss: 8.3982\n",
      "\n",
      "Epoch 512/1000\n",
      "----------\n",
      "train Loss: 10.3231\n",
      "val Loss: 9.1313\n",
      "\n",
      "Epoch 513/1000\n",
      "----------\n",
      "train Loss: 18.4822\n",
      "val Loss: 105.5658\n",
      "\n",
      "Epoch 514/1000\n",
      "----------\n",
      "train Loss: 24.6059\n",
      "val Loss: 46.8629\n",
      "\n",
      "Epoch 515/1000\n",
      "----------\n",
      "train Loss: 7.1679\n",
      "val Loss: 14.0198\n",
      "\n",
      "Epoch 516/1000\n",
      "----------\n",
      "train Loss: 10.5865\n",
      "val Loss: 6.2674\n",
      "\n",
      "Epoch 517/1000\n",
      "----------\n",
      "train Loss: 8.2297\n",
      "val Loss: 34.5301\n",
      "\n",
      "Epoch 518/1000\n",
      "----------\n",
      "train Loss: 20.8674\n",
      "val Loss: 59.3927\n",
      "\n",
      "Epoch 519/1000\n",
      "----------\n",
      "train Loss: 48.7285\n",
      "val Loss: 16.3723\n",
      "\n",
      "Epoch 520/1000\n",
      "----------\n",
      "train Loss: 10.5027\n",
      "val Loss: 14.5977\n",
      "\n",
      "Epoch 521/1000\n",
      "----------\n",
      "train Loss: 81.4420\n",
      "val Loss: 303.0821\n",
      "\n",
      "Epoch 522/1000\n",
      "----------\n",
      "train Loss: 52.3806\n",
      "val Loss: 65.7039\n",
      "\n",
      "Epoch 523/1000\n",
      "----------\n",
      "train Loss: 12.0841\n",
      "val Loss: 9.4700\n",
      "\n",
      "Epoch 524/1000\n",
      "----------\n",
      "train Loss: 9.7729\n",
      "val Loss: 9.8434\n",
      "\n",
      "Epoch 525/1000\n",
      "----------\n",
      "train Loss: 7.2243\n",
      "val Loss: 14.6210\n",
      "\n",
      "Epoch 526/1000\n",
      "----------\n",
      "train Loss: 6.6950\n",
      "val Loss: 19.9641\n",
      "\n",
      "Epoch 527/1000\n",
      "----------\n",
      "train Loss: 20.1876\n",
      "val Loss: 11.4652\n",
      "\n",
      "Epoch 528/1000\n",
      "----------\n",
      "train Loss: 7.1018\n",
      "val Loss: 6.8765\n",
      "\n",
      "Epoch 529/1000\n",
      "----------\n",
      "train Loss: 17.6296\n",
      "val Loss: 16.2862\n",
      "\n",
      "Epoch 530/1000\n",
      "----------\n",
      "train Loss: 13.5941\n",
      "val Loss: 5.3571\n",
      "\n",
      "Epoch 531/1000\n",
      "----------\n",
      "train Loss: 17.3736\n",
      "val Loss: 19.4788\n",
      "\n",
      "Epoch 532/1000\n",
      "----------\n",
      "train Loss: 9.0671\n",
      "val Loss: 5.1133\n",
      "\n",
      "Epoch 533/1000\n",
      "----------\n",
      "train Loss: 10.9805\n",
      "val Loss: 10.0166\n",
      "\n",
      "Epoch 534/1000\n",
      "----------\n",
      "train Loss: 51.1716\n",
      "val Loss: 60.9260\n",
      "\n",
      "Epoch 535/1000\n",
      "----------\n",
      "train Loss: 17.2494\n",
      "val Loss: 12.2630\n",
      "\n",
      "Epoch 536/1000\n",
      "----------\n",
      "train Loss: 10.0714\n",
      "val Loss: 7.5787\n",
      "\n",
      "Epoch 537/1000\n",
      "----------\n",
      "train Loss: 22.8237\n",
      "val Loss: 21.2775\n",
      "\n",
      "Epoch 538/1000\n",
      "----------\n",
      "train Loss: 17.5183\n",
      "val Loss: 15.0775\n",
      "\n",
      "Epoch 539/1000\n",
      "----------\n",
      "train Loss: 30.1524\n",
      "val Loss: 17.1342\n",
      "\n",
      "Epoch 540/1000\n",
      "----------\n",
      "train Loss: 50.0197\n",
      "val Loss: 40.5474\n",
      "\n",
      "Epoch 541/1000\n",
      "----------\n",
      "train Loss: 17.3177\n",
      "val Loss: 7.7748\n",
      "\n",
      "Epoch 542/1000\n",
      "----------\n",
      "train Loss: 38.9253\n",
      "val Loss: 16.5230\n",
      "\n",
      "Epoch 543/1000\n",
      "----------\n",
      "train Loss: 10.6693\n",
      "val Loss: 7.3535\n",
      "\n",
      "Epoch 544/1000\n",
      "----------\n",
      "train Loss: 3.9121\n",
      "val Loss: 7.3980\n",
      "\n",
      "Epoch 545/1000\n",
      "----------\n",
      "train Loss: 6.8055\n",
      "val Loss: 13.1582\n",
      "\n",
      "Epoch 546/1000\n",
      "----------\n",
      "train Loss: 14.1273\n",
      "val Loss: 5.4121\n",
      "\n",
      "Epoch 547/1000\n",
      "----------\n",
      "train Loss: 17.9984\n",
      "val Loss: 14.1773\n",
      "\n",
      "Epoch 548/1000\n",
      "----------\n",
      "train Loss: 17.2381\n",
      "val Loss: 8.8645\n",
      "\n",
      "Epoch 549/1000\n",
      "----------\n",
      "train Loss: 25.2617\n",
      "val Loss: 57.5048\n",
      "\n",
      "Epoch 550/1000\n",
      "----------\n",
      "train Loss: 50.5334\n",
      "val Loss: 43.7892\n",
      "\n",
      "Epoch 551/1000\n",
      "----------\n",
      "train Loss: 13.1811\n",
      "val Loss: 8.2777\n",
      "\n",
      "Epoch 552/1000\n",
      "----------\n",
      "train Loss: 28.5038\n",
      "val Loss: 18.5910\n",
      "\n",
      "Epoch 553/1000\n",
      "----------\n",
      "train Loss: 11.9706\n",
      "val Loss: 40.3865\n",
      "\n",
      "Epoch 554/1000\n",
      "----------\n",
      "train Loss: 23.3929\n",
      "val Loss: 15.7483\n",
      "\n",
      "Epoch 555/1000\n",
      "----------\n",
      "train Loss: 6.7084\n",
      "val Loss: 11.7201\n",
      "\n",
      "Epoch 556/1000\n",
      "----------\n",
      "train Loss: 17.5653\n",
      "val Loss: 48.6936\n",
      "\n",
      "Epoch 557/1000\n",
      "----------\n",
      "train Loss: 14.1794\n",
      "val Loss: 19.3957\n",
      "\n",
      "Epoch 558/1000\n",
      "----------\n",
      "train Loss: 35.2291\n",
      "val Loss: 91.1865\n",
      "\n",
      "Epoch 559/1000\n",
      "----------\n",
      "train Loss: 32.2092\n",
      "val Loss: 10.2961\n",
      "\n",
      "Epoch 560/1000\n",
      "----------\n",
      "train Loss: 59.0808\n",
      "val Loss: 105.2354\n",
      "\n",
      "Epoch 561/1000\n",
      "----------\n",
      "train Loss: 28.9173\n",
      "val Loss: 9.1947\n",
      "\n",
      "Epoch 562/1000\n",
      "----------\n",
      "train Loss: 6.8108\n",
      "val Loss: 123.2325\n",
      "\n",
      "Epoch 563/1000\n",
      "----------\n",
      "train Loss: 15.5190\n",
      "val Loss: 6.0261\n",
      "\n",
      "Epoch 564/1000\n",
      "----------\n",
      "train Loss: 14.7044\n",
      "val Loss: 5.9886\n",
      "\n",
      "Epoch 565/1000\n",
      "----------\n",
      "train Loss: 14.9690\n",
      "val Loss: 13.0484\n",
      "\n",
      "Epoch 566/1000\n",
      "----------\n",
      "train Loss: 25.3457\n",
      "val Loss: 34.5216\n",
      "\n",
      "Epoch 567/1000\n",
      "----------\n",
      "train Loss: 37.8323\n",
      "val Loss: 165.9567\n",
      "\n",
      "Epoch 568/1000\n",
      "----------\n",
      "train Loss: 63.7830\n",
      "val Loss: 12.3746\n",
      "\n",
      "Epoch 569/1000\n",
      "----------\n",
      "train Loss: 15.7138\n",
      "val Loss: 11.7233\n",
      "\n",
      "Epoch 570/1000\n",
      "----------\n",
      "train Loss: 10.6863\n",
      "val Loss: 11.3141\n",
      "\n",
      "Epoch 571/1000\n",
      "----------\n",
      "train Loss: 10.6222\n",
      "val Loss: 14.0368\n",
      "\n",
      "Epoch 572/1000\n",
      "----------\n",
      "train Loss: 13.2245\n",
      "val Loss: 10.3281\n",
      "\n",
      "Epoch 573/1000\n",
      "----------\n",
      "train Loss: 20.1236\n",
      "val Loss: 26.2071\n",
      "\n",
      "Epoch 574/1000\n",
      "----------\n",
      "train Loss: 15.0882\n",
      "val Loss: 5.0248\n",
      "\n",
      "Epoch 575/1000\n",
      "----------\n",
      "train Loss: 23.2002\n",
      "val Loss: 9.6827\n",
      "\n",
      "Epoch 576/1000\n",
      "----------\n",
      "train Loss: 9.0840\n",
      "val Loss: 5.9563\n",
      "\n",
      "Epoch 577/1000\n",
      "----------\n",
      "train Loss: 16.1606\n",
      "val Loss: 17.1616\n",
      "\n",
      "Epoch 578/1000\n",
      "----------\n",
      "train Loss: 14.5934\n",
      "val Loss: 18.3253\n",
      "\n",
      "Epoch 579/1000\n",
      "----------\n",
      "train Loss: 7.1859\n",
      "val Loss: 6.1017\n",
      "\n",
      "Epoch 580/1000\n",
      "----------\n",
      "train Loss: 8.0992\n",
      "val Loss: 17.7979\n",
      "\n",
      "Epoch 581/1000\n",
      "----------\n",
      "train Loss: 8.3644\n",
      "val Loss: 57.6585\n",
      "\n",
      "Epoch 582/1000\n",
      "----------\n",
      "train Loss: 29.3288\n",
      "val Loss: 23.0665\n",
      "\n",
      "Epoch 583/1000\n",
      "----------\n",
      "train Loss: 59.4223\n",
      "val Loss: 55.2826\n",
      "\n",
      "Epoch 584/1000\n",
      "----------\n",
      "train Loss: 21.6051\n",
      "val Loss: 177.5443\n",
      "\n",
      "Epoch 585/1000\n",
      "----------\n",
      "train Loss: 20.1428\n",
      "val Loss: 9.7742\n",
      "\n",
      "Epoch 586/1000\n",
      "----------\n",
      "train Loss: 9.3039\n",
      "val Loss: 7.2735\n",
      "\n",
      "Epoch 587/1000\n",
      "----------\n",
      "train Loss: 11.1158\n",
      "val Loss: 4.5920\n",
      "\n",
      "Epoch 588/1000\n",
      "----------\n",
      "train Loss: 6.1864\n",
      "val Loss: 7.4266\n",
      "\n",
      "Epoch 589/1000\n",
      "----------\n",
      "train Loss: 9.6130\n",
      "val Loss: 32.7434\n",
      "\n",
      "Epoch 590/1000\n",
      "----------\n",
      "train Loss: 24.4871\n",
      "val Loss: 39.1440\n",
      "\n",
      "Epoch 591/1000\n",
      "----------\n",
      "train Loss: 18.0414\n",
      "val Loss: 8.8889\n",
      "\n",
      "Epoch 592/1000\n",
      "----------\n",
      "train Loss: 14.0547\n",
      "val Loss: 20.0243\n",
      "\n",
      "Epoch 593/1000\n",
      "----------\n",
      "train Loss: 18.3426\n",
      "val Loss: 210.8099\n",
      "\n",
      "Epoch 594/1000\n",
      "----------\n",
      "train Loss: 49.5046\n",
      "val Loss: 25.5176\n",
      "\n",
      "Epoch 595/1000\n",
      "----------\n",
      "train Loss: 15.6143\n",
      "val Loss: 27.7056\n",
      "\n",
      "Epoch 596/1000\n",
      "----------\n",
      "train Loss: 37.9434\n",
      "val Loss: 122.7107\n",
      "\n",
      "Epoch 597/1000\n",
      "----------\n",
      "train Loss: 44.2306\n",
      "val Loss: 16.7466\n",
      "\n",
      "Epoch 598/1000\n",
      "----------\n",
      "train Loss: 26.8917\n",
      "val Loss: 146.3451\n",
      "\n",
      "Epoch 599/1000\n",
      "----------\n",
      "train Loss: 24.1921\n",
      "val Loss: 12.1977\n",
      "\n",
      "Epoch 600/1000\n",
      "----------\n",
      "train Loss: 9.2137\n",
      "val Loss: 8.3309\n",
      "\n",
      "Epoch 601/1000\n",
      "----------\n",
      "train Loss: 3.8908\n",
      "val Loss: 13.9886\n",
      "\n",
      "Epoch 602/1000\n",
      "----------\n",
      "train Loss: 15.4681\n",
      "val Loss: 51.5760\n",
      "\n",
      "Epoch 603/1000\n",
      "----------\n",
      "train Loss: 17.0083\n",
      "val Loss: 21.3104\n",
      "\n",
      "Epoch 604/1000\n",
      "----------\n",
      "train Loss: 12.0060\n",
      "val Loss: 113.5679\n",
      "\n",
      "Epoch 605/1000\n",
      "----------\n",
      "train Loss: 41.8329\n",
      "val Loss: 8.2278\n",
      "\n",
      "Epoch 606/1000\n",
      "----------\n",
      "train Loss: 32.3865\n",
      "val Loss: 7.9797\n",
      "\n",
      "Epoch 607/1000\n",
      "----------\n",
      "train Loss: 20.8227\n",
      "val Loss: 23.4679\n",
      "\n",
      "Epoch 608/1000\n",
      "----------\n",
      "train Loss: 15.4209\n",
      "val Loss: 44.4800\n",
      "\n",
      "Epoch 609/1000\n",
      "----------\n",
      "train Loss: 19.8365\n",
      "val Loss: 52.7731\n",
      "\n",
      "Epoch 610/1000\n",
      "----------\n",
      "train Loss: 13.4651\n",
      "val Loss: 7.3593\n",
      "\n",
      "Epoch 611/1000\n",
      "----------\n",
      "train Loss: 4.3186\n",
      "val Loss: 30.7712\n",
      "\n",
      "Epoch 612/1000\n",
      "----------\n",
      "train Loss: 19.5963\n",
      "val Loss: 18.8781\n",
      "\n",
      "Epoch 613/1000\n",
      "----------\n",
      "train Loss: 20.3762\n",
      "val Loss: 55.6432\n",
      "\n",
      "Epoch 614/1000\n",
      "----------\n",
      "train Loss: 28.1580\n",
      "val Loss: 47.9537\n",
      "\n",
      "Epoch 615/1000\n",
      "----------\n",
      "train Loss: 11.4707\n",
      "val Loss: 25.0120\n",
      "\n",
      "Epoch 616/1000\n",
      "----------\n",
      "train Loss: 48.0386\n",
      "val Loss: 69.7077\n",
      "\n",
      "Epoch 617/1000\n",
      "----------\n",
      "train Loss: 23.9340\n",
      "val Loss: 11.3982\n",
      "\n",
      "Epoch 618/1000\n",
      "----------\n",
      "train Loss: 31.4359\n",
      "val Loss: 13.6810\n",
      "\n",
      "Epoch 619/1000\n",
      "----------\n",
      "train Loss: 39.8230\n",
      "val Loss: 207.0385\n",
      "\n",
      "Epoch 620/1000\n",
      "----------\n",
      "train Loss: 85.9771\n",
      "val Loss: 31.2513\n",
      "\n",
      "Epoch 621/1000\n",
      "----------\n",
      "train Loss: 32.8947\n",
      "val Loss: 21.2465\n",
      "\n",
      "Epoch 622/1000\n",
      "----------\n",
      "train Loss: 11.7113\n",
      "val Loss: 23.1007\n",
      "\n",
      "Epoch 623/1000\n",
      "----------\n",
      "train Loss: 23.3400\n",
      "val Loss: 9.6794\n",
      "\n",
      "Epoch 624/1000\n",
      "----------\n",
      "train Loss: 7.7360\n",
      "val Loss: 71.8199\n",
      "\n",
      "Epoch 625/1000\n",
      "----------\n",
      "train Loss: 32.0522\n",
      "val Loss: 7.6965\n",
      "\n",
      "Epoch 626/1000\n",
      "----------\n",
      "train Loss: 3.2339\n",
      "val Loss: 16.7413\n",
      "\n",
      "Epoch 627/1000\n",
      "----------\n",
      "train Loss: 14.6979\n",
      "val Loss: 6.3916\n",
      "\n",
      "Epoch 628/1000\n",
      "----------\n",
      "train Loss: 11.3535\n",
      "val Loss: 29.1024\n",
      "\n",
      "Epoch 629/1000\n",
      "----------\n",
      "train Loss: 16.8916\n",
      "val Loss: 8.5070\n",
      "\n",
      "Epoch 630/1000\n",
      "----------\n",
      "train Loss: 12.8849\n",
      "val Loss: 20.0247\n",
      "\n",
      "Epoch 631/1000\n",
      "----------\n",
      "train Loss: 7.1932\n",
      "val Loss: 9.5336\n",
      "\n",
      "Epoch 632/1000\n",
      "----------\n",
      "train Loss: 12.8969\n",
      "val Loss: 5.6942\n",
      "\n",
      "Epoch 633/1000\n",
      "----------\n",
      "train Loss: 7.9268\n",
      "val Loss: 5.2911\n",
      "\n",
      "Epoch 634/1000\n",
      "----------\n",
      "train Loss: 9.5874\n",
      "val Loss: 7.2615\n",
      "\n",
      "Epoch 635/1000\n",
      "----------\n",
      "train Loss: 4.5135\n",
      "val Loss: 16.8906\n",
      "\n",
      "Epoch 636/1000\n",
      "----------\n",
      "train Loss: 16.5295\n",
      "val Loss: 102.9071\n",
      "\n",
      "Epoch 637/1000\n",
      "----------\n",
      "train Loss: 18.7667\n",
      "val Loss: 32.0541\n",
      "\n",
      "Epoch 638/1000\n",
      "----------\n",
      "train Loss: 39.6664\n",
      "val Loss: 7.4233\n",
      "\n",
      "Epoch 639/1000\n",
      "----------\n",
      "train Loss: 15.9050\n",
      "val Loss: 11.8481\n",
      "\n",
      "Epoch 640/1000\n",
      "----------\n",
      "train Loss: 8.5457\n",
      "val Loss: 121.3176\n",
      "\n",
      "Epoch 641/1000\n",
      "----------\n",
      "train Loss: 24.4749\n",
      "val Loss: 14.6045\n",
      "\n",
      "Epoch 642/1000\n",
      "----------\n",
      "train Loss: 13.4421\n",
      "val Loss: 5.2387\n",
      "\n",
      "Epoch 643/1000\n",
      "----------\n",
      "train Loss: 37.2843\n",
      "val Loss: 250.3344\n",
      "\n",
      "Epoch 644/1000\n",
      "----------\n",
      "train Loss: 37.0266\n",
      "val Loss: 51.5967\n",
      "\n",
      "Epoch 645/1000\n",
      "----------\n",
      "train Loss: 21.9411\n",
      "val Loss: 39.0954\n",
      "\n",
      "Epoch 646/1000\n",
      "----------\n",
      "train Loss: 5.9385\n",
      "val Loss: 14.1901\n",
      "\n",
      "Epoch 647/1000\n",
      "----------\n",
      "train Loss: 24.9372\n",
      "val Loss: 6.8210\n",
      "\n",
      "Epoch 648/1000\n",
      "----------\n",
      "train Loss: 15.0960\n",
      "val Loss: 11.1589\n",
      "\n",
      "Epoch 649/1000\n",
      "----------\n",
      "train Loss: 3.5886\n",
      "val Loss: 30.6971\n",
      "\n",
      "Epoch 650/1000\n",
      "----------\n",
      "train Loss: 26.1921\n",
      "val Loss: 8.2182\n",
      "\n",
      "Epoch 651/1000\n",
      "----------\n",
      "train Loss: 3.6327\n",
      "val Loss: 15.0897\n",
      "\n",
      "Epoch 652/1000\n",
      "----------\n",
      "train Loss: 13.6467\n",
      "val Loss: 61.3509\n",
      "\n",
      "Epoch 653/1000\n",
      "----------\n",
      "train Loss: 17.5838\n",
      "val Loss: 9.8099\n",
      "\n",
      "Epoch 654/1000\n",
      "----------\n",
      "train Loss: 13.3754\n",
      "val Loss: 8.3677\n",
      "\n",
      "Epoch 655/1000\n",
      "----------\n",
      "train Loss: 9.0511\n",
      "val Loss: 44.4176\n",
      "\n",
      "Epoch 656/1000\n",
      "----------\n",
      "train Loss: 10.7998\n",
      "val Loss: 70.1734\n",
      "\n",
      "Epoch 657/1000\n",
      "----------\n",
      "train Loss: 21.2529\n",
      "val Loss: 6.2669\n",
      "\n",
      "Epoch 658/1000\n",
      "----------\n",
      "train Loss: 20.5232\n",
      "val Loss: 5.4260\n",
      "\n",
      "Epoch 659/1000\n",
      "----------\n",
      "train Loss: 7.3112\n",
      "val Loss: 14.0825\n",
      "\n",
      "Epoch 660/1000\n",
      "----------\n",
      "train Loss: 17.3463\n",
      "val Loss: 14.4605\n",
      "\n",
      "Epoch 661/1000\n",
      "----------\n",
      "train Loss: 29.8438\n",
      "val Loss: 78.5417\n",
      "\n",
      "Epoch 662/1000\n",
      "----------\n",
      "train Loss: 41.0150\n",
      "val Loss: 64.4282\n",
      "\n",
      "Epoch 663/1000\n",
      "----------\n",
      "train Loss: 33.9227\n",
      "val Loss: 76.5543\n",
      "\n",
      "Epoch 664/1000\n",
      "----------\n",
      "train Loss: 16.9522\n",
      "val Loss: 183.2552\n",
      "\n",
      "Epoch 665/1000\n",
      "----------\n",
      "train Loss: 27.7130\n",
      "val Loss: 60.8545\n",
      "\n",
      "Epoch 666/1000\n",
      "----------\n",
      "train Loss: 27.6054\n",
      "val Loss: 47.4130\n",
      "\n",
      "Epoch 667/1000\n",
      "----------\n",
      "train Loss: 18.5590\n",
      "val Loss: 17.7444\n",
      "\n",
      "Epoch 668/1000\n",
      "----------\n",
      "train Loss: 11.5588\n",
      "val Loss: 7.8202\n",
      "\n",
      "Epoch 669/1000\n",
      "----------\n",
      "train Loss: 8.0020\n",
      "val Loss: 8.6860\n",
      "\n",
      "Epoch 670/1000\n",
      "----------\n",
      "train Loss: 29.3975\n",
      "val Loss: 72.6481\n",
      "\n",
      "Epoch 671/1000\n",
      "----------\n",
      "train Loss: 108.0465\n",
      "val Loss: 226.1199\n",
      "\n",
      "Epoch 672/1000\n",
      "----------\n",
      "train Loss: 35.7041\n",
      "val Loss: 30.6759\n",
      "\n",
      "Epoch 673/1000\n",
      "----------\n",
      "train Loss: 13.4935\n",
      "val Loss: 41.8388\n",
      "\n",
      "Epoch 674/1000\n",
      "----------\n",
      "train Loss: 30.6307\n",
      "val Loss: 96.3946\n",
      "\n",
      "Epoch 675/1000\n",
      "----------\n",
      "train Loss: 24.9029\n",
      "val Loss: 9.4073\n",
      "\n",
      "Epoch 676/1000\n",
      "----------\n",
      "train Loss: 7.8117\n",
      "val Loss: 27.2849\n",
      "\n",
      "Epoch 677/1000\n",
      "----------\n",
      "train Loss: 16.9261\n",
      "val Loss: 8.0460\n",
      "\n",
      "Epoch 678/1000\n",
      "----------\n",
      "train Loss: 7.0446\n",
      "val Loss: 4.6716\n",
      "\n",
      "Epoch 679/1000\n",
      "----------\n",
      "train Loss: 11.8547\n",
      "val Loss: 61.0974\n",
      "\n",
      "Epoch 680/1000\n",
      "----------\n",
      "train Loss: 8.8123\n",
      "val Loss: 7.5482\n",
      "\n",
      "Epoch 681/1000\n",
      "----------\n",
      "train Loss: 8.8769\n",
      "val Loss: 27.8101\n",
      "\n",
      "Epoch 682/1000\n",
      "----------\n",
      "train Loss: 10.0888\n",
      "val Loss: 13.9037\n",
      "\n",
      "Epoch 683/1000\n",
      "----------\n",
      "train Loss: 16.2296\n",
      "val Loss: 49.6788\n",
      "\n",
      "Epoch 684/1000\n",
      "----------\n",
      "train Loss: 14.9203\n",
      "val Loss: 29.9798\n",
      "\n",
      "Epoch 685/1000\n",
      "----------\n",
      "train Loss: 34.1259\n",
      "val Loss: 21.7485\n",
      "\n",
      "Epoch 686/1000\n",
      "----------\n",
      "train Loss: 9.0957\n",
      "val Loss: 7.5930\n",
      "\n",
      "Epoch 687/1000\n",
      "----------\n",
      "train Loss: 5.2730\n",
      "val Loss: 6.0293\n",
      "\n",
      "Epoch 688/1000\n",
      "----------\n",
      "train Loss: 20.7408\n",
      "val Loss: 10.3205\n",
      "\n",
      "Epoch 689/1000\n",
      "----------\n",
      "train Loss: 3.1669\n",
      "val Loss: 11.3978\n",
      "\n",
      "Epoch 690/1000\n",
      "----------\n",
      "train Loss: 74.5085\n",
      "val Loss: 12.5938\n",
      "\n",
      "Epoch 691/1000\n",
      "----------\n",
      "train Loss: 28.1872\n",
      "val Loss: 53.9461\n",
      "\n",
      "Epoch 692/1000\n",
      "----------\n",
      "train Loss: 18.0659\n",
      "val Loss: 14.4447\n",
      "\n",
      "Epoch 693/1000\n",
      "----------\n",
      "train Loss: 25.4111\n",
      "val Loss: 101.0041\n",
      "\n",
      "Epoch 694/1000\n",
      "----------\n",
      "train Loss: 29.5379\n",
      "val Loss: 7.9711\n",
      "\n",
      "Epoch 695/1000\n",
      "----------\n",
      "train Loss: 25.0644\n",
      "val Loss: 21.7549\n",
      "\n",
      "Epoch 696/1000\n",
      "----------\n",
      "train Loss: 15.8476\n",
      "val Loss: 12.4941\n",
      "\n",
      "Epoch 697/1000\n",
      "----------\n",
      "train Loss: 4.8148\n",
      "val Loss: 9.5103\n",
      "\n",
      "Epoch 698/1000\n",
      "----------\n",
      "train Loss: 5.1913\n",
      "val Loss: 19.5778\n",
      "\n",
      "Epoch 699/1000\n",
      "----------\n",
      "train Loss: 10.3354\n",
      "val Loss: 8.5682\n",
      "\n",
      "Epoch 700/1000\n",
      "----------\n",
      "train Loss: 13.6479\n",
      "val Loss: 8.4747\n",
      "\n",
      "Epoch 701/1000\n",
      "----------\n",
      "train Loss: 7.3026\n",
      "val Loss: 8.7900\n",
      "\n",
      "Epoch 702/1000\n",
      "----------\n",
      "train Loss: 21.9808\n",
      "val Loss: 20.7735\n",
      "\n",
      "Epoch 703/1000\n",
      "----------\n",
      "train Loss: 8.4298\n",
      "val Loss: 25.2095\n",
      "\n",
      "Epoch 704/1000\n",
      "----------\n",
      "train Loss: 7.0636\n",
      "val Loss: 10.3799\n",
      "\n",
      "Epoch 705/1000\n",
      "----------\n",
      "train Loss: 3.1637\n",
      "val Loss: 11.1998\n",
      "\n",
      "Epoch 706/1000\n",
      "----------\n",
      "train Loss: 14.3261\n",
      "val Loss: 9.2448\n",
      "\n",
      "Epoch 707/1000\n",
      "----------\n",
      "train Loss: 11.2733\n",
      "val Loss: 10.8676\n",
      "\n",
      "Epoch 708/1000\n",
      "----------\n",
      "train Loss: 5.6517\n",
      "val Loss: 6.6364\n",
      "\n",
      "Epoch 709/1000\n",
      "----------\n",
      "train Loss: 13.3149\n",
      "val Loss: 10.1030\n",
      "\n",
      "Epoch 710/1000\n",
      "----------\n",
      "train Loss: 9.9195\n",
      "val Loss: 32.4134\n",
      "\n",
      "Epoch 711/1000\n",
      "----------\n",
      "train Loss: 9.7402\n",
      "val Loss: 6.2895\n",
      "\n",
      "Epoch 712/1000\n",
      "----------\n",
      "train Loss: 16.0921\n",
      "val Loss: 7.2958\n",
      "\n",
      "Epoch 713/1000\n",
      "----------\n",
      "train Loss: 15.3596\n",
      "val Loss: 9.8020\n",
      "\n",
      "Epoch 714/1000\n",
      "----------\n",
      "train Loss: 18.6744\n",
      "val Loss: 10.9091\n",
      "\n",
      "Epoch 715/1000\n",
      "----------\n",
      "train Loss: 33.5034\n",
      "val Loss: 82.8929\n",
      "\n",
      "Epoch 716/1000\n",
      "----------\n",
      "train Loss: 27.6786\n",
      "val Loss: 12.2617\n",
      "\n",
      "Epoch 717/1000\n",
      "----------\n",
      "train Loss: 7.5156\n",
      "val Loss: 8.6082\n",
      "\n",
      "Epoch 718/1000\n",
      "----------\n",
      "train Loss: 5.2077\n",
      "val Loss: 9.3042\n",
      "\n",
      "Epoch 719/1000\n",
      "----------\n",
      "train Loss: 12.5398\n",
      "val Loss: 6.4165\n",
      "\n",
      "Epoch 720/1000\n",
      "----------\n",
      "train Loss: 15.3644\n",
      "val Loss: 46.3222\n",
      "\n",
      "Epoch 721/1000\n",
      "----------\n",
      "train Loss: 57.7690\n",
      "val Loss: 77.2947\n",
      "\n",
      "Epoch 722/1000\n",
      "----------\n",
      "train Loss: 14.8303\n",
      "val Loss: 12.7416\n",
      "\n",
      "Epoch 723/1000\n",
      "----------\n",
      "train Loss: 5.6358\n",
      "val Loss: 12.1470\n",
      "\n",
      "Epoch 724/1000\n",
      "----------\n",
      "train Loss: 17.5550\n",
      "val Loss: 37.0072\n",
      "\n",
      "Epoch 725/1000\n",
      "----------\n",
      "train Loss: 9.7367\n",
      "val Loss: 6.2306\n",
      "\n",
      "Epoch 726/1000\n",
      "----------\n",
      "train Loss: 15.6599\n",
      "val Loss: 26.6650\n",
      "\n",
      "Epoch 727/1000\n",
      "----------\n",
      "train Loss: 14.1511\n",
      "val Loss: 21.4842\n",
      "\n",
      "Epoch 728/1000\n",
      "----------\n",
      "train Loss: 12.4149\n",
      "val Loss: 18.0069\n",
      "\n",
      "Epoch 729/1000\n",
      "----------\n",
      "train Loss: 20.5435\n",
      "val Loss: 62.6904\n",
      "\n",
      "Epoch 730/1000\n",
      "----------\n",
      "train Loss: 19.1645\n",
      "val Loss: 92.6292\n",
      "\n",
      "Epoch 731/1000\n",
      "----------\n",
      "train Loss: 19.5176\n",
      "val Loss: 13.9317\n",
      "\n",
      "Epoch 732/1000\n",
      "----------\n",
      "train Loss: 14.1518\n",
      "val Loss: 109.6279\n",
      "\n",
      "Epoch 733/1000\n",
      "----------\n",
      "train Loss: 25.2144\n",
      "val Loss: 28.4909\n",
      "\n",
      "Epoch 734/1000\n",
      "----------\n",
      "train Loss: 4.7145\n",
      "val Loss: 16.8848\n",
      "\n",
      "Epoch 735/1000\n",
      "----------\n",
      "train Loss: 10.4638\n",
      "val Loss: 33.5814\n",
      "\n",
      "Epoch 736/1000\n",
      "----------\n",
      "train Loss: 9.0344\n",
      "val Loss: 33.8159\n",
      "\n",
      "Epoch 737/1000\n",
      "----------\n",
      "train Loss: 9.2151\n",
      "val Loss: 6.5140\n",
      "\n",
      "Epoch 738/1000\n",
      "----------\n",
      "train Loss: 14.6555\n",
      "val Loss: 12.5973\n",
      "\n",
      "Epoch 739/1000\n",
      "----------\n",
      "train Loss: 10.9181\n",
      "val Loss: 45.9895\n",
      "\n",
      "Epoch 740/1000\n",
      "----------\n",
      "train Loss: 29.4379\n",
      "val Loss: 77.8097\n",
      "\n",
      "Epoch 741/1000\n",
      "----------\n",
      "train Loss: 22.5680\n",
      "val Loss: 21.0997\n",
      "\n",
      "Epoch 742/1000\n",
      "----------\n",
      "train Loss: 17.3660\n",
      "val Loss: 21.7233\n",
      "\n",
      "Epoch 743/1000\n",
      "----------\n",
      "train Loss: 12.7813\n",
      "val Loss: 33.3905\n",
      "\n",
      "Epoch 744/1000\n",
      "----------\n",
      "train Loss: 15.7996\n",
      "val Loss: 37.2477\n",
      "\n",
      "Epoch 745/1000\n",
      "----------\n",
      "train Loss: 22.9384\n",
      "val Loss: 17.1971\n",
      "\n",
      "Epoch 746/1000\n",
      "----------\n",
      "train Loss: 24.8903\n",
      "val Loss: 35.4475\n",
      "\n",
      "Epoch 747/1000\n",
      "----------\n",
      "train Loss: 9.3656\n",
      "val Loss: 7.5142\n",
      "\n",
      "Epoch 748/1000\n",
      "----------\n",
      "train Loss: 4.5384\n",
      "val Loss: 6.4693\n",
      "\n",
      "Epoch 749/1000\n",
      "----------\n",
      "train Loss: 7.7169\n",
      "val Loss: 32.1761\n",
      "\n",
      "Epoch 750/1000\n",
      "----------\n",
      "train Loss: 27.7469\n",
      "val Loss: 9.8266\n",
      "\n",
      "Epoch 751/1000\n",
      "----------\n",
      "train Loss: 7.0168\n",
      "val Loss: 39.0334\n",
      "\n",
      "Epoch 752/1000\n",
      "----------\n",
      "train Loss: 15.8549\n",
      "val Loss: 99.5975\n",
      "\n",
      "Epoch 753/1000\n",
      "----------\n",
      "train Loss: 55.5529\n",
      "val Loss: 25.4353\n",
      "\n",
      "Epoch 754/1000\n",
      "----------\n",
      "train Loss: 11.7701\n",
      "val Loss: 24.8950\n",
      "\n",
      "Epoch 755/1000\n",
      "----------\n",
      "train Loss: 13.0021\n",
      "val Loss: 13.1713\n",
      "\n",
      "Epoch 756/1000\n",
      "----------\n",
      "train Loss: 7.3530\n",
      "val Loss: 17.9287\n",
      "\n",
      "Epoch 757/1000\n",
      "----------\n",
      "train Loss: 8.8731\n",
      "val Loss: 33.2733\n",
      "\n",
      "Epoch 758/1000\n",
      "----------\n",
      "train Loss: 21.8830\n",
      "val Loss: 15.0813\n",
      "\n",
      "Epoch 759/1000\n",
      "----------\n",
      "train Loss: 21.5472\n",
      "val Loss: 16.1747\n",
      "\n",
      "Epoch 760/1000\n",
      "----------\n",
      "train Loss: 15.0687\n",
      "val Loss: 7.9860\n",
      "\n",
      "Epoch 761/1000\n",
      "----------\n",
      "train Loss: 34.5689\n",
      "val Loss: 84.9530\n",
      "\n",
      "Epoch 762/1000\n",
      "----------\n",
      "train Loss: 34.8452\n",
      "val Loss: 44.8728\n",
      "\n",
      "Epoch 763/1000\n",
      "----------\n",
      "train Loss: 26.6400\n",
      "val Loss: 40.0427\n",
      "\n",
      "Epoch 764/1000\n",
      "----------\n",
      "train Loss: 27.6632\n",
      "val Loss: 21.6359\n",
      "\n",
      "Epoch 765/1000\n",
      "----------\n",
      "train Loss: 21.1078\n",
      "val Loss: 9.1682\n",
      "\n",
      "Epoch 766/1000\n",
      "----------\n",
      "train Loss: 12.3880\n",
      "val Loss: 15.7050\n",
      "\n",
      "Epoch 767/1000\n",
      "----------\n",
      "train Loss: 19.3916\n",
      "val Loss: 53.6760\n",
      "\n",
      "Epoch 768/1000\n",
      "----------\n",
      "train Loss: 29.1197\n",
      "val Loss: 10.0853\n",
      "\n",
      "Epoch 769/1000\n",
      "----------\n",
      "train Loss: 7.4352\n",
      "val Loss: 8.3382\n",
      "\n",
      "Epoch 770/1000\n",
      "----------\n",
      "train Loss: 8.8344\n",
      "val Loss: 6.9829\n",
      "\n",
      "Epoch 771/1000\n",
      "----------\n",
      "train Loss: 10.2224\n",
      "val Loss: 11.6894\n",
      "\n",
      "Epoch 772/1000\n",
      "----------\n",
      "train Loss: 2.7803\n",
      "val Loss: 13.4480\n",
      "\n",
      "Epoch 773/1000\n",
      "----------\n",
      "train Loss: 17.8135\n",
      "val Loss: 9.3003\n",
      "\n",
      "Epoch 774/1000\n",
      "----------\n",
      "train Loss: 18.3847\n",
      "val Loss: 63.2218\n",
      "\n",
      "Epoch 775/1000\n",
      "----------\n",
      "train Loss: 16.0672\n",
      "val Loss: 55.8789\n",
      "\n",
      "Epoch 776/1000\n",
      "----------\n",
      "train Loss: 20.8065\n",
      "val Loss: 18.9067\n",
      "\n",
      "Epoch 777/1000\n",
      "----------\n",
      "train Loss: 11.6097\n",
      "val Loss: 11.1084\n",
      "\n",
      "Epoch 778/1000\n",
      "----------\n",
      "train Loss: 14.1924\n",
      "val Loss: 9.4841\n",
      "\n",
      "Epoch 779/1000\n",
      "----------\n",
      "train Loss: 10.9469\n",
      "val Loss: 21.4754\n",
      "\n",
      "Epoch 780/1000\n",
      "----------\n",
      "train Loss: 7.7479\n",
      "val Loss: 10.0205\n",
      "\n",
      "Epoch 781/1000\n",
      "----------\n",
      "train Loss: 5.0633\n",
      "val Loss: 16.5210\n",
      "\n",
      "Epoch 782/1000\n",
      "----------\n",
      "train Loss: 10.0621\n",
      "val Loss: 32.5211\n",
      "\n",
      "Epoch 783/1000\n",
      "----------\n",
      "train Loss: 16.0765\n",
      "val Loss: 59.2712\n",
      "\n",
      "Epoch 784/1000\n",
      "----------\n",
      "train Loss: 36.3923\n",
      "val Loss: 41.5105\n",
      "\n",
      "Epoch 785/1000\n",
      "----------\n",
      "train Loss: 32.4491\n",
      "val Loss: 13.9337\n",
      "\n",
      "Epoch 786/1000\n",
      "----------\n",
      "train Loss: 24.1271\n",
      "val Loss: 10.3344\n",
      "\n",
      "Epoch 787/1000\n",
      "----------\n",
      "train Loss: 21.7699\n",
      "val Loss: 48.0422\n",
      "\n",
      "Epoch 788/1000\n",
      "----------\n",
      "train Loss: 23.1550\n",
      "val Loss: 27.9091\n",
      "\n",
      "Epoch 789/1000\n",
      "----------\n",
      "train Loss: 16.5853\n",
      "val Loss: 45.1269\n",
      "\n",
      "Epoch 790/1000\n",
      "----------\n",
      "train Loss: 33.6105\n",
      "val Loss: 9.9827\n",
      "\n",
      "Epoch 791/1000\n",
      "----------\n",
      "train Loss: 14.6625\n",
      "val Loss: 6.3609\n",
      "\n",
      "Epoch 792/1000\n",
      "----------\n",
      "train Loss: 4.0638\n",
      "val Loss: 28.1401\n",
      "\n",
      "Epoch 793/1000\n",
      "----------\n",
      "train Loss: 17.1923\n",
      "val Loss: 118.3363\n",
      "\n",
      "Epoch 794/1000\n",
      "----------\n",
      "train Loss: 26.0774\n",
      "val Loss: 16.9769\n",
      "\n",
      "Epoch 795/1000\n",
      "----------\n",
      "train Loss: 20.3250\n",
      "val Loss: 20.6735\n",
      "\n",
      "Epoch 796/1000\n",
      "----------\n",
      "train Loss: 4.7633\n",
      "val Loss: 9.4551\n",
      "\n",
      "Epoch 797/1000\n",
      "----------\n",
      "train Loss: 11.2300\n",
      "val Loss: 22.0491\n",
      "\n",
      "Epoch 798/1000\n",
      "----------\n",
      "train Loss: 12.4417\n",
      "val Loss: 10.2549\n",
      "\n",
      "Epoch 799/1000\n",
      "----------\n",
      "train Loss: 8.1624\n",
      "val Loss: 24.3695\n",
      "\n",
      "Epoch 800/1000\n",
      "----------\n",
      "train Loss: 38.2060\n",
      "val Loss: 44.3248\n",
      "\n",
      "Epoch 801/1000\n",
      "----------\n",
      "train Loss: 7.0075\n",
      "val Loss: 6.8664\n",
      "\n",
      "Epoch 802/1000\n",
      "----------\n",
      "train Loss: 8.4301\n",
      "val Loss: 10.9526\n",
      "\n",
      "Epoch 803/1000\n",
      "----------\n",
      "train Loss: 14.0476\n",
      "val Loss: 66.6499\n",
      "\n",
      "Epoch 804/1000\n",
      "----------\n",
      "train Loss: 10.1987\n",
      "val Loss: 10.2852\n",
      "\n",
      "Epoch 805/1000\n",
      "----------\n",
      "train Loss: 11.7866\n",
      "val Loss: 36.8878\n",
      "\n",
      "Epoch 806/1000\n",
      "----------\n",
      "train Loss: 29.7975\n",
      "val Loss: 7.8768\n",
      "\n",
      "Epoch 807/1000\n",
      "----------\n",
      "train Loss: 10.7276\n",
      "val Loss: 31.7968\n",
      "\n",
      "Epoch 808/1000\n",
      "----------\n",
      "train Loss: 11.7002\n",
      "val Loss: 28.9910\n",
      "\n",
      "Epoch 809/1000\n",
      "----------\n",
      "train Loss: 8.8417\n",
      "val Loss: 22.6288\n",
      "\n",
      "Epoch 810/1000\n",
      "----------\n",
      "train Loss: 20.6313\n",
      "val Loss: 21.3202\n",
      "\n",
      "Epoch 811/1000\n",
      "----------\n",
      "train Loss: 22.5184\n",
      "val Loss: 56.2709\n",
      "\n",
      "Epoch 812/1000\n",
      "----------\n",
      "train Loss: 13.5153\n",
      "val Loss: 5.7773\n",
      "\n",
      "Epoch 813/1000\n",
      "----------\n",
      "train Loss: 12.7687\n",
      "val Loss: 41.6392\n",
      "\n",
      "Epoch 814/1000\n",
      "----------\n",
      "train Loss: 28.9433\n",
      "val Loss: 20.7287\n",
      "\n",
      "Epoch 815/1000\n",
      "----------\n",
      "train Loss: 14.1252\n",
      "val Loss: 8.2441\n",
      "\n",
      "Epoch 816/1000\n",
      "----------\n",
      "train Loss: 9.5771\n",
      "val Loss: 69.1173\n",
      "\n",
      "Epoch 817/1000\n",
      "----------\n",
      "train Loss: 35.3006\n",
      "val Loss: 86.9206\n",
      "\n",
      "Epoch 818/1000\n",
      "----------\n",
      "train Loss: 33.4782\n",
      "val Loss: 33.8481\n",
      "\n",
      "Epoch 819/1000\n",
      "----------\n",
      "train Loss: 7.4818\n",
      "val Loss: 5.5220\n",
      "\n",
      "Epoch 820/1000\n",
      "----------\n",
      "train Loss: 11.0687\n",
      "val Loss: 5.6261\n",
      "\n",
      "Epoch 821/1000\n",
      "----------\n",
      "train Loss: 20.1566\n",
      "val Loss: 6.7311\n",
      "\n",
      "Epoch 822/1000\n",
      "----------\n",
      "train Loss: 12.9852\n",
      "val Loss: 44.0565\n",
      "\n",
      "Epoch 823/1000\n",
      "----------\n",
      "train Loss: 24.6297\n",
      "val Loss: 19.5876\n",
      "\n",
      "Epoch 824/1000\n",
      "----------\n",
      "train Loss: 11.4201\n",
      "val Loss: 14.3547\n",
      "\n",
      "Epoch 825/1000\n",
      "----------\n",
      "train Loss: 15.5800\n",
      "val Loss: 12.9998\n",
      "\n",
      "Epoch 826/1000\n",
      "----------\n",
      "train Loss: 3.6778\n",
      "val Loss: 6.7530\n",
      "\n",
      "Epoch 827/1000\n",
      "----------\n",
      "train Loss: 10.3745\n",
      "val Loss: 7.1967\n",
      "\n",
      "Epoch 828/1000\n",
      "----------\n",
      "train Loss: 15.1961\n",
      "val Loss: 17.9107\n",
      "\n",
      "Epoch 829/1000\n",
      "----------\n",
      "train Loss: 14.4924\n",
      "val Loss: 5.7885\n",
      "\n",
      "Epoch 830/1000\n",
      "----------\n",
      "train Loss: 10.8006\n",
      "val Loss: 53.4990\n",
      "\n",
      "Epoch 831/1000\n",
      "----------\n",
      "train Loss: 31.6566\n",
      "val Loss: 89.4314\n",
      "\n",
      "Epoch 832/1000\n",
      "----------\n",
      "train Loss: 18.9433\n",
      "val Loss: 13.6659\n",
      "\n",
      "Epoch 833/1000\n",
      "----------\n",
      "train Loss: 7.0180\n",
      "val Loss: 16.9572\n",
      "\n",
      "Epoch 834/1000\n",
      "----------\n",
      "train Loss: 7.7018\n",
      "val Loss: 6.6936\n",
      "\n",
      "Epoch 835/1000\n",
      "----------\n",
      "train Loss: 7.1817\n",
      "val Loss: 22.1706\n",
      "\n",
      "Epoch 836/1000\n",
      "----------\n",
      "train Loss: 7.6318\n",
      "val Loss: 10.7971\n",
      "\n",
      "Epoch 837/1000\n",
      "----------\n",
      "train Loss: 7.5143\n",
      "val Loss: 11.1721\n",
      "\n",
      "Epoch 838/1000\n",
      "----------\n",
      "train Loss: 41.5022\n",
      "val Loss: 53.0014\n",
      "\n",
      "Epoch 839/1000\n",
      "----------\n",
      "train Loss: 27.0579\n",
      "val Loss: 51.2660\n",
      "\n",
      "Epoch 840/1000\n",
      "----------\n",
      "train Loss: 17.8087\n",
      "val Loss: 14.7826\n",
      "\n",
      "Epoch 841/1000\n",
      "----------\n",
      "train Loss: 4.0242\n",
      "val Loss: 13.0914\n",
      "\n",
      "Epoch 842/1000\n",
      "----------\n",
      "train Loss: 28.9703\n",
      "val Loss: 106.5472\n",
      "\n",
      "Epoch 843/1000\n",
      "----------\n",
      "train Loss: 33.5698\n",
      "val Loss: 49.8864\n",
      "\n",
      "Epoch 844/1000\n",
      "----------\n",
      "train Loss: 17.3337\n",
      "val Loss: 36.1268\n",
      "\n",
      "Epoch 845/1000\n",
      "----------\n",
      "train Loss: 25.3186\n",
      "val Loss: 117.9831\n",
      "\n",
      "Epoch 846/1000\n",
      "----------\n",
      "train Loss: 12.5253\n",
      "val Loss: 8.3659\n",
      "\n",
      "Epoch 847/1000\n",
      "----------\n",
      "train Loss: 5.9526\n",
      "val Loss: 7.5784\n",
      "\n",
      "Epoch 848/1000\n",
      "----------\n",
      "train Loss: 49.9383\n",
      "val Loss: 308.7911\n",
      "\n",
      "Epoch 849/1000\n",
      "----------\n",
      "train Loss: 56.8551\n",
      "val Loss: 88.3327\n",
      "\n",
      "Epoch 850/1000\n",
      "----------\n",
      "train Loss: 10.7172\n",
      "val Loss: 7.6673\n",
      "\n",
      "Epoch 851/1000\n",
      "----------\n",
      "train Loss: 15.6588\n",
      "val Loss: 46.4112\n",
      "\n",
      "Epoch 852/1000\n",
      "----------\n",
      "train Loss: 37.0738\n",
      "val Loss: 22.6272\n",
      "\n",
      "Epoch 853/1000\n",
      "----------\n",
      "train Loss: 31.0300\n",
      "val Loss: 74.3046\n",
      "\n",
      "Epoch 854/1000\n",
      "----------\n",
      "train Loss: 33.9436\n",
      "val Loss: 100.8684\n",
      "\n",
      "Epoch 855/1000\n",
      "----------\n",
      "train Loss: 21.6259\n",
      "val Loss: 6.2845\n",
      "\n",
      "Epoch 856/1000\n",
      "----------\n",
      "train Loss: 17.6713\n",
      "val Loss: 90.3934\n",
      "\n",
      "Epoch 857/1000\n",
      "----------\n",
      "train Loss: 18.0618\n",
      "val Loss: 9.5159\n",
      "\n",
      "Epoch 858/1000\n",
      "----------\n",
      "train Loss: 4.7436\n",
      "val Loss: 12.3079\n",
      "\n",
      "Epoch 859/1000\n",
      "----------\n",
      "train Loss: 12.6816\n",
      "val Loss: 72.7963\n",
      "\n",
      "Epoch 860/1000\n",
      "----------\n",
      "train Loss: 30.3200\n",
      "val Loss: 10.7278\n",
      "\n",
      "Epoch 861/1000\n",
      "----------\n",
      "train Loss: 13.2983\n",
      "val Loss: 35.9512\n",
      "\n",
      "Epoch 862/1000\n",
      "----------\n",
      "train Loss: 24.6191\n",
      "val Loss: 8.7014\n",
      "\n",
      "Epoch 863/1000\n",
      "----------\n",
      "train Loss: 20.7164\n",
      "val Loss: 7.7319\n",
      "\n",
      "Epoch 864/1000\n",
      "----------\n",
      "train Loss: 14.2393\n",
      "val Loss: 10.7568\n",
      "\n",
      "Epoch 865/1000\n",
      "----------\n",
      "train Loss: 10.8638\n",
      "val Loss: 10.5928\n",
      "\n",
      "Epoch 866/1000\n",
      "----------\n",
      "train Loss: 14.0469\n",
      "val Loss: 8.4797\n",
      "\n",
      "Epoch 867/1000\n",
      "----------\n",
      "train Loss: 9.1038\n",
      "val Loss: 21.2436\n",
      "\n",
      "Epoch 868/1000\n",
      "----------\n",
      "train Loss: 12.5271\n",
      "val Loss: 10.5519\n",
      "\n",
      "Epoch 869/1000\n",
      "----------\n",
      "train Loss: 7.1846\n",
      "val Loss: 26.3909\n",
      "\n",
      "Epoch 870/1000\n",
      "----------\n",
      "train Loss: 15.4023\n",
      "val Loss: 10.1059\n",
      "\n",
      "Epoch 871/1000\n",
      "----------\n",
      "train Loss: 9.9213\n",
      "val Loss: 7.5101\n",
      "\n",
      "Epoch 872/1000\n",
      "----------\n",
      "train Loss: 9.9304\n",
      "val Loss: 20.0630\n",
      "\n",
      "Epoch 873/1000\n",
      "----------\n",
      "train Loss: 12.1213\n",
      "val Loss: 9.9526\n",
      "\n",
      "Epoch 874/1000\n",
      "----------\n",
      "train Loss: 9.3482\n",
      "val Loss: 10.3250\n",
      "\n",
      "Epoch 875/1000\n",
      "----------\n",
      "train Loss: 3.0088\n",
      "val Loss: 14.3138\n",
      "\n",
      "Epoch 876/1000\n",
      "----------\n",
      "train Loss: 15.1143\n",
      "val Loss: 85.8678\n",
      "\n",
      "Epoch 877/1000\n",
      "----------\n",
      "train Loss: 40.3536\n",
      "val Loss: 131.9429\n",
      "\n",
      "Epoch 878/1000\n",
      "----------\n",
      "train Loss: 13.5444\n",
      "val Loss: 13.6648\n",
      "\n",
      "Epoch 879/1000\n",
      "----------\n",
      "train Loss: 13.7903\n",
      "val Loss: 7.0187\n",
      "\n",
      "Epoch 880/1000\n",
      "----------\n",
      "train Loss: 16.7816\n",
      "val Loss: 10.2257\n",
      "\n",
      "Epoch 881/1000\n",
      "----------\n",
      "train Loss: 21.0884\n",
      "val Loss: 7.2723\n",
      "\n",
      "Epoch 882/1000\n",
      "----------\n",
      "train Loss: 8.9294\n",
      "val Loss: 10.8087\n",
      "\n",
      "Epoch 883/1000\n",
      "----------\n",
      "train Loss: 11.6069\n",
      "val Loss: 13.9310\n",
      "\n",
      "Epoch 884/1000\n",
      "----------\n",
      "train Loss: 15.9742\n",
      "val Loss: 36.4201\n",
      "\n",
      "Epoch 885/1000\n",
      "----------\n",
      "train Loss: 24.3039\n",
      "val Loss: 7.5149\n",
      "\n",
      "Epoch 886/1000\n",
      "----------\n",
      "train Loss: 11.6415\n",
      "val Loss: 30.8066\n",
      "\n",
      "Epoch 887/1000\n",
      "----------\n",
      "train Loss: 18.1902\n",
      "val Loss: 50.6277\n",
      "\n",
      "Epoch 888/1000\n",
      "----------\n",
      "train Loss: 23.0415\n",
      "val Loss: 36.3812\n",
      "\n",
      "Epoch 889/1000\n",
      "----------\n",
      "train Loss: 29.6197\n",
      "val Loss: 37.3463\n",
      "\n",
      "Epoch 890/1000\n",
      "----------\n",
      "train Loss: 13.8877\n",
      "val Loss: 6.3827\n",
      "\n",
      "Epoch 891/1000\n",
      "----------\n",
      "train Loss: 7.5995\n",
      "val Loss: 63.9030\n",
      "\n",
      "Epoch 892/1000\n",
      "----------\n",
      "train Loss: 24.0785\n",
      "val Loss: 15.9473\n",
      "\n",
      "Epoch 893/1000\n",
      "----------\n",
      "train Loss: 12.4245\n",
      "val Loss: 6.6860\n",
      "\n",
      "Epoch 894/1000\n",
      "----------\n",
      "train Loss: 9.0981\n",
      "val Loss: 87.8099\n",
      "\n",
      "Epoch 895/1000\n",
      "----------\n",
      "train Loss: 15.8229\n",
      "val Loss: 35.2853\n",
      "\n",
      "Epoch 896/1000\n",
      "----------\n",
      "train Loss: 5.3729\n",
      "val Loss: 41.8674\n",
      "\n",
      "Epoch 897/1000\n",
      "----------\n",
      "train Loss: 9.7560\n",
      "val Loss: 8.3745\n",
      "\n",
      "Epoch 898/1000\n",
      "----------\n",
      "train Loss: 8.9957\n",
      "val Loss: 16.1312\n",
      "\n",
      "Epoch 899/1000\n",
      "----------\n",
      "train Loss: 9.9530\n",
      "val Loss: 9.3295\n",
      "\n",
      "Epoch 900/1000\n",
      "----------\n",
      "train Loss: 8.6329\n",
      "val Loss: 19.9946\n",
      "\n",
      "Epoch 901/1000\n",
      "----------\n",
      "train Loss: 8.6348\n",
      "val Loss: 11.2616\n",
      "\n",
      "Epoch 902/1000\n",
      "----------\n",
      "train Loss: 11.1154\n",
      "val Loss: 38.1831\n",
      "\n",
      "Epoch 903/1000\n",
      "----------\n",
      "train Loss: 18.8765\n",
      "val Loss: 61.8377\n",
      "\n",
      "Epoch 904/1000\n",
      "----------\n",
      "train Loss: 23.5627\n",
      "val Loss: 16.8252\n",
      "\n",
      "Epoch 905/1000\n",
      "----------\n",
      "train Loss: 31.5393\n",
      "val Loss: 47.7468\n",
      "\n",
      "Epoch 906/1000\n",
      "----------\n",
      "train Loss: 36.2720\n",
      "val Loss: 77.9270\n",
      "\n",
      "Epoch 907/1000\n",
      "----------\n",
      "train Loss: 22.5848\n",
      "val Loss: 63.5182\n",
      "\n",
      "Epoch 908/1000\n",
      "----------\n",
      "train Loss: 28.4039\n",
      "val Loss: 7.6285\n",
      "\n",
      "Epoch 909/1000\n",
      "----------\n",
      "train Loss: 10.1479\n",
      "val Loss: 25.2593\n",
      "\n",
      "Epoch 910/1000\n",
      "----------\n",
      "train Loss: 6.2053\n",
      "val Loss: 5.5584\n",
      "\n",
      "Epoch 911/1000\n",
      "----------\n",
      "train Loss: 5.4719\n",
      "val Loss: 18.9734\n",
      "\n",
      "Epoch 912/1000\n",
      "----------\n",
      "train Loss: 19.7094\n",
      "val Loss: 45.7094\n",
      "\n",
      "Epoch 913/1000\n",
      "----------\n",
      "train Loss: 37.1825\n",
      "val Loss: 75.3701\n",
      "\n",
      "Epoch 914/1000\n",
      "----------\n",
      "train Loss: 34.4057\n",
      "val Loss: 17.8316\n",
      "\n",
      "Epoch 915/1000\n",
      "----------\n",
      "train Loss: 35.7717\n",
      "val Loss: 11.9029\n",
      "\n",
      "Epoch 916/1000\n",
      "----------\n",
      "train Loss: 24.7822\n",
      "val Loss: 55.6027\n",
      "\n",
      "Epoch 917/1000\n",
      "----------\n",
      "train Loss: 19.6059\n",
      "val Loss: 12.7746\n",
      "\n",
      "Epoch 918/1000\n",
      "----------\n",
      "train Loss: 7.9609\n",
      "val Loss: 5.3796\n",
      "\n",
      "Epoch 919/1000\n",
      "----------\n",
      "train Loss: 3.9322\n",
      "val Loss: 13.7253\n",
      "\n",
      "Epoch 920/1000\n",
      "----------\n",
      "train Loss: 15.1068\n",
      "val Loss: 62.2643\n",
      "\n",
      "Epoch 921/1000\n",
      "----------\n",
      "train Loss: 13.7666\n",
      "val Loss: 55.6633\n",
      "\n",
      "Epoch 922/1000\n",
      "----------\n",
      "train Loss: 43.8524\n",
      "val Loss: 32.6207\n",
      "\n",
      "Epoch 923/1000\n",
      "----------\n",
      "train Loss: 13.0991\n",
      "val Loss: 6.4412\n",
      "\n",
      "Epoch 924/1000\n",
      "----------\n",
      "train Loss: 11.2251\n",
      "val Loss: 12.2345\n",
      "\n",
      "Epoch 925/1000\n",
      "----------\n",
      "train Loss: 8.9424\n",
      "val Loss: 8.0314\n",
      "\n",
      "Epoch 926/1000\n",
      "----------\n",
      "train Loss: 7.7442\n",
      "val Loss: 20.7658\n",
      "\n",
      "Epoch 927/1000\n",
      "----------\n",
      "train Loss: 29.6538\n",
      "val Loss: 31.0457\n",
      "\n",
      "Epoch 928/1000\n",
      "----------\n",
      "train Loss: 24.5416\n",
      "val Loss: 7.2126\n",
      "\n",
      "Epoch 929/1000\n",
      "----------\n",
      "train Loss: 13.2165\n",
      "val Loss: 57.9920\n",
      "\n",
      "Epoch 930/1000\n",
      "----------\n",
      "train Loss: 8.2573\n",
      "val Loss: 10.3521\n",
      "\n",
      "Epoch 931/1000\n",
      "----------\n",
      "train Loss: 9.0355\n",
      "val Loss: 7.5951\n",
      "\n",
      "Epoch 932/1000\n",
      "----------\n",
      "train Loss: 18.5822\n",
      "val Loss: 56.7371\n",
      "\n",
      "Epoch 933/1000\n",
      "----------\n",
      "train Loss: 22.2423\n",
      "val Loss: 10.2374\n",
      "\n",
      "Epoch 934/1000\n",
      "----------\n",
      "train Loss: 6.4046\n",
      "val Loss: 5.4985\n",
      "\n",
      "Epoch 935/1000\n",
      "----------\n",
      "train Loss: 10.6952\n",
      "val Loss: 8.4324\n",
      "\n",
      "Epoch 936/1000\n",
      "----------\n",
      "train Loss: 20.6840\n",
      "val Loss: 7.7399\n",
      "\n",
      "Epoch 937/1000\n",
      "----------\n",
      "train Loss: 15.9992\n",
      "val Loss: 19.0705\n",
      "\n",
      "Epoch 938/1000\n",
      "----------\n",
      "train Loss: 10.7696\n",
      "val Loss: 32.0463\n",
      "\n",
      "Epoch 939/1000\n",
      "----------\n",
      "train Loss: 12.2672\n",
      "val Loss: 10.3242\n",
      "\n",
      "Epoch 940/1000\n",
      "----------\n",
      "train Loss: 28.5607\n",
      "val Loss: 92.4060\n",
      "\n",
      "Epoch 941/1000\n",
      "----------\n",
      "train Loss: 28.0974\n",
      "val Loss: 9.2281\n",
      "\n",
      "Epoch 942/1000\n",
      "----------\n",
      "train Loss: 16.9212\n",
      "val Loss: 54.9930\n",
      "\n",
      "Epoch 943/1000\n",
      "----------\n",
      "train Loss: 16.4105\n",
      "val Loss: 27.2532\n",
      "\n",
      "Epoch 944/1000\n",
      "----------\n",
      "train Loss: 7.6383\n",
      "val Loss: 11.0575\n",
      "\n",
      "Epoch 945/1000\n",
      "----------\n",
      "train Loss: 4.8800\n",
      "val Loss: 13.0102\n",
      "\n",
      "Epoch 946/1000\n",
      "----------\n",
      "train Loss: 6.9872\n",
      "val Loss: 8.7163\n",
      "\n",
      "Epoch 947/1000\n",
      "----------\n",
      "train Loss: 25.2857\n",
      "val Loss: 9.2185\n",
      "\n",
      "Epoch 948/1000\n",
      "----------\n",
      "train Loss: 33.4293\n",
      "val Loss: 33.8548\n",
      "\n",
      "Epoch 949/1000\n",
      "----------\n",
      "train Loss: 17.1790\n",
      "val Loss: 48.8762\n",
      "\n",
      "Epoch 950/1000\n",
      "----------\n",
      "train Loss: 32.3060\n",
      "val Loss: 9.0191\n",
      "\n",
      "Epoch 951/1000\n",
      "----------\n",
      "train Loss: 12.6955\n",
      "val Loss: 121.3810\n",
      "\n",
      "Epoch 952/1000\n",
      "----------\n",
      "train Loss: 26.9039\n",
      "val Loss: 20.8125\n",
      "\n",
      "Epoch 953/1000\n",
      "----------\n",
      "train Loss: 15.6034\n",
      "val Loss: 21.6022\n",
      "\n",
      "Epoch 954/1000\n",
      "----------\n",
      "train Loss: 4.1228\n",
      "val Loss: 8.3807\n",
      "\n",
      "Epoch 955/1000\n",
      "----------\n",
      "train Loss: 3.4737\n",
      "val Loss: 10.1064\n",
      "\n",
      "Epoch 956/1000\n",
      "----------\n",
      "train Loss: 10.3152\n",
      "val Loss: 10.6868\n",
      "\n",
      "Epoch 957/1000\n",
      "----------\n",
      "train Loss: 19.6457\n",
      "val Loss: 9.5686\n",
      "\n",
      "Epoch 958/1000\n",
      "----------\n",
      "train Loss: 11.2900\n",
      "val Loss: 17.3522\n",
      "\n",
      "Epoch 959/1000\n",
      "----------\n",
      "train Loss: 3.4270\n",
      "val Loss: 43.8046\n",
      "\n",
      "Epoch 960/1000\n",
      "----------\n",
      "train Loss: 18.4069\n",
      "val Loss: 48.2812\n",
      "\n",
      "Epoch 961/1000\n",
      "----------\n",
      "train Loss: 31.4846\n",
      "val Loss: 9.3677\n",
      "\n",
      "Epoch 962/1000\n",
      "----------\n",
      "train Loss: 17.1343\n",
      "val Loss: 11.3262\n",
      "\n",
      "Epoch 963/1000\n",
      "----------\n",
      "train Loss: 7.7558\n",
      "val Loss: 56.6514\n",
      "\n",
      "Epoch 964/1000\n",
      "----------\n",
      "train Loss: 9.7201\n",
      "val Loss: 54.6315\n",
      "\n",
      "Epoch 965/1000\n",
      "----------\n",
      "train Loss: 6.2031\n",
      "val Loss: 22.9961\n",
      "\n",
      "Epoch 966/1000\n",
      "----------\n",
      "train Loss: 14.5646\n",
      "val Loss: 49.6996\n",
      "\n",
      "Epoch 967/1000\n",
      "----------\n",
      "train Loss: 16.1370\n",
      "val Loss: 12.5715\n",
      "\n",
      "Epoch 968/1000\n",
      "----------\n",
      "train Loss: 21.2537\n",
      "val Loss: 8.1431\n",
      "\n",
      "Epoch 969/1000\n",
      "----------\n",
      "train Loss: 17.3270\n",
      "val Loss: 22.7161\n",
      "\n",
      "Epoch 970/1000\n",
      "----------\n",
      "train Loss: 7.6493\n",
      "val Loss: 14.9313\n",
      "\n",
      "Epoch 971/1000\n",
      "----------\n",
      "train Loss: 15.6033\n",
      "val Loss: 43.9544\n",
      "\n",
      "Epoch 972/1000\n",
      "----------\n",
      "train Loss: 19.8844\n",
      "val Loss: 8.0559\n",
      "\n",
      "Epoch 973/1000\n",
      "----------\n",
      "train Loss: 12.6865\n",
      "val Loss: 7.6254\n",
      "\n",
      "Epoch 974/1000\n",
      "----------\n",
      "train Loss: 10.2411\n",
      "val Loss: 15.0428\n",
      "\n",
      "Epoch 975/1000\n",
      "----------\n",
      "train Loss: 17.5925\n",
      "val Loss: 41.2136\n",
      "\n",
      "Epoch 976/1000\n",
      "----------\n",
      "train Loss: 21.4415\n",
      "val Loss: 75.1815\n",
      "\n",
      "Epoch 977/1000\n",
      "----------\n",
      "train Loss: 17.2860\n",
      "val Loss: 10.0709\n",
      "\n",
      "Epoch 978/1000\n",
      "----------\n",
      "train Loss: 13.3140\n",
      "val Loss: 34.9651\n",
      "\n",
      "Epoch 979/1000\n",
      "----------\n",
      "train Loss: 44.9492\n",
      "val Loss: 31.1537\n",
      "\n",
      "Epoch 980/1000\n",
      "----------\n",
      "train Loss: 19.5203\n",
      "val Loss: 25.8650\n",
      "\n",
      "Epoch 981/1000\n",
      "----------\n",
      "train Loss: 7.0695\n",
      "val Loss: 29.4368\n",
      "\n",
      "Epoch 982/1000\n",
      "----------\n",
      "train Loss: 4.3752\n",
      "val Loss: 8.5365\n",
      "\n",
      "Epoch 983/1000\n",
      "----------\n",
      "train Loss: 8.4214\n",
      "val Loss: 26.2183\n",
      "\n",
      "Epoch 984/1000\n",
      "----------\n",
      "train Loss: 9.9061\n",
      "val Loss: 55.2924\n",
      "\n",
      "Epoch 985/1000\n",
      "----------\n",
      "train Loss: 23.6154\n",
      "val Loss: 61.5555\n",
      "\n",
      "Epoch 986/1000\n",
      "----------\n",
      "train Loss: 15.0185\n",
      "val Loss: 63.2674\n",
      "\n",
      "Epoch 987/1000\n",
      "----------\n",
      "train Loss: 30.1588\n",
      "val Loss: 36.4462\n",
      "\n",
      "Epoch 988/1000\n",
      "----------\n",
      "train Loss: 9.5751\n",
      "val Loss: 6.8739\n",
      "\n",
      "Epoch 989/1000\n",
      "----------\n",
      "train Loss: 8.5264\n",
      "val Loss: 18.0618\n",
      "\n",
      "Epoch 990/1000\n",
      "----------\n",
      "train Loss: 4.4178\n",
      "val Loss: 7.1490\n",
      "\n",
      "Epoch 991/1000\n",
      "----------\n",
      "train Loss: 5.8079\n",
      "val Loss: 20.3565\n",
      "\n",
      "Epoch 992/1000\n",
      "----------\n",
      "train Loss: 16.4689\n",
      "val Loss: 40.6384\n",
      "\n",
      "Epoch 993/1000\n",
      "----------\n",
      "train Loss: 9.3796\n",
      "val Loss: 17.8496\n",
      "\n",
      "Epoch 994/1000\n",
      "----------\n",
      "train Loss: 45.4632\n",
      "val Loss: 164.0010\n",
      "\n",
      "Epoch 995/1000\n",
      "----------\n",
      "train Loss: 42.2945\n",
      "val Loss: 35.8039\n",
      "\n",
      "Epoch 996/1000\n",
      "----------\n",
      "train Loss: 9.4282\n",
      "val Loss: 11.0226\n",
      "\n",
      "Epoch 997/1000\n",
      "----------\n",
      "train Loss: 11.4366\n",
      "val Loss: 37.8112\n",
      "\n",
      "Epoch 998/1000\n",
      "----------\n",
      "train Loss: 12.8327\n",
      "val Loss: 5.9570\n",
      "\n",
      "Epoch 999/1000\n",
      "----------\n",
      "train Loss: 29.6606\n",
      "val Loss: 102.4946\n",
      "\n",
      "Epoch 1000/1000\n",
      "----------\n",
      "train Loss: 69.6689\n",
      "val Loss: 24.2758\n",
      "\n",
      "Training complete in 17m 28s\n",
      "Best val Loss: 4.591955\n",
      "model saved to ./checkpoints/A_normal_model\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for dataT in ['mixed', 'normal']:\n",
    "    args.data_type = dataT\n",
    "    train_set, train_loader = data_provider(args, 'train')\n",
    "    train_c_set, train_c_loader = data_provider(args, 'train_c')\n",
    "    valid_set, valid_loader = data_provider(args, 'valid')\n",
    "    test_set, test_loader = data_provider(args, 'test')\n",
    "    total_set, total_loader = data_provider(args, 'pred')\n",
    "    \n",
    "    if args.data == '3D/new':\n",
    "        for chT in ['Rmi', 'Vmi', 'Wmi', 'Accm', 'Ang', 'Fout', 'Fcmd']:\n",
    "            ch = {'Rmi':3, 'Vmi':3, 'Wmi':3, 'Accm':3, 'Ang':3, 'Fout':4, 'Fcmd':4}\n",
    "            args.input_channel = ch[chT]\n",
    "            model = TrAE(ConvEncoder, ConvDecoder, args).to(args.device)\n",
    "            criterion = nn.MSELoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "            data_loaders = {'train': train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "            trained_model, best_model_wts, train_loss_history, val_loss_history = AE_train(model, data_loaders, criterion, optimizer, num_epochs=args.train_epochs, channels = args.input_channel, channel_type = chT)\n",
    "            torch.save(trained_model.state_dict(), args.checkpoints+args.model+'_'+args.data_type+'_'+chT+'_model.pth')\n",
    "            torch.save(trained_model.encoder.state_dict(), args.checkpoints+args.model+'_'+args.data_type+'_'+chT+'_model_e.pth')\n",
    "            trained_model.load_state_dict(best_model_wts)\n",
    "            torch.save(trained_model.state_dict(), args.checkpoints+args.model+'_'+args.data_type+'_'+chT+'_best_model.pth')\n",
    "            torch.save(trained_model.encoder.state_dict(), args.checkpoints+args.model+'_'+args.data_type+'_'+chT+'_best_model_e.pth')\n",
    "            print('model saved to %s' % args.checkpoints+args.model+'_'+args.data_type+'_'+chT+'_model')\n",
    "            \n",
    "            plt.plot(train_loss_history, label='train')\n",
    "            plt.plot(val_loss_history, label='val')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    else:\n",
    "        model = TrAE(ConvEncoder, ConvDecoder, args).to(args.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "        data_loaders = {'train': train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "        trained_model, best_model_wts, train_loss_history, val_loss_history = AE_train(model, data_loaders, criterion, optimizer, num_epochs=args.train_epochs, channels = args.input_channel)\n",
    "        torch.save(trained_model.state_dict(), args.checkpoints+args.model+'_'+args.data_type+'_model.pth')\n",
    "        torch.save(trained_model.encoder.state_dict(), args.checkpoints+args.model+'_'+args.data_type+'_model_e.pth')\n",
    "        trained_model.load_state_dict(best_model_wts)\n",
    "        torch.save(trained_model.state_dict(), args.checkpoints+args.model+'_'+args.data_type+'_best_model.pth')\n",
    "        torch.save(trained_model.encoder.state_dict(), args.checkpoints+args.model+'_'+args.data_type+'_best_model_e.pth')\n",
    "        print('model saved to %s' % args.checkpoints+args.model+'_'+args.data_type+'_model')\n",
    "        \n",
    "        plt.plot(train_loss_history, label='train')\n",
    "        plt.plot(val_loss_history, label='val')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's draw a learning curve like below.\n",
    "plt.plot(train_loss_history, label='train')\n",
    "plt.plot(val_loss_history, label='val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299.7319561767578\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in data_loaders[\"test\"]:\n",
    "        inputs = inputs.to(args.device)[:,:,:2].type(torch.float32).transpose(1, 2)\n",
    "\n",
    "        outputs, encoded = trained_model(inputs)\n",
    "        test_loss = criterion(outputs, inputs)\n",
    "        \n",
    "        running_loss += test_loss.item() * inputs.size(0)\n",
    "\n",
    "    test_loss = running_loss / len(data_loaders[\"test\"].dataset)\n",
    "    print(test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "dataset_array = total_set.data_x / 255\n",
    "dataset_array = np.float32(dataset_array)\n",
    "labels = total_set.data_y\n",
    "\n",
    "dataset_array = torch.tensor(dataset_array)\n",
    "inputs = dataset_array.to(args.device)[:,:,:2].type(torch.float32).transpose(1, 2)\n",
    "outputs, encoded = trained_model(inputs)\n",
    "\n",
    "encoded = encoded.cpu().detach().numpy().reshape(encoded.size(0), -1)\n",
    "tsne = TSNE()   \n",
    "X_test_2D = tsne.fit_transform(encoded)\n",
    "X_test_2D = (X_test_2D - X_test_2D.min()) / (X_test_2D.max() - X_test_2D.min())\n",
    "\n",
    "plt.scatter(X_test_2D[:, 0], X_test_2D[:, 1], c=labels, s=10, cmap=\"tab10\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConvEncoder(args.input_channel, args.t, args.output_channel)\n",
    "encoder.load_state_dict(torch.load(args.checkpoints+f'{args.model}_{args.data_type}_model_e.pth'))\n",
    "encoder = encoder.to(args.device)\n",
    "classifier = Classifier(args.seq_len//(2**args.layer_num)*args.class_num, args.class_num).to(args.device)\n",
    "classifier_lossf = nn.MSELoss()\n",
    "# classifier_lossf = nn.CrossEntropyLoss()\n",
    "classifier_optim = optim.Adam(classifier.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AC_train(encoder, classifier, dataloaders, criterion, optimizer, num_epochs=10, channels = 2, channel_type = None):\n",
    "    \"\"\"\n",
    "    model: model to train\n",
    "    dataloaders: train, val, test data's loader\n",
    "    criterion: loss function\n",
    "    optimizer: optimizer to update your model\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(classifier.state_dict())\n",
    "    best_val_loss = 100000000\n",
    "    \n",
    "    if channel_type == None:\n",
    "        cl = 0\n",
    "    elif channel_type == 'Rmi':\n",
    "        cl = 0\n",
    "    elif channel_type == 'Vmi':\n",
    "        cl = 3\n",
    "    elif channel_type == 'Wmb':\n",
    "        cl = 6\n",
    "    elif channel_type == 'Accm':\n",
    "        cl = 9\n",
    "    elif channel_type == 'ang':\n",
    "        cl = 12\n",
    "    elif channel_type == 'Fout':\n",
    "        cl = 15\n",
    "    elif channel_type == 'Fcmd':\n",
    "        cl = 19\n",
    "    cr = cl + channels\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            encoder.eval()\n",
    "            if phase == 'train':\n",
    "                classifier.train()            # Set model to training mode\n",
    "            else:\n",
    "                classifier.eval()            # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(args.device)[:,:,cl:cr].type(torch.float32).transpose(1, 2)\n",
    "                labels = F.one_hot(labels, num_classes=4).to(args.device).type(torch.float32)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    hidden = encoder(inputs)\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = classifier(hidden.reshape(hidden.size(0), -1)).type(torch.float32)\n",
    "                    # print(outputs.shape, labels.shape)\n",
    "                    # loss = criterion(outputs.view(-1), labels.view(-1))\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        # loss.requires_grad = True\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()        \n",
    "            epoch_loss = running_loss #/ len(dataloaders[phase].dataset)\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            if phase == 'val':\n",
    "                val_loss_history.append(epoch_loss)\n",
    "            if phase == 'val' and epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(classifier.state_dict())\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Loss: {:4f}'.format(best_val_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    # classifier.load_state_dict(best_model_wts)\n",
    "    return classifier, best_model_wts, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "----------\n",
      "train Loss: 12.6638\n",
      "val Loss: 1.7205\n",
      "\n",
      "Epoch 2/2000\n",
      "----------\n",
      "train Loss: 3.6783\n",
      "val Loss: 1.2258\n",
      "\n",
      "Epoch 3/2000\n",
      "----------\n",
      "train Loss: 3.2092\n",
      "val Loss: 1.1392\n",
      "\n",
      "Epoch 4/2000\n",
      "----------\n",
      "train Loss: 3.0799\n",
      "val Loss: 1.0894\n",
      "\n",
      "Epoch 5/2000\n",
      "----------\n",
      "train Loss: 2.8471\n",
      "val Loss: 0.9992\n",
      "\n",
      "Epoch 6/2000\n",
      "----------\n",
      "train Loss: 2.6299\n",
      "val Loss: 0.9398\n",
      "\n",
      "Epoch 7/2000\n",
      "----------\n",
      "train Loss: 2.5224\n",
      "val Loss: 0.9193\n",
      "\n",
      "Epoch 8/2000\n",
      "----------\n",
      "train Loss: 2.4457\n",
      "val Loss: 0.8590\n",
      "\n",
      "Epoch 9/2000\n",
      "----------\n",
      "train Loss: 2.3655\n",
      "val Loss: 0.8803\n",
      "\n",
      "Epoch 10/2000\n",
      "----------\n",
      "train Loss: 2.2915\n",
      "val Loss: 0.8166\n",
      "\n",
      "Epoch 11/2000\n",
      "----------\n",
      "train Loss: 2.2053\n",
      "val Loss: 0.9141\n",
      "\n",
      "Epoch 12/2000\n",
      "----------\n",
      "train Loss: 2.3637\n",
      "val Loss: 0.8675\n",
      "\n",
      "Epoch 13/2000\n",
      "----------\n",
      "train Loss: 2.1966\n",
      "val Loss: 0.8334\n",
      "\n",
      "Epoch 14/2000\n",
      "----------\n",
      "train Loss: 2.0661\n",
      "val Loss: 0.8098\n",
      "\n",
      "Epoch 15/2000\n",
      "----------\n",
      "train Loss: 1.9707\n",
      "val Loss: 0.7206\n",
      "\n",
      "Epoch 16/2000\n",
      "----------\n",
      "train Loss: 1.8420\n",
      "val Loss: 0.7117\n",
      "\n",
      "Epoch 17/2000\n",
      "----------\n",
      "train Loss: 1.8473\n",
      "val Loss: 0.8145\n",
      "\n",
      "Epoch 18/2000\n",
      "----------\n",
      "train Loss: 1.9945\n",
      "val Loss: 0.6534\n",
      "\n",
      "Epoch 19/2000\n",
      "----------\n",
      "train Loss: 1.7069\n",
      "val Loss: 0.5839\n",
      "\n",
      "Epoch 20/2000\n",
      "----------\n",
      "train Loss: 1.6124\n",
      "val Loss: 0.8038\n",
      "\n",
      "Epoch 21/2000\n",
      "----------\n",
      "train Loss: 1.9538\n",
      "val Loss: 0.5975\n",
      "\n",
      "Epoch 22/2000\n",
      "----------\n",
      "train Loss: 1.6456\n",
      "val Loss: 0.5081\n",
      "\n",
      "Epoch 23/2000\n",
      "----------\n",
      "train Loss: 1.3235\n",
      "val Loss: 0.5037\n",
      "\n",
      "Epoch 24/2000\n",
      "----------\n",
      "train Loss: 1.7263\n",
      "val Loss: 0.5453\n",
      "\n",
      "Epoch 25/2000\n",
      "----------\n",
      "train Loss: 1.3753\n",
      "val Loss: 0.5339\n",
      "\n",
      "Epoch 26/2000\n",
      "----------\n",
      "train Loss: 1.0361\n",
      "val Loss: 0.3969\n",
      "\n",
      "Epoch 27/2000\n",
      "----------\n",
      "train Loss: 0.9654\n",
      "val Loss: 0.3369\n",
      "\n",
      "Epoch 28/2000\n",
      "----------\n",
      "train Loss: 0.8464\n",
      "val Loss: 0.3289\n",
      "\n",
      "Epoch 29/2000\n",
      "----------\n",
      "train Loss: 1.0203\n",
      "val Loss: 0.6171\n",
      "\n",
      "Epoch 30/2000\n",
      "----------\n",
      "train Loss: 1.4182\n",
      "val Loss: 0.5929\n",
      "\n",
      "Epoch 31/2000\n",
      "----------\n",
      "train Loss: 1.1836\n",
      "val Loss: 0.3676\n",
      "\n",
      "Epoch 32/2000\n",
      "----------\n",
      "train Loss: 0.8936\n",
      "val Loss: 0.3416\n",
      "\n",
      "Epoch 33/2000\n",
      "----------\n",
      "train Loss: 0.8172\n",
      "val Loss: 0.3240\n",
      "\n",
      "Epoch 34/2000\n",
      "----------\n",
      "train Loss: 0.9495\n",
      "val Loss: 0.3626\n",
      "\n",
      "Epoch 35/2000\n",
      "----------\n",
      "train Loss: 1.0111\n",
      "val Loss: 0.5555\n",
      "\n",
      "Epoch 36/2000\n",
      "----------\n",
      "train Loss: 1.0966\n",
      "val Loss: 0.3120\n",
      "\n",
      "Epoch 37/2000\n",
      "----------\n",
      "train Loss: 0.7875\n",
      "val Loss: 0.3423\n",
      "\n",
      "Epoch 38/2000\n",
      "----------\n",
      "train Loss: 0.7264\n",
      "val Loss: 0.2849\n",
      "\n",
      "Epoch 39/2000\n",
      "----------\n",
      "train Loss: 0.7146\n",
      "val Loss: 0.3341\n",
      "\n",
      "Epoch 40/2000\n",
      "----------\n",
      "train Loss: 0.7165\n",
      "val Loss: 0.2681\n",
      "\n",
      "Epoch 41/2000\n",
      "----------\n",
      "train Loss: 0.8028\n",
      "val Loss: 0.4888\n",
      "\n",
      "Epoch 42/2000\n",
      "----------\n",
      "train Loss: 0.8172\n",
      "val Loss: 0.2806\n",
      "\n",
      "Epoch 43/2000\n",
      "----------\n",
      "train Loss: 0.6685\n",
      "val Loss: 0.2315\n",
      "\n",
      "Epoch 44/2000\n",
      "----------\n",
      "train Loss: 0.6728\n",
      "val Loss: 0.2532\n",
      "\n",
      "Epoch 45/2000\n",
      "----------\n",
      "train Loss: 0.6193\n",
      "val Loss: 0.2392\n",
      "\n",
      "Epoch 46/2000\n",
      "----------\n",
      "train Loss: 0.9202\n",
      "val Loss: 0.3736\n",
      "\n",
      "Epoch 47/2000\n",
      "----------\n",
      "train Loss: 1.7880\n",
      "val Loss: 0.5582\n",
      "\n",
      "Epoch 48/2000\n",
      "----------\n",
      "train Loss: 1.1050\n",
      "val Loss: 0.3570\n",
      "\n",
      "Epoch 49/2000\n",
      "----------\n",
      "train Loss: 1.0048\n",
      "val Loss: 0.3966\n",
      "\n",
      "Epoch 50/2000\n",
      "----------\n",
      "train Loss: 0.8908\n",
      "val Loss: 0.2794\n",
      "\n",
      "Epoch 51/2000\n",
      "----------\n",
      "train Loss: 0.9556\n",
      "val Loss: 0.3303\n",
      "\n",
      "Epoch 52/2000\n",
      "----------\n",
      "train Loss: 1.0244\n",
      "val Loss: 0.3455\n",
      "\n",
      "Epoch 53/2000\n",
      "----------\n",
      "train Loss: 1.0087\n",
      "val Loss: 0.3542\n",
      "\n",
      "Epoch 54/2000\n",
      "----------\n",
      "train Loss: 0.8772\n",
      "val Loss: 0.2884\n",
      "\n",
      "Epoch 55/2000\n",
      "----------\n",
      "train Loss: 0.8432\n",
      "val Loss: 0.3483\n",
      "\n",
      "Epoch 56/2000\n",
      "----------\n",
      "train Loss: 0.7358\n",
      "val Loss: 0.3390\n",
      "\n",
      "Epoch 57/2000\n",
      "----------\n",
      "train Loss: 0.7292\n",
      "val Loss: 0.2909\n",
      "\n",
      "Epoch 58/2000\n",
      "----------\n",
      "train Loss: 0.7193\n",
      "val Loss: 0.3139\n",
      "\n",
      "Epoch 59/2000\n",
      "----------\n",
      "train Loss: 0.7489\n",
      "val Loss: 0.3107\n",
      "\n",
      "Epoch 60/2000\n",
      "----------\n",
      "train Loss: 0.7205\n",
      "val Loss: 0.3330\n",
      "\n",
      "Epoch 61/2000\n",
      "----------\n",
      "train Loss: 0.8199\n",
      "val Loss: 0.3289\n",
      "\n",
      "Epoch 62/2000\n",
      "----------\n",
      "train Loss: 1.2383\n",
      "val Loss: 0.3949\n",
      "\n",
      "Epoch 63/2000\n",
      "----------\n",
      "train Loss: 0.8139\n",
      "val Loss: 0.2635\n",
      "\n",
      "Epoch 64/2000\n",
      "----------\n",
      "train Loss: 0.6770\n",
      "val Loss: 0.3084\n",
      "\n",
      "Epoch 65/2000\n",
      "----------\n",
      "train Loss: 0.6451\n",
      "val Loss: 0.2460\n",
      "\n",
      "Epoch 66/2000\n",
      "----------\n",
      "train Loss: 0.7070\n",
      "val Loss: 0.3364\n",
      "\n",
      "Epoch 67/2000\n",
      "----------\n",
      "train Loss: 0.8228\n",
      "val Loss: 0.2889\n",
      "\n",
      "Epoch 68/2000\n",
      "----------\n",
      "train Loss: 0.6762\n",
      "val Loss: 0.2781\n",
      "\n",
      "Epoch 69/2000\n",
      "----------\n",
      "train Loss: 0.6874\n",
      "val Loss: 0.2570\n",
      "\n",
      "Epoch 70/2000\n",
      "----------\n",
      "train Loss: 0.6802\n",
      "val Loss: 0.3097\n",
      "\n",
      "Epoch 71/2000\n",
      "----------\n",
      "train Loss: 0.6090\n",
      "val Loss: 0.2073\n",
      "\n",
      "Epoch 72/2000\n",
      "----------\n",
      "train Loss: 0.6951\n",
      "val Loss: 0.3074\n",
      "\n",
      "Epoch 73/2000\n",
      "----------\n",
      "train Loss: 0.6496\n",
      "val Loss: 0.2169\n",
      "\n",
      "Epoch 74/2000\n",
      "----------\n",
      "train Loss: 0.5591\n",
      "val Loss: 0.2560\n",
      "\n",
      "Epoch 75/2000\n",
      "----------\n",
      "train Loss: 0.6121\n",
      "val Loss: 0.3382\n",
      "\n",
      "Epoch 76/2000\n",
      "----------\n",
      "train Loss: 0.7321\n",
      "val Loss: 0.2745\n",
      "\n",
      "Epoch 77/2000\n",
      "----------\n",
      "train Loss: 0.6416\n",
      "val Loss: 0.2541\n",
      "\n",
      "Epoch 78/2000\n",
      "----------\n",
      "train Loss: 0.6353\n",
      "val Loss: 0.2360\n",
      "\n",
      "Epoch 79/2000\n",
      "----------\n",
      "train Loss: 0.6091\n",
      "val Loss: 0.2346\n",
      "\n",
      "Epoch 80/2000\n",
      "----------\n",
      "train Loss: 0.5106\n",
      "val Loss: 0.2081\n",
      "\n",
      "Epoch 81/2000\n",
      "----------\n",
      "train Loss: 0.7728\n",
      "val Loss: 0.6511\n",
      "\n",
      "Epoch 82/2000\n",
      "----------\n",
      "train Loss: 0.9529\n",
      "val Loss: 0.2851\n",
      "\n",
      "Epoch 83/2000\n",
      "----------\n",
      "train Loss: 0.6557\n",
      "val Loss: 0.3197\n",
      "\n",
      "Epoch 84/2000\n",
      "----------\n",
      "train Loss: 0.5863\n",
      "val Loss: 0.2345\n",
      "\n",
      "Epoch 85/2000\n",
      "----------\n",
      "train Loss: 0.5928\n",
      "val Loss: 0.1949\n",
      "\n",
      "Epoch 86/2000\n",
      "----------\n",
      "train Loss: 0.4842\n",
      "val Loss: 0.2077\n",
      "\n",
      "Epoch 87/2000\n",
      "----------\n",
      "train Loss: 0.4714\n",
      "val Loss: 0.2273\n",
      "\n",
      "Epoch 88/2000\n",
      "----------\n",
      "train Loss: 0.5628\n",
      "val Loss: 0.2707\n",
      "\n",
      "Epoch 89/2000\n",
      "----------\n",
      "train Loss: 0.7842\n",
      "val Loss: 0.5835\n",
      "\n",
      "Epoch 90/2000\n",
      "----------\n",
      "train Loss: 1.1268\n",
      "val Loss: 0.2839\n",
      "\n",
      "Epoch 91/2000\n",
      "----------\n",
      "train Loss: 0.7469\n",
      "val Loss: 0.2787\n",
      "\n",
      "Epoch 92/2000\n",
      "----------\n",
      "train Loss: 0.5697\n",
      "val Loss: 0.1949\n",
      "\n",
      "Epoch 93/2000\n",
      "----------\n",
      "train Loss: 0.5041\n",
      "val Loss: 0.2031\n",
      "\n",
      "Epoch 94/2000\n",
      "----------\n",
      "train Loss: 0.8330\n",
      "val Loss: 0.2234\n",
      "\n",
      "Epoch 95/2000\n",
      "----------\n",
      "train Loss: 0.5754\n",
      "val Loss: 0.1769\n",
      "\n",
      "Epoch 96/2000\n",
      "----------\n",
      "train Loss: 0.7014\n",
      "val Loss: 0.5558\n",
      "\n",
      "Epoch 97/2000\n",
      "----------\n",
      "train Loss: 1.6272\n",
      "val Loss: 1.2844\n",
      "\n",
      "Epoch 98/2000\n",
      "----------\n",
      "train Loss: 1.6254\n",
      "val Loss: 0.2887\n",
      "\n",
      "Epoch 99/2000\n",
      "----------\n",
      "train Loss: 0.6290\n",
      "val Loss: 0.2019\n",
      "\n",
      "Epoch 100/2000\n",
      "----------\n",
      "train Loss: 0.5397\n",
      "val Loss: 0.1945\n",
      "\n",
      "Epoch 101/2000\n",
      "----------\n",
      "train Loss: 0.4763\n",
      "val Loss: 0.2054\n",
      "\n",
      "Epoch 102/2000\n",
      "----------\n",
      "train Loss: 0.5210\n",
      "val Loss: 0.2155\n",
      "\n",
      "Epoch 103/2000\n",
      "----------\n",
      "train Loss: 0.5303\n",
      "val Loss: 0.2183\n",
      "\n",
      "Epoch 104/2000\n",
      "----------\n",
      "train Loss: 0.5013\n",
      "val Loss: 0.1946\n",
      "\n",
      "Epoch 105/2000\n",
      "----------\n",
      "train Loss: 0.6465\n",
      "val Loss: 0.3558\n",
      "\n",
      "Epoch 106/2000\n",
      "----------\n",
      "train Loss: 0.7363\n",
      "val Loss: 0.1894\n",
      "\n",
      "Epoch 107/2000\n",
      "----------\n",
      "train Loss: 0.5442\n",
      "val Loss: 0.2066\n",
      "\n",
      "Epoch 108/2000\n",
      "----------\n",
      "train Loss: 0.6170\n",
      "val Loss: 0.2620\n",
      "\n",
      "Epoch 109/2000\n",
      "----------\n",
      "train Loss: 1.2781\n",
      "val Loss: 0.4931\n",
      "\n",
      "Epoch 110/2000\n",
      "----------\n",
      "train Loss: 0.8567\n",
      "val Loss: 0.2371\n",
      "\n",
      "Epoch 111/2000\n",
      "----------\n",
      "train Loss: 0.6289\n",
      "val Loss: 0.2288\n",
      "\n",
      "Epoch 112/2000\n",
      "----------\n",
      "train Loss: 0.6203\n",
      "val Loss: 0.2213\n",
      "\n",
      "Epoch 113/2000\n",
      "----------\n",
      "train Loss: 0.6395\n",
      "val Loss: 0.2550\n",
      "\n",
      "Epoch 114/2000\n",
      "----------\n",
      "train Loss: 0.6794\n",
      "val Loss: 0.2822\n",
      "\n",
      "Epoch 115/2000\n",
      "----------\n",
      "train Loss: 0.5311\n",
      "val Loss: 0.1849\n",
      "\n",
      "Epoch 116/2000\n",
      "----------\n",
      "train Loss: 0.4867\n",
      "val Loss: 0.2050\n",
      "\n",
      "Epoch 117/2000\n",
      "----------\n",
      "train Loss: 0.6455\n",
      "val Loss: 0.2496\n",
      "\n",
      "Epoch 118/2000\n",
      "----------\n",
      "train Loss: 1.0755\n",
      "val Loss: 0.3755\n",
      "\n",
      "Epoch 119/2000\n",
      "----------\n",
      "train Loss: 0.6978\n",
      "val Loss: 0.2125\n",
      "\n",
      "Epoch 120/2000\n",
      "----------\n",
      "train Loss: 0.4597\n",
      "val Loss: 0.1826\n",
      "\n",
      "Epoch 121/2000\n",
      "----------\n",
      "train Loss: 0.4656\n",
      "val Loss: 0.1560\n",
      "\n",
      "Epoch 122/2000\n",
      "----------\n",
      "train Loss: 0.5344\n",
      "val Loss: 0.2836\n",
      "\n",
      "Epoch 123/2000\n",
      "----------\n",
      "train Loss: 0.9264\n",
      "val Loss: 0.3548\n",
      "\n",
      "Epoch 124/2000\n",
      "----------\n",
      "train Loss: 0.7319\n",
      "val Loss: 0.2127\n",
      "\n",
      "Epoch 125/2000\n",
      "----------\n",
      "train Loss: 0.5259\n",
      "val Loss: 0.1751\n",
      "\n",
      "Epoch 126/2000\n",
      "----------\n",
      "train Loss: 0.5614\n",
      "val Loss: 0.2821\n",
      "\n",
      "Epoch 127/2000\n",
      "----------\n",
      "train Loss: 0.4636\n",
      "val Loss: 0.2579\n",
      "\n",
      "Epoch 128/2000\n",
      "----------\n",
      "train Loss: 0.4473\n",
      "val Loss: 0.1834\n",
      "\n",
      "Epoch 129/2000\n",
      "----------\n",
      "train Loss: 0.5182\n",
      "val Loss: 0.2925\n",
      "\n",
      "Epoch 130/2000\n",
      "----------\n",
      "train Loss: 0.5947\n",
      "val Loss: 0.1827\n",
      "\n",
      "Epoch 131/2000\n",
      "----------\n",
      "train Loss: 0.4750\n",
      "val Loss: 0.2954\n",
      "\n",
      "Epoch 132/2000\n",
      "----------\n",
      "train Loss: 0.4496\n",
      "val Loss: 0.2502\n",
      "\n",
      "Epoch 133/2000\n",
      "----------\n",
      "train Loss: 0.4385\n",
      "val Loss: 0.1958\n",
      "\n",
      "Epoch 134/2000\n",
      "----------\n",
      "train Loss: 0.3943\n",
      "val Loss: 0.1722\n",
      "\n",
      "Epoch 135/2000\n",
      "----------\n",
      "train Loss: 0.4157\n",
      "val Loss: 0.1516\n",
      "\n",
      "Epoch 136/2000\n",
      "----------\n",
      "train Loss: 0.3822\n",
      "val Loss: 0.1514\n",
      "\n",
      "Epoch 137/2000\n",
      "----------\n",
      "train Loss: 0.5611\n",
      "val Loss: 0.1763\n",
      "\n",
      "Epoch 138/2000\n",
      "----------\n",
      "train Loss: 0.5376\n",
      "val Loss: 0.1711\n",
      "\n",
      "Epoch 139/2000\n",
      "----------\n",
      "train Loss: 0.4328\n",
      "val Loss: 0.1671\n",
      "\n",
      "Epoch 140/2000\n",
      "----------\n",
      "train Loss: 0.4817\n",
      "val Loss: 0.2749\n",
      "\n",
      "Epoch 141/2000\n",
      "----------\n",
      "train Loss: 0.9343\n",
      "val Loss: 0.4758\n",
      "\n",
      "Epoch 142/2000\n",
      "----------\n",
      "train Loss: 0.9440\n",
      "val Loss: 0.1795\n",
      "\n",
      "Epoch 143/2000\n",
      "----------\n",
      "train Loss: 0.4453\n",
      "val Loss: 0.1508\n",
      "\n",
      "Epoch 144/2000\n",
      "----------\n",
      "train Loss: 0.4161\n",
      "val Loss: 0.1994\n",
      "\n",
      "Epoch 145/2000\n",
      "----------\n",
      "train Loss: 0.5290\n",
      "val Loss: 0.2098\n",
      "\n",
      "Epoch 146/2000\n",
      "----------\n",
      "train Loss: 0.6021\n",
      "val Loss: 0.2380\n",
      "\n",
      "Epoch 147/2000\n",
      "----------\n",
      "train Loss: 0.4993\n",
      "val Loss: 0.2365\n",
      "\n",
      "Epoch 148/2000\n",
      "----------\n",
      "train Loss: 0.4460\n",
      "val Loss: 0.1797\n",
      "\n",
      "Epoch 149/2000\n",
      "----------\n",
      "train Loss: 0.4315\n",
      "val Loss: 0.1655\n",
      "\n",
      "Epoch 150/2000\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for dataT in ['mixed', 'normal']:\n",
    "    if dataT == 'mixed':\n",
    "        continue\n",
    "    args.data_type = dataT\n",
    "    \n",
    "    train_set, train_loader = data_provider(args, 'train')\n",
    "    train_c_set, train_c_loader = data_provider(args, 'train_c')\n",
    "    valid_set, valid_loader = data_provider(args, 'valid')\n",
    "    test_set, test_loader = data_provider(args, 'test')\n",
    "    total_set, total_loader = data_provider(args, 'pred')\n",
    "    \n",
    "    if args.data == '3D/new':\n",
    "        for chT in ['Rmi', 'Vmi', 'Wmi', 'Accm', 'Ang', 'Fout', 'Fcmd']:\n",
    "            ch = {'Rmi':3, 'Vmi':3, 'Wmi':3, 'Accm':3, 'Ang':3, 'Fout':4, 'Fcmd':4}\n",
    "            args.input_channel = ch[chT]\n",
    "    \n",
    "            for lossf in ['MSE', 'CE']:\n",
    "                if lossf == 'MSE':\n",
    "                    classifier_lossf = nn.MSELoss()\n",
    "                elif lossf == 'CE':\n",
    "                    classifier_lossf = nn.CrossEntropyLoss()\n",
    "                encoder = ConvEncoder(args.input_channel, args.t, args.output_channel)\n",
    "                encoder.load_state_dict(torch.load(args.checkpoints+f'{args.model}_{args.data_type}_{chT}_model_e.pth'))\n",
    "                encoder = encoder.to(args.device)\n",
    "                classifier = Classifier(args.seq_len//(2**args.layer_num)*args.class_num, args.class_num).to(args.device)\n",
    "                classifier_optim = optim.Adam(classifier.parameters(), lr=args.lr)\n",
    "\n",
    "                data_loaders = {'train': train_c_loader, 'val': valid_loader, 'test': test_loader}\n",
    "                trained_classifier, best_classifier_wts, train_loss_history, val_loss_history = AC_train(encoder, classifier, data_loaders, classifier_lossf, classifier_optim, num_epochs=args.train_epochs, channels=args.input_channel, channel_type=chT)\n",
    "                torch.save(trained_classifier.state_dict(), args.checkpoints+lossf+'_3L_'+args.model+'_'+args.data_type+'_'+chT+'_model_c.pth')\n",
    "                torch.save(best_classifier_wts, args.checkpoints+lossf+'_3L_'+args.model+'_'+args.data_type+'_'+chT+'_best_model_c.pth')\n",
    "                print('model saved to %s' % args.checkpoints+lossf+'_3L_'+args.model+'_'+args.data_type+'_'+chT+'_model_c')\n",
    "                \n",
    "                # Let's draw a learning curve like below.\n",
    "                plt.plot(train_loss_history, label='train')\n",
    "                plt.plot(val_loss_history, label='val')\n",
    "                plt.xlabel('epoch')\n",
    "                plt.ylabel('loss')\n",
    "                plt.legend()\n",
    "                plt.show() \n",
    "    else:\n",
    "        for lossf in ['MSE', 'CE']:\n",
    "            if lossf == 'MSE':\n",
    "                classifier_lossf = nn.MSELoss()\n",
    "            elif lossf == 'CE':\n",
    "                classifier_lossf = nn.CrossEntropyLoss()\n",
    "            encoder = ConvEncoder(args.input_channel, args.t, args.output_channel)\n",
    "            encoder.load_state_dict(torch.load(args.checkpoints+f'{args.model}_{args.data_type}_model_e.pth'))\n",
    "            encoder = encoder.to(args.device)\n",
    "            classifier = Classifier(args.seq_len//(2**args.layer_num)*args.class_num, args.class_num).to(args.device)\n",
    "            classifier_optim = optim.Adam(classifier.parameters(), lr=args.lr)\n",
    "\n",
    "            data_loaders = {'train': train_c_loader, 'val': valid_loader, 'test': test_loader}\n",
    "            trained_classifier, best_classifier_wts, train_loss_history, val_loss_history = AC_train(encoder, classifier, data_loaders, classifier_lossf, classifier_optim, num_epochs=args.train_epochs)\n",
    "            torch.save(trained_classifier.state_dict(), args.checkpoints+lossf+'_3L_'+args.model+'_'+args.data_type+'_model_c.pth')\n",
    "            torch.save(best_classifier_wts, args.checkpoints+lossf+'_3L_'+args.model+'_'+args.data_type+'_best_model_c.pth')\n",
    "            print('model saved to %s' % args.checkpoints+lossf+'_3L_'+args.model+'_'+args.data_type+'_model_c')\n",
    "            \n",
    "            # Let's draw a learning curve like below.\n",
    "            plt.plot(train_loss_history, label='train')\n",
    "            plt.plot(val_loss_history, label='val')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('loss')\n",
    "            plt.legend()\n",
    "            plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's draw a learning curve like below.\n",
    "plt.plot(train_loss_history, label='train')\n",
    "plt.plot(val_loss_history, label='val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossF = 'MSE'\n",
    "layerN = 3\n",
    "modelt = 'A'\n",
    "dtype = 'normal'\n",
    "paramT = 'best_model'\n",
    "\n",
    "# trained_classifier = Classifier(args.seq_len//(2**args.layer_num)*args.class_num, args.class_num).to(args.device)\n",
    "encoder.load_state_dict(torch.load(args.checkpoints+f'{modelt}_{dtype}_model_e.pth'))\n",
    "trained_classifier.load_state_dict(torch.load(args.checkpoints+f'{lossF}_{layerN}L_{modelt}_{dtype}_{paramT}_c.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed model MSE\n",
      "Accuracy: 79.0 %\n",
      "mixed model CE\n",
      "Accuracy: 25.0 %\n",
      "mixed best_model MSE\n",
      "Accuracy: 95.25 %\n",
      "mixed best_model CE\n",
      "Accuracy: 97.0 %\n",
      "normal model MSE\n",
      "Accuracy: 25.0 %\n",
      "normal model CE\n",
      "Accuracy: 96.0 %\n",
      "normal best_model MSE\n",
      "Accuracy: 92.5 %\n",
      "normal best_model CE\n",
      "Accuracy: 96.75 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "for dataT in ['mixed', 'normal']:\n",
    "    args.data_type = dataT\n",
    "    # if dataT == 'mixed':\n",
    "    #     continue\n",
    "    for modelT in ['model', 'best_model']:\n",
    "        for lossf in ['MSE', 'CE']:\n",
    "            encoder.load_state_dict(torch.load(args.checkpoints+f'{args.model}_{args.data_type}_model_e.pth'))\n",
    "            trained_classifier.load_state_dict(torch.load(args.checkpoints+f'{lossf}_3L_{args.model}_{args.data_type}_{modelT}_c.pth'))\n",
    "\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in data_loaders[\"test\"]:\n",
    "                    inputs = inputs.to(args.device)[:,:,:2].type(torch.float32).transpose(1, 2)\n",
    "                    labels = F.one_hot(labels, num_classes=4).to(args.device).type(torch.float32)\n",
    "\n",
    "                    hidden = encoder(inputs)\n",
    "                    outputs = trained_classifier(hidden.reshape(hidden.size(0), -1)).type(torch.float32)\n",
    "                    \n",
    "                    total += labels.size(0)\n",
    "                    correct += (outputs.argmax(1) == labels.argmax(1)).sum().item()\n",
    "            print(args.data_type, modelT, lossf)\n",
    "            print('Accuracy:', correct/total * 100, '%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000 for AE, 2000 for C\n",
    "### Mixed\n",
    "3 layer\n",
    "- MSE: 96\n",
    "- CE: 96.75\n",
    "\n",
    "### Normal\n",
    "3 layer\n",
    "- MSE: 97.25\n",
    "- CE: 98.75"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
