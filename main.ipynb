{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Models import ConvEncoder, ConvDecoder, TrAE, Classifier, TrVAE, VAE_loss\n",
    "from Dataset import data_provider\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'is_training': 1,\n",
    "    'model': 'A', # ['A', 'B']\n",
    "    ### data loader\n",
    "    'root_path': './Data/',\n",
    "    'data': '3D/new/',\n",
    "    'data_path': 'csv/',\n",
    "    'data_type': 'normal',\n",
    "    'checkpoints': './checkpoints/',\n",
    "    'Nsamples': 500, \n",
    "    'ratio': [7, 1, 2],\n",
    "    ### forecasting task\n",
    "    'seq_len': 2651, # [2144, 4576, 2656] # 3200 in paper\n",
    "    'label_len': 100,\n",
    "    'pred_len': 100,\n",
    "    'class_num': 5,\n",
    "    'individual': False,\n",
    "    ### transformer\n",
    "    'layer_num': 1,\n",
    "    'dropout': 0.1,\n",
    "    'max_len': 5000,\n",
    "    'd_model': 512,\n",
    "    'd_h': 8,\n",
    "    'd_ff': 2048,\n",
    "    ### Layers\n",
    "    'layer_num': 5,\n",
    "    'input_channel': 23,\n",
    "    't': 128, \n",
    "    'output_channel': 23,\n",
    "    'do_predict': True,\n",
    "    ### optimization\n",
    "    'num_workers': 8,\n",
    "    'itr': 1,\n",
    "    'train_epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'patience': 3,\n",
    "    'lr': 0.005,\n",
    "    'scheduling': False,\n",
    "    ### GPU\n",
    "    'device': 'cuda:0',\n",
    "    'use_gpu': True,\n",
    "    'multi_gpu': True,\n",
    "})\n",
    "\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "if args.data == '2D/':\n",
    "    args.data_path = ''\n",
    "    args.seq_len = 3200  # 3200 in paper\n",
    "    args.class_num = 4\n",
    "    args.input_channel = 2\n",
    "    args.t = 16\n",
    "    args.output_channel = 4\n",
    "    args.ratio = [3,1,1]\n",
    "elif args.data == '3D/old/':\n",
    "    args.data_path = ''\n",
    "    args.seq_len = 4576\n",
    "    args.input_channel = 3\n",
    "    args.t = 64\n",
    "    args.output_channel = 8\n",
    "elif args.data == '3D/new/':\n",
    "    args.input_channel = 3 # [3, 3, 3, 3, 3, 4, 4]\n",
    "    args.t = 64\n",
    "    args.output_channel = 4\n",
    "    args.batch_size = 128\n",
    "    \n",
    "if args.model == 'B':\n",
    "    args.seq_len = 400\n",
    "    \n",
    "temp_data_type = args.data_type\n",
    "args.data_type = 'normal'\n",
    "normal_train_set, normal_train_loader = data_provider(args, 'train')\n",
    "normal_train_C_set, normal_train_C_loader = data_provider(args, 'train_c')\n",
    "args.data_type = 'mixed'\n",
    "mixed_train_set, mixed_train_loader = data_provider(args, 'train')\n",
    "mixed_train_C_set, mixed_train_C_loader = data_provider(args, 'train_c')\n",
    "\n",
    "valid_set, valid_loader = data_provider(args, 'valid')\n",
    "test_set, test_loader = data_provider(args, 'test')\n",
    "\n",
    "normal_loaders = {'train': normal_train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "normal_C_loaders = {'train': normal_train_C_loader, 'val': valid_loader, 'test': test_loader}\n",
    "mixed_loaders = {'train': mixed_train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "mixed_C_loaders = {'train': mixed_train_C_loader, 'val': valid_loader, 'test': test_loader}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Dimensional Trajectory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 .pickle 로드 및 전처리 후 .npy 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "'''\n",
    "Trajectory types: Normal, autopilot_lag, LOSrate_bias, LOSrate_delay\n",
    "Target distance = uniform(4500, 5500)\n",
    "states = [x, y, vm, path_angle] + 1 control input, 5 states\n",
    "label = 0, 1, 2, 3 for normal, autopilot_lag, LOSrate_bias, LOSrate_delay respectively, 4 classes\n",
    "minimum length of trajectory = 1360 (1360*0.01 = 13.6 seconds)\n",
    "maximum length of trajectory = 2131 (2131*0.01 = 21.31 seconds)\n",
    "fixed sequence length = 2144 (2400*0.01 = 24 seconds)\n",
    "'''\n",
    "start = time.time()\n",
    "with open('Data/2D/Trajectories.pickle', 'rb') as f:\n",
    "    Dataset = pickle.load(f)\n",
    "\n",
    "# normal = Dataset['normal_PNG']\n",
    "# lag = Dataset['autopilot_lag']\n",
    "# bias = Dataset['LOSrate_bias']\n",
    "# delay = Dataset['LOSrate_delay']\n",
    "seq_len = 2144\n",
    "\n",
    "label = 0\n",
    "x = []\n",
    "y = []\n",
    "for tr_type in Dataset:\n",
    "    states = Dataset[tr_type]['states']\n",
    "    inputs = Dataset[tr_type]['actions']\n",
    "    for i in range(len(states)):\n",
    "        tr = np.concatenate((states[i], np.insert(inputs[i], 0, 0).reshape(-1 ,1)), axis=1)\n",
    "        terminal = tr[-1].copy()\n",
    "        terminal[2]=terminal[4]=0\n",
    "        tr = np.concatenate((tr, np.tile(terminal, (seq_len-len(tr), 1))), axis=0)\n",
    "        x.append(tr)\n",
    "        y.append(label)\n",
    "    label += 1\n",
    "x = np.array(x)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "np.save('Data/2D/x.npy', x)\n",
    "np.save('Data/2D/y.npy', y)\n",
    "# x = np.load('Data/2D/x.npy')\n",
    "# y = np.load('Data/2D/y.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".npy 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('Data/2D/x.npy')\n",
    "y = np.load('Data/2D/y.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Dimentional Trajectory old version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 .pickle 로드 및 전처리 후 .npy 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trajectory types: Normal, Burn time, Xcp position, Thrust Tilt Angle, Fin bias\n",
    "Target Distance = 4000\n",
    "States = ['Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Accm_Cmd_1', 'Accm_Cmd_2', 'Accm_Cmd_3', 'PhiCmd'], 16 states\n",
    "label = [0, 1, 2, 3, 4] for normal, burn_time, xcp_pos, thrust_tilt, fin_bias respectively, 5 classes\n",
    "minimum length of trajectory = 412 (412*0.01 = 4.12 seconds)\n",
    "maximum length of trajectory = 4569 (4569*0.01 = 45.69 seconds)\n",
    "fixed sequence length = 4576 (4576*0.01 = 45.76 seconds)\n",
    "'''\n",
    "start = time.time()\n",
    "seq_len = 4576\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i, tr_type in enumerate(['Normal', 'Burntime', 'Xcpposition', \"ThrustTiltAngle\", 'Finbias']):\n",
    "    gid = pd.read_excel(f'Data/3D/old/Gid_{i+1}.xlsx', sheet_name=None)\n",
    "    msl = pd.read_excel(f'Data/3D/old/Msl_{i+1}.xlsx', sheet_name=None)\n",
    "    \n",
    "    for sheet in gid:\n",
    "        tr = pd.merge(msl[sheet][['Time', 'Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3']], gid[sheet][['Time', 'Accm_Cmd_1', 'Accm_Cmd_2', 'Accm_Cmd_3', 'Phi_Cmd']]).to_numpy()[:, 1:]\n",
    "        terminal = tr[-1].copy()\n",
    "        terminal[3]=terminal[4]=terminal[5]=terminal[6]=terminal[7]=terminal[8]=terminal[12]=terminal[13]=terminal[14]=terminal[15]=0\n",
    "        tr = np.concatenate((tr, np.tile(terminal, (seq_len-len(tr), 1))), axis=0)\n",
    "        if sheet == 'Sheet1' and tr_type == 'Normal':\n",
    "            x = [tr for _ in range(140)]\n",
    "            y = [i for _ in range(140)]\n",
    "        else:\n",
    "            x.append(tr)\n",
    "            y.append(i)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "np.save('Data/3D/x_old.npy', x)\n",
    "np.save('Data/3D/y_old.npy', y)\n",
    "# x = np.load('Data/3D/x_old.npy')\n",
    "# y = np.load('Data/3D/y_old.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".npy 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "x = np.load('Data/3D/x_old.npy')\n",
    "y = np.load('Data/3D/y_old.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Dimensional Trajectory new version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 .pickle 로드 및 전처리 후 .npy 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trajectory types: Normal, Burn time, Xcp position, Thrust Tilt Angle, Fin bias\n",
    "Target Distance = 4000\n",
    "Total states = ['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'err_FinBias_1', 'err_FinBias_2', 'err_FinBias_3', 'err_FinBias_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4', 'err_BurnTime', 'err_Tilt_1', 'err_Tilt_2', 'err_delXcp'], 31 states\n",
    "Used States = ['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4'], 23 states\n",
    "label = [0, 1, 2, 3, 4] for normal, burn_time, xcp_pos, thrust_tilt, fin_bias respectively, 5 classes\n",
    "minimum length of trajectory = 2449 (2448*0.01 = 24.48 seconds)\n",
    "maximum length of trajectory = 2635 (2635*0.01 = 26.35 seconds)\n",
    "fixed sequence length = 2656 (2656*0.01 = 26.56 seconds)\n",
    "'''\n",
    "start = time.time()\n",
    "seq_len = 2656\n",
    "x = []\n",
    "y = []\n",
    "tr = {}\n",
    "for i, tr_type in enumerate(['Normal', 'Burntime', 'Xcpposition', \"ThrustTiltAngle\", 'Finbias']):\n",
    "    N = 1 if i == 0 else 500\n",
    "    for tr_i in range(N):\n",
    "        tr = pd.read_csv(f'Data/3D/new/csv/Type_{i+1}_{tr_i+1}.csv', header=None)\n",
    "        tr.columns = ['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'err_FinBias_1', 'err_FinBias_2', 'err_FinBias_3', 'err_FinBias_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4', 'err_BurnTime', 'err_Tilt_1', 'err_Tilt_2', 'err_delXcp']\n",
    "        tr = tr[['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4']].to_numpy()\n",
    "        tr[0] = 4000 - tr[0]\n",
    "        terminal = tr[-1].copy()\n",
    "        terminal[3]=terminal[4]=terminal[5]=terminal[6]=terminal[7]=terminal[8]=terminal[12]=terminal[13]=terminal[14]=terminal[19]=terminal[20]=terminal[21]=terminal[22]=0\n",
    "        tr = np.concatenate((tr, np.tile(terminal, (seq_len-len(tr), 1))), axis=0)\n",
    "        if i == 0:\n",
    "            x = [tr for _ in range(500)]\n",
    "            y = [i for _ in range(500)]\n",
    "        else:\n",
    "            x.append(tr)\n",
    "            y.append(i)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "np.save('Data/3D/x_new.npy', x)\n",
    "np.save('Data/3D/y_new.npy', y)\n",
    "# x = np.load('Data/3D/x_new.npy')\n",
    "# y = np.load('Data/3D/y_new.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".npy 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "x = np.load('Data/3D/x_new.npy')\n",
    "y = np.load('Data/3D/y_new.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_type = args.data_type\n",
    "args.data_type = 'normal'\n",
    "normal_train_set, normal_train_loader = data_provider(args, 'train')\n",
    "normal_train_C_set, normal_train_C_loader = data_provider(args, 'train_c')\n",
    "args.data_type = 'mixed'\n",
    "mixed_train_set, mixed_train_loader = data_provider(args, 'train')\n",
    "mixed_train_C_set, mixed_train_C_loader = data_provider(args, 'train_c')\n",
    "\n",
    "valid_set, valid_loader = data_provider(args, 'valid')\n",
    "test_set, test_loader = data_provider(args, 'test')\n",
    "\n",
    "normal_loaders = {'train': normal_train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "normal_C_loaders = {'train': normal_train_C_loader, 'val': valid_loader, 'test': test_loader}\n",
    "mixed_loaders = {'train': mixed_train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "mixed_C_loaders = {'train': mixed_train_C_loader, 'val': valid_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrAE(ConvEncoder, ConvDecoder, args).to(args.device)\n",
    "model = nn.DataParallel(model)\n",
    "criterion = nn.MSELoss().to(args.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3, 352]), torch.Size([64, 3, 352]), torch.Size([64, 4, 11]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(iter(normal_train_loader))[0].to(args.device)[:, 80:(80+352), :3].type(torch.float32).transpose(1, 2)\n",
    "# x.shape\n",
    "y, h = model(x)\n",
    "x.shape, y.shape, h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AutoEncoder(model, dataloaders, criterion, optimizer, scheduler = None, num_epochs=10, phase = None, channel_type = None):\n",
    "    \"\"\"\n",
    "    model: model to train\n",
    "    dataloaders: train, val, test data's loader\n",
    "    criterion: loss function\n",
    "    optimizer: optimizer to update your model\n",
    "    \"\"\"\n",
    "    if phase == 'before':\n",
    "        ts = 0\n",
    "    elif phase == 'initial':\n",
    "        ts = 80\n",
    "    elif phase == 'final':\n",
    "        ts = 411\n",
    "    else:\n",
    "        ts = 0\n",
    "    te = ts + args.seq_len\n",
    "    \n",
    "    if channel_type == 'Rmi':\n",
    "        cs = 0\n",
    "    elif channel_type == 'Vmi':\n",
    "        cs = 3\n",
    "    elif channel_type == 'Wmb':\n",
    "        cs = 6\n",
    "    elif channel_type == 'Accm':\n",
    "        cs = 9\n",
    "    elif channel_type == 'angEuler':\n",
    "        cs = 12\n",
    "    elif channel_type == 'FinOut':\n",
    "        cs = 15\n",
    "    elif channel_type == 'FinCmd':\n",
    "        cs = 19\n",
    "    else:\n",
    "        cs = 0\n",
    "    ce = cs + args.input_channel\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.module.state_dict()) if args.multi_gpu else copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = 100000000\n",
    "    \n",
    "    epoch_iterater = tqdm(range(num_epochs), desc='AE Training', total=num_epochs)\n",
    "    # since = time.time()\n",
    "    for epoch in epoch_iterater:\n",
    "        epoch_iterater.set_description('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "\n",
    "        for mode in ['train', 'val']:\n",
    "            if mode == 'train':\n",
    "                model.train()            # Set model to training mode\n",
    "            else:\n",
    "                model.eval()            # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for inputs, _ in dataloaders[mode]:\n",
    "                end_steps = inputs[:, -1, -1]\n",
    "                inputs = inputs.to(args.device)[:, ts:te, cs:ce].type(torch.float32).transpose(1, 2)  # transfer inputs to GPU \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(mode == 'train'):\n",
    "                    loss = 0\n",
    "                    outputs, _ = model(inputs)\n",
    "                    for i in range(len(outputs)):\n",
    "                        end_step = int(end_steps[i])\n",
    "                        loss += criterion(outputs[i, :, :end_step], inputs[i, :, :end_step])\n",
    "                    # loss = criterion(outputs, inputs)           # calculate a loss\n",
    "                    if mode == 'train':\n",
    "                        loss.backward()                             # perform back-propagation from the loss\n",
    "                        optimizer.step()                             # perform gradient descent with given optimizer\n",
    "                        if scheduler is not None:\n",
    "                            scheduler.step()                             # update learning rate\n",
    "                        \n",
    "                running_loss += loss.item() # * inputs.size(0)                    \n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[mode].dataset)\n",
    "            # deep copy the model\n",
    "            if mode == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            if mode == 'val':\n",
    "                val_loss_history.append(epoch_loss)\n",
    "            if mode == 'val' and epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.module.state_dict()) if args.multi_gpu else copy.deepcopy(model.state_dict())\n",
    "        epoch_iterater.set_postfix({'data':dataT, 'phase':f'{phases.index(phase)+1}/{len(phases)}', 'states': f'{chTs.index(channel_type)+1}/{len(chTs)}', 'train loss': train_loss_history[-1], 'val loss': val_loss_history[-1], 'best val loss': best_val_loss})\n",
    "\n",
    "    # time_elapsed = time.time() - since\n",
    "    # print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    # print('Best val Loss: {:4f}'.format(best_val_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "    return model, best_model_wts, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:20<00:00,  1.46s/it, data=mixed, phase=1/3, states=1/7, train loss=0.000807, val loss=0.00115, best val loss=9.83e-5]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:19<00:00,  1.46s/it, data=mixed, phase=1/3, states=2/7, train loss=0.0115, val loss=0.0111, best val loss=0.0034] \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:20<00:00,  1.46s/it, data=mixed, phase=1/3, states=3/7, train loss=0.00107, val loss=0.00213, best val loss=4.5e-5]  \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:24<00:00,  1.46s/it, data=mixed, phase=1/3, states=4/7, train loss=0.00127, val loss=0.00109, best val loss=0.000231]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:20<00:00,  1.46s/it, data=mixed, phase=1/3, states=5/7, train loss=0.0285, val loss=0.0174, best val loss=0.00201]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:23<00:00,  1.46s/it, data=mixed, phase=1/3, states=6/7, train loss=0.00844, val loss=0.0101, best val loss=0.00655]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:23<00:00,  1.46s/it, data=mixed, phase=1/3, states=7/7, train loss=0.0226, val loss=0.0332, best val loss=0.00614]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:30<00:00,  1.47s/it, data=mixed, phase=2/3, states=1/7, train loss=6.4, val loss=17.2, best val loss=0.442]   \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:30<00:00,  1.47s/it, data=mixed, phase=2/3, states=2/7, train loss=2.87, val loss=0.706, best val loss=0.141]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:33<00:00,  1.47s/it, data=mixed, phase=2/3, states=3/7, train loss=2.31, val loss=1.67, best val loss=0.179]  \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:29<00:00,  1.47s/it, data=mixed, phase=2/3, states=4/7, train loss=0.00456, val loss=0.0017, best val loss=0.000297] \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:33<00:00,  1.47s/it, data=mixed, phase=2/3, states=5/7, train loss=0.163, val loss=0.421, best val loss=0.0133]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:32<00:00,  1.47s/it, data=mixed, phase=2/3, states=6/7, train loss=0.0755, val loss=0.116, best val loss=0.0516]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:33<00:00,  1.47s/it, data=mixed, phase=2/3, states=7/7, train loss=0.0852, val loss=0.24, best val loss=0.0823] \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:49<00:00,  1.55s/it, data=mixed, phase=3/3, states=1/7, train loss=501, val loss=192, best val loss=167]        \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:51<00:00,  1.55s/it, data=mixed, phase=3/3, states=2/7, train loss=3.52, val loss=1.6, best val loss=0.0988]  \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:46<00:00,  1.55s/it, data=mixed, phase=3/3, states=3/7, train loss=445, val loss=461, best val loss=163]        \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:51<00:00,  1.55s/it, data=mixed, phase=3/3, states=4/7, train loss=0.000241, val loss=8.08e-5, best val loss=6.07e-5]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:51<00:00,  1.55s/it, data=mixed, phase=3/3, states=5/7, train loss=0.0901, val loss=0.158, best val loss=0.029]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:53<00:00,  1.55s/it, data=mixed, phase=3/3, states=6/7, train loss=0.0395, val loss=0.0408, best val loss=0.00923]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:47<00:00,  1.55s/it, data=mixed, phase=3/3, states=7/7, train loss=0.0614, val loss=0.119, best val loss=0.00998]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:21<00:00,  1.46s/it, data=normal, phase=1/3, states=1/7, train loss=0.000127, val loss=0.0275, best val loss=0.00173]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:22<00:00,  1.46s/it, data=normal, phase=1/3, states=2/7, train loss=0.0359, val loss=1.09, best val loss=0.0864] \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:24<00:00,  1.46s/it, data=normal, phase=1/3, states=3/7, train loss=3.89e-5, val loss=0.0426, best val loss=0.00502]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:24<00:00,  1.46s/it, data=normal, phase=1/3, states=4/7, train loss=5.27e-5, val loss=0.00852, best val loss=0.000426]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:24<00:00,  1.46s/it, data=normal, phase=1/3, states=5/7, train loss=16.8, val loss=102, best val loss=94.4]    \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:24<00:00,  1.46s/it, data=normal, phase=1/3, states=6/7, train loss=4.14e-11, val loss=2.07, best val loss=1.65]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:25<00:00,  1.47s/it, data=normal, phase=1/3, states=7/7, train loss=1.18e-9, val loss=3.45, best val loss=3.45]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:34<00:00,  1.47s/it, data=normal, phase=2/3, states=1/7, train loss=0.123, val loss=49.4, best val loss=44.7]   \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:28<00:00,  1.47s/it, data=normal, phase=2/3, states=2/7, train loss=0.11, val loss=73, best val loss=52.8]     \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:32<00:00,  1.47s/it, data=normal, phase=2/3, states=3/7, train loss=0.0378, val loss=67, best val loss=48.2]   \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:35<00:00,  1.48s/it, data=normal, phase=2/3, states=4/7, train loss=2.71e-5, val loss=0.047, best val loss=0.0396]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:36<00:00,  1.48s/it, data=normal, phase=2/3, states=5/7, train loss=0.0231, val loss=407, best val loss=43.1]    \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:31<00:00,  1.47s/it, data=normal, phase=2/3, states=6/7, train loss=5.48e-5, val loss=3.16, best val loss=3.03]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [24:34<00:00,  1.47s/it, data=normal, phase=2/3, states=7/7, train loss=0.000462, val loss=4.89, best val loss=4.71]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:49<00:00,  1.55s/it, data=normal, phase=3/3, states=1/7, train loss=1.27e+3, val loss=1.01e+3, best val loss=622]  \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:49<00:00,  1.55s/it, data=normal, phase=3/3, states=2/7, train loss=1.68, val loss=8.3, best val loss=5.17]   \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:51<00:00,  1.55s/it, data=normal, phase=3/3, states=3/7, train loss=890, val loss=711, best val loss=586]          \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:49<00:00,  1.55s/it, data=normal, phase=3/3, states=4/7, train loss=8.62e-6, val loss=0.00764, best val loss=0.00763]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:50<00:00,  1.55s/it, data=normal, phase=3/3, states=5/7, train loss=0.00266, val loss=3.4, best val loss=0.598]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:51<00:00,  1.55s/it, data=normal, phase=3/3, states=6/7, train loss=0.000905, val loss=0.517, best val loss=0.511]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [25:51<00:00,  1.55s/it, data=normal, phase=3/3, states=7/7, train loss=0.00041, val loss=0.528, best val loss=0.515]\n"
     ]
    }
   ],
   "source": [
    "phases = ['before', 'initial', 'final'] if args.data == '3D/new/' else ['whole']\n",
    "phase2len = {'before':96, 'initial':352, 'final':2240} if args.data == '3D/new/' else {'whole':3200}\n",
    "chTs = ['Rmi', 'Vmi', 'Wmi', 'Accm', 'angEuler', 'FinOut', 'FinCmd'] if args.data == '3D/new/' else ['Rmi']\n",
    "ch = {'Rmi':3, 'Vmi':3, 'Wmi':3, 'Accm':3, 'angEuler':3, 'FinOut':4, 'FinCmd':4} if args.data == '3D/new/' else {'Rmi':2}\n",
    "# plot_cnt = 0\n",
    "temp_seq_len = args.seq_len\n",
    "\n",
    "loss_history = {'train':[], 'val':[]}\n",
    "\n",
    "for dataT in ['mixed', 'normal']:\n",
    "    # if dataT == 'normal':\n",
    "    #     continue\n",
    "    args.data_type = dataT\n",
    "    # train_set, train_loader = data_provider(args, 'train')\n",
    "    # valid_set, valid_loader = data_provider(args, 'valid')\n",
    "    # test_set, test_loader = data_provider(args, 'test')\n",
    "    # data_loaders = {'train': train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "    if dataT == 'mixed':\n",
    "        data_loaders = mixed_loaders\n",
    "    else:\n",
    "        data_loaders = normal_loaders\n",
    "    \n",
    "    for phase in phases:\n",
    "        for chT in chTs:\n",
    "            if args.data == '3D/new/':\n",
    "                args.seq_len = phase2len[phase]\n",
    "                args.input_channel = ch[chT]\n",
    "            save_ckpt = f'{args.checkpoints+args.data}{args.train_epochs}/{args.model}_{args.data_type}_{chT}_{phase}_'\n",
    "            # print(f'{dataT} set {chT} featrue {phase} phase Auto Encoder model training start')\n",
    "            model = TrAE(ConvEncoder, ConvDecoder, args).to(args.device)\n",
    "            if args.multi_gpu:\n",
    "                model = nn.DataParallel(model)\n",
    "            criterion = nn.MSELoss().to(args.device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5) if args.scheduling else None\n",
    "            \n",
    "            trained_model, best_wts, train_loss_history, val_loss_history = train_AutoEncoder(model, data_loaders, criterion, optimizer, scheduler, args.train_epochs, phase, chT) # \n",
    "            if args.multi_gpu:\n",
    "                torch.save(trained_model.module.state_dict(), save_ckpt+'model.pth')\n",
    "                torch.save(trained_model.module.encoder.state_dict(), save_ckpt+'encoder.pth')\n",
    "            else:\n",
    "                torch.save(trained_model.state_dict(), save_ckpt+'model.pth')\n",
    "                torch.save(trained_model.encoder.state_dict(), save_ckpt+'encoder.pth')\n",
    "            if args.multi_gpu:\n",
    "                trained_model.module.load_state_dict(best_wts)\n",
    "            else:\n",
    "                trained_model.load_state_dict(best_wts)\n",
    "            if args.multi_gpu:\n",
    "                torch.save(trained_model.module.state_dict(), save_ckpt+'model.pth')\n",
    "                torch.save(trained_model.module.encoder.state_dict(), save_ckpt+'encoder.pth')\n",
    "            else:\n",
    "                torch.save(trained_model.state_dict(), save_ckpt+'model.pth')\n",
    "                torch.save(trained_model.encoder.state_dict(), save_ckpt+'encoder.pth')\n",
    "            # print('model saved to %s' % save_ckpt)\n",
    "            \n",
    "            loss_history['train'].append(train_loss_history)\n",
    "            loss_history['val'].append(val_loss_history)\n",
    "            # plot_cnt += 1\n",
    "            # plt.subplot(4, len(phases)+len(chTs), plot_cnt)\n",
    "            # plt.figure(figsize=(4, 3))\n",
    "            # plt.plot(train_loss_history, label=f'{phase} {chT} train')\n",
    "            # plt.plot(val_loss_history, label=f'{phase} {chT} val')\n",
    "        #     break\n",
    "        # break\n",
    "    args.seq_len = temp_seq_len\n",
    "    # break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's draw a learning curve like below.\n",
    "plt.plot(train_loss_history, label='train')\n",
    "plt.plot(val_loss_history, label='val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4150302.8733333335\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in data_loaders[\"test\"]:\n",
    "        inputs = inputs.to(args.device)[:, 0:96, :4].type(torch.float32).transpose(1, 2)\n",
    "\n",
    "        outputs, encoded = trained_model(inputs)\n",
    "        test_loss = criterion(outputs, inputs)\n",
    "        \n",
    "        running_loss += test_loss.item() * inputs.size(0)\n",
    "\n",
    "    test_loss = running_loss / len(data_loaders[\"test\"].dataset)\n",
    "    print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = ['before', 'initial', 'final'] if args.data == '3D/new/' else ['whole']\n",
    "phase2len = {'before':96, 'initial':352, 'final':2240} if args.data == '3D/new/' else {'whole':3200}\n",
    "seq_start = {'before':0, 'initial':80, 'final':411} if args.data == '3D/new/' else {'whole':0}\n",
    "chTs = ['Rmi', 'Vmi', 'Wmi', 'Accm', 'angEuler', 'FinOut', 'FinCmd'] if args.data == '3D/new/' else ['Rmi']\n",
    "ch = {'Rmi':3, 'Vmi':3, 'Wmi':3, 'Accm':3, 'angEuler':3, 'FinOut':4, 'FinCmd':4} if args.data == '3D/new/' else {'Rmi':2}\n",
    "channel_start = {'Rmi':0, 'Vmi':3, 'Wmi':6, 'Accm':9, 'angEuler':12, 'FinOut':15, 'FinCmd':19} if args.data == '3D/new/' else {'Rmi':0}\n",
    "plot_cnt = 0\n",
    "temp_seq_len = args.seq_len\n",
    "\n",
    "for dataT in ['mixed', 'normal']:\n",
    "    # if dataT == 'normal':\n",
    "    #     continue\n",
    "    args.data_type = dataT\n",
    "    # train_set, train_loader = data_provider(args, 'train')\n",
    "    # valid_set, valid_loader = data_provider(args, 'valid')\n",
    "    # test_set, test_loader = data_provider(args, 'test')\n",
    "    # data_loaders = {'train': train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "    \n",
    "    if dataT == 'mixed':\n",
    "        data_loaders = mixed_loaders\n",
    "    else:\n",
    "        data_loaders = normal_loaders\n",
    "    \n",
    "    for phase in phases:\n",
    "        plot_cnt += 1\n",
    "        plt.subplot(4, len(phases), plot_cnt)\n",
    "        for chT in chTs:\n",
    "            args.seq_len = phase2len[phase]\n",
    "            args.input_channel = ch[chT]\n",
    "            for modelT in ['model', 'best_model']:\n",
    "                save_ckpt = f'{args.checkpoints+args.data+args.model}_{args.data_type}_{chT}_{phase}_{modelT}'\n",
    "                model = TrAE(ConvEncoder, ConvDecoder, args).to(args.device)\n",
    "                model.load_state_dict(torch.load(save_ckpt+'.pth'))\n",
    "                \n",
    "                ts = seq_start[phase]\n",
    "                te = ts + args.seq_len\n",
    "                cs = channel_start[chT]\n",
    "                ce = cs + args.input_channel\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    running_loss = 0.0\n",
    "                    for inputs, labels in data_loaders[\"test\"]:\n",
    "                        inputs = inputs.to(args.device)[:,ts:te,cs:ce].type(torch.float32).transpose(1, 2)\n",
    "\n",
    "                        outputs, encoded = model(inputs)\n",
    "                        test_loss = criterion(outputs, inputs)\n",
    "                        \n",
    "                        running_loss += test_loss.item() * inputs.size(0)\n",
    "\n",
    "                    test_loss = running_loss / len(data_loaders[\"test\"].dataset)\n",
    "                    print(test_loss)\n",
    "\n",
    "                tr = inputs.cpu().numpy()\n",
    "                tr_hat = outputs.cpu().numpy()\n",
    "\n",
    "                f, axes = plt.subplots(2, 5)\n",
    "                f.set_size_inches((15, 5))\n",
    "                plt.subplots_adjust(wspace = 0.3, hspace = 0.3)\n",
    "\n",
    "                for i, n in enumerate(range(0, len(tr)-1, len(tr)//5)):\n",
    "                    axes[0, i].plot(tr[n, 0, :], tr[n, 1, :])\n",
    "                    # axes[0, i].plot(tr[n, 0, :])\n",
    "                    # axes[0, i].plot(tr[n, 1, :])\n",
    "                    axes[1, i].plot(tr_hat[n, 0, :], tr_hat[n, 1, :])\n",
    "                    # axes[1, i].plot(tr_hat[n, 0, :])\n",
    "                    # axes[1, i].plot(tr_hat[n, 1, :])\n",
    "\n",
    "                plt.show()\n",
    "        #         break\n",
    "        #     break\n",
    "        # break\n",
    "    args.seq_len = temp_seq_len\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(encoder, model, dataloaders, criterion, optimizer, scheduler = None, num_epochs=10, phase = None, channel_type = None):\n",
    "    \"\"\"\n",
    "    model: model to train\n",
    "    dataloaders: train, val, test data's loader\n",
    "    criterion: loss function\n",
    "    optimizer: optimizer to update your model\n",
    "    \"\"\"\n",
    "    if phase == 'before':\n",
    "        ts = 0\n",
    "    elif phase == 'initial':\n",
    "        ts = 80\n",
    "    elif phase == 'final':\n",
    "        ts = 411\n",
    "    else:\n",
    "        ts = 0\n",
    "    te = ts + args.seq_len\n",
    "    \n",
    "    if channel_type == 'Rmi':\n",
    "        cs = 0\n",
    "    elif channel_type == 'Vmi':\n",
    "        cs = 3\n",
    "    elif channel_type == 'Wmb':\n",
    "        cs = 6\n",
    "    elif channel_type == 'Accm':\n",
    "        cs = 9\n",
    "    elif channel_type == 'angEuler':\n",
    "        cs = 12\n",
    "    elif channel_type == 'FinOut':\n",
    "        cs = 15\n",
    "    elif channel_type == 'FinCmd':\n",
    "        cs = 19\n",
    "    else:\n",
    "        cs = 0\n",
    "    ce = cs + args.input_channel\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = 100000000\n",
    "\n",
    "    encoder.eval()\n",
    "    \n",
    "    # since = time.time()\n",
    "    epoch_iterater = tqdm(range(num_epochs), desc='C Training', total=num_epochs)\n",
    "    for epoch in epoch_iterater:\n",
    "        epoch_iterater.set_description('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "\n",
    "        for mode in ['train', 'val']:\n",
    "            if mode == 'train':\n",
    "                model.train()            # Set model to training mode\n",
    "            else:\n",
    "                model.eval()            # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in dataloaders[mode]:\n",
    "                inputs = inputs.to(args.device)[:, ts:te, cs:ce].type(torch.float32).transpose(1, 2)  # transfer inputs to GPU \n",
    "                encoded = encoder(inputs)\n",
    "                labels = labels.to(args.device).type(torch.float32)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(mode == 'train'):\n",
    "                    outputs = model(encoded.reshape(encoded.size(0), -1))  # forward pass\n",
    "                    loss = criterion(outputs, labels)           # calculate a loss\n",
    "                    loss = criterion(outputs.view(-1), labels.view(-1))\n",
    "                    if mode == 'train':\n",
    "                        loss.backward()                             # perform back-propagation from the loss\n",
    "                        optimizer.step()                             # perform gradient descent with given optimizer\n",
    "                        if scheduler is not None:\n",
    "                            scheduler.step()                             # update learning rate\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)                    \n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[mode].dataset)\n",
    "            # print('{} Loss: {:.4f}'.format(mode, epoch_loss))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if mode == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            if mode == 'val':\n",
    "                val_loss_history.append(epoch_loss)\n",
    "            if mode == 'val' and epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.module.state_dict()) if args.multi_gpu else copy.deepcopy(model.state_dict())\n",
    "        epoch_iterater.set_postfix({'data':dataT, 'phase':f'{phases.index(phase)+1}/{len(phases)}', 'states': f'{chTs.index(channel_type)+1}/{len(chTs)}', 'loss':lossf[:-1],  'train loss': train_loss_history[-1], 'val loss': val_loss_history[-1], 'best val loss': best_val_loss})\n",
    "        # print()\n",
    "\n",
    "    # time_elapsed = time.time() - since\n",
    "    # print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    # print('Best val Loss: {:4f}'.format(best_val_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "    return model, best_model_wts, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:21<00:00,  1.02it/s, data=mixed, phase=1/3, states=1/7, loss=MSE, train loss=0.0197, val loss=0.0298, best val loss=0.00572]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:26<00:00,  1.01it/s, data=mixed, phase=1/3, states=1/7, loss=CE, train loss=640, val loss=617, best val loss=605]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:23<00:00,  1.02it/s, data=mixed, phase=1/3, states=2/7, loss=MSE, train loss=0.0377, val loss=0.0133, best val loss=0.011]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:33<00:00,  1.01it/s, data=mixed, phase=1/3, states=2/7, loss=CE, train loss=641, val loss=607, best val loss=607]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:30<00:00,  1.01it/s, data=mixed, phase=1/3, states=3/7, loss=MSE, train loss=0.0451, val loss=0.0237, best val loss=0.0178]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:32<00:00,  1.01it/s, data=mixed, phase=1/3, states=3/7, loss=CE, train loss=655, val loss=620, best val loss=610]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:37<00:00,  1.00it/s, data=mixed, phase=1/3, states=4/7, loss=MSE, train loss=0.114, val loss=0.119, best val loss=0.119]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:36<00:00,  1.00it/s, data=mixed, phase=1/3, states=4/7, loss=CE, train loss=744, val loss=743, best val loss=718]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:35<00:00,  1.00it/s, data=mixed, phase=1/3, states=5/7, loss=MSE, train loss=0.00595, val loss=0.00409, best val loss=0.0019]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:34<00:00,  1.01it/s, data=mixed, phase=1/3, states=5/7, loss=CE, train loss=616, val loss=594, best val loss=591]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:33<00:00,  1.01it/s, data=mixed, phase=1/3, states=6/7, loss=MSE, train loss=0.0616, val loss=0.0878, best val loss=0.0837]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:37<00:00,  1.00it/s, data=mixed, phase=1/3, states=6/7, loss=CE, train loss=679, val loss=683, best val loss=673]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:38<00:00,  1.00it/s, data=mixed, phase=1/3, states=7/7, loss=MSE, train loss=0.0539, val loss=0.0814, best val loss=0.0745]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:39<00:00,  1.00it/s, data=mixed, phase=1/3, states=7/7, loss=CE, train loss=658, val loss=665, best val loss=658]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:40<00:00,  1.00s/it, data=mixed, phase=2/3, states=1/7, loss=MSE, train loss=0.00243, val loss=0.00337, best val loss=0.00174]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:40<00:00,  1.00s/it, data=mixed, phase=2/3, states=1/7, loss=CE, train loss=613, val loss=592, best val loss=589]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:39<00:00,  1.00it/s, data=mixed, phase=2/3, states=2/7, loss=MSE, train loss=0.0019, val loss=0.00163, best val loss=0.00054] \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:39<00:00,  1.00it/s, data=mixed, phase=2/3, states=2/7, loss=CE, train loss=616, val loss=592, best val loss=588]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:40<00:00,  1.00s/it, data=mixed, phase=2/3, states=3/7, loss=MSE, train loss=0.00226, val loss=0.00151, best val loss=0.00151]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:44<00:00,  1.00s/it, data=mixed, phase=2/3, states=3/7, loss=CE, train loss=611, val loss=591, best val loss=590]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:41<00:00,  1.00s/it, data=mixed, phase=2/3, states=4/7, loss=MSE, train loss=0.000162, val loss=0.000392, best val loss=0.000351]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:42<00:00,  1.00s/it, data=mixed, phase=2/3, states=4/7, loss=CE, train loss=610, val loss=594, best val loss=588]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:42<00:00,  1.00s/it, data=mixed, phase=2/3, states=5/7, loss=MSE, train loss=0.00103, val loss=0.00129, best val loss=0.000837]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:44<00:00,  1.00s/it, data=mixed, phase=2/3, states=5/7, loss=CE, train loss=612, val loss=592, best val loss=589]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:43<00:00,  1.00s/it, data=mixed, phase=2/3, states=6/7, loss=MSE, train loss=0.000165, val loss=0.000684, best val loss=0.000275]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:40<00:00,  1.00s/it, data=mixed, phase=2/3, states=6/7, loss=CE, train loss=609, val loss=591, best val loss=588]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:40<00:00,  1.00s/it, data=mixed, phase=2/3, states=7/7, loss=MSE, train loss=0.00406, val loss=0.00467, best val loss=0.000278]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:44<00:00,  1.00s/it, data=mixed, phase=2/3, states=7/7, loss=CE, train loss=610, val loss=588, best val loss=588]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:28<00:00,  1.05s/it, data=mixed, phase=3/3, states=1/7, loss=MSE, train loss=0.0292, val loss=0.0118, best val loss=0.00638]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:31<00:00,  1.05s/it, data=mixed, phase=3/3, states=1/7, loss=CE, train loss=642, val loss=614, best val loss=600]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:26<00:00,  1.05s/it, data=mixed, phase=3/3, states=2/7, loss=MSE, train loss=0.0021, val loss=0.00134, best val loss=0.000959]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:28<00:00,  1.05s/it, data=mixed, phase=3/3, states=2/7, loss=CE, train loss=612, val loss=594, best val loss=590]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:27<00:00,  1.05s/it, data=mixed, phase=3/3, states=3/7, loss=MSE, train loss=0.0297, val loss=0.0235, best val loss=0.00413]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:29<00:00,  1.05s/it, data=mixed, phase=3/3, states=3/7, loss=CE, train loss=636, val loss=613, best val loss=597]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:29<00:00,  1.05s/it, data=mixed, phase=3/3, states=4/7, loss=MSE, train loss=0.00145, val loss=0.000683, best val loss=0.000194]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:27<00:00,  1.05s/it, data=mixed, phase=3/3, states=4/7, loss=CE, train loss=611, val loss=591, best val loss=588]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:30<00:00,  1.05s/it, data=mixed, phase=3/3, states=5/7, loss=MSE, train loss=0.0104, val loss=0.00815, best val loss=0.00181]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:29<00:00,  1.05s/it, data=mixed, phase=3/3, states=5/7, loss=CE, train loss=629, val loss=602, best val loss=590]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:27<00:00,  1.05s/it, data=mixed, phase=3/3, states=6/7, loss=MSE, train loss=0.00136, val loss=0.00158, best val loss=8.68e-6] \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:26<00:00,  1.05s/it, data=mixed, phase=3/3, states=6/7, loss=CE, train loss=609, val loss=588, best val loss=588]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:27<00:00,  1.05s/it, data=mixed, phase=3/3, states=7/7, loss=MSE, train loss=0.00276, val loss=0.00184, best val loss=0.000188]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:31<00:00,  1.05s/it, data=mixed, phase=3/3, states=7/7, loss=CE, train loss=610, val loss=588, best val loss=588]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:43<00:00,  1.00s/it, data=normal, phase=1/3, states=1/7, loss=MSE, train loss=0.121, val loss=0.135, best val loss=0.132]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:42<00:00,  1.00s/it, data=normal, phase=1/3, states=1/7, loss=CE, train loss=812, val loss=784, best val loss=778]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:40<00:00,  1.00s/it, data=normal, phase=1/3, states=2/7, loss=MSE, train loss=0.12, val loss=0.137, best val loss=0.126]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:41<00:00,  1.00s/it, data=normal, phase=1/3, states=2/7, loss=CE, train loss=748, val loss=746, best val loss=735]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:42<00:00,  1.00s/it, data=normal, phase=1/3, states=3/7, loss=MSE, train loss=0.123, val loss=0.145, best val loss=0.111]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:44<00:00,  1.00s/it, data=normal, phase=1/3, states=3/7, loss=CE, train loss=716, val loss=712, best val loss=710]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:43<00:00,  1.00s/it, data=normal, phase=1/3, states=4/7, loss=MSE, train loss=0.121, val loss=0.139, best val loss=0.128]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:42<00:00,  1.00s/it, data=normal, phase=1/3, states=4/7, loss=CE, train loss=750, val loss=742, best val loss=735]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:39<00:00,  1.00it/s, data=normal, phase=1/3, states=5/7, loss=MSE, train loss=0.0108, val loss=0.00668, best val loss=0.00438]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:45<00:00,  1.01s/it, data=normal, phase=1/3, states=5/7, loss=CE, train loss=624, val loss=598, best val loss=593]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:38<00:00,  1.00it/s, data=normal, phase=1/3, states=6/7, loss=MSE, train loss=0.117, val loss=0.112, best val loss=0.11]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:42<00:00,  1.00s/it, data=normal, phase=1/3, states=6/7, loss=CE, train loss=751, val loss=711, best val loss=706]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:40<00:00,  1.00s/it, data=normal, phase=1/3, states=7/7, loss=MSE, train loss=0.0874, val loss=0.101, best val loss=0.096]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:44<00:00,  1.00s/it, data=normal, phase=1/3, states=7/7, loss=CE, train loss=705, val loss=717, best val loss=685]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:43<00:00,  1.00s/it, data=normal, phase=2/3, states=1/7, loss=MSE, train loss=0.0516, val loss=0.0336, best val loss=0.0156]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:44<00:00,  1.00s/it, data=normal, phase=2/3, states=1/7, loss=CE, train loss=639, val loss=618, best val loss=608]  \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:42<00:00,  1.00s/it, data=normal, phase=2/3, states=2/7, loss=MSE, train loss=0.0194, val loss=0.00881, best val loss=0.0056]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:45<00:00,  1.01s/it, data=normal, phase=2/3, states=2/7, loss=CE, train loss=637, val loss=617, best val loss=600]  \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:45<00:00,  1.01s/it, data=normal, phase=2/3, states=3/7, loss=MSE, train loss=0.0265, val loss=0.0164, best val loss=0.0106]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:46<00:00,  1.01s/it, data=normal, phase=2/3, states=3/7, loss=CE, train loss=633, val loss=605, best val loss=602]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:42<00:00,  1.00s/it, data=normal, phase=2/3, states=4/7, loss=MSE, train loss=0.0166, val loss=0.0061, best val loss=0.00255]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:48<00:00,  1.01s/it, data=normal, phase=2/3, states=4/7, loss=CE, train loss=625, val loss=599, best val loss=597]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:47<00:00,  1.01s/it, data=normal, phase=2/3, states=5/7, loss=MSE, train loss=0.0011, val loss=0.0018, best val loss=0.000343]   \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:44<00:00,  1.00s/it, data=normal, phase=2/3, states=5/7, loss=CE, train loss=612, val loss=589, best val loss=588]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:42<00:00,  1.00s/it, data=normal, phase=2/3, states=6/7, loss=MSE, train loss=0.000986, val loss=0.00163, best val loss=0.00069]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:45<00:00,  1.01s/it, data=normal, phase=2/3, states=6/7, loss=CE, train loss=610, val loss=592, best val loss=589]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:43<00:00,  1.00s/it, data=normal, phase=2/3, states=7/7, loss=MSE, train loss=0.0008, val loss=0.00103, best val loss=0.000447]  \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [16:49<00:00,  1.01s/it, data=normal, phase=2/3, states=7/7, loss=CE, train loss=610, val loss=590, best val loss=588]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:32<00:00,  1.05s/it, data=normal, phase=3/3, states=1/7, loss=MSE, train loss=0.0747, val loss=0.0502, best val loss=0.0377]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:33<00:00,  1.05s/it, data=normal, phase=3/3, states=1/7, loss=CE, train loss=812, val loss=784, best val loss=783]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:31<00:00,  1.05s/it, data=normal, phase=3/3, states=2/7, loss=MSE, train loss=0.0642, val loss=0.0778, best val loss=0.0111]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:28<00:00,  1.05s/it, data=normal, phase=3/3, states=2/7, loss=CE, train loss=649, val loss=615, best val loss=608]  \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:32<00:00,  1.05s/it, data=normal, phase=3/3, states=3/7, loss=MSE, train loss=0.163, val loss=0.149, best val loss=0.0376]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:32<00:00,  1.05s/it, data=normal, phase=3/3, states=3/7, loss=CE, train loss=679, val loss=678, best val loss=635]  \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:28<00:00,  1.05s/it, data=normal, phase=3/3, states=4/7, loss=MSE, train loss=0.0282, val loss=0.013, best val loss=0.00109] \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:29<00:00,  1.05s/it, data=normal, phase=3/3, states=4/7, loss=CE, train loss=671, val loss=622, best val loss=590]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:31<00:00,  1.05s/it, data=normal, phase=3/3, states=5/7, loss=MSE, train loss=0.003, val loss=0.00278, best val loss=0.0019] \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:32<00:00,  1.05s/it, data=normal, phase=3/3, states=5/7, loss=CE, train loss=621, val loss=599, best val loss=591]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:33<00:00,  1.05s/it, data=normal, phase=3/3, states=6/7, loss=MSE, train loss=0.0682, val loss=0.132, best val loss=0.000542]  \n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:35<00:00,  1.06s/it, data=normal, phase=3/3, states=6/7, loss=CE, train loss=615, val loss=594, best val loss=589]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:25<00:00,  1.05s/it, data=normal, phase=3/3, states=7/7, loss=MSE, train loss=0.00474, val loss=0.0027, best val loss=0.000806]\n",
      "Epoch 1000/1000: 100%|██████████| 1000/1000 [17:24<00:00,  1.04s/it, data=normal, phase=3/3, states=7/7, loss=CE, train loss=616, val loss=592, best val loss=589]\n"
     ]
    }
   ],
   "source": [
    "phases = ['before', 'initial', 'final'] if args.data == '3D/new/' else ['whole']\n",
    "phase2len = {'before':96, 'initial':352, 'final':2240} if args.data == '3D/new/' else {'whole':3200}\n",
    "chTs = ['Rmi', 'Vmi', 'Wmi', 'Accm', 'angEuler', 'FinOut', 'FinCmd'] if args.data == '3D/new/' else ['Rmi']\n",
    "ch = {'Rmi':3, 'Vmi':3, 'Wmi':3, 'Accm':3, 'angEuler':3, 'FinOut':4, 'FinCmd':4} if args.data == '3D/new/' else {'Rmi':2}\n",
    "# plot_cnt = 0\n",
    "temp_seq_len = args.seq_len\n",
    "\n",
    "for dataT in ['mixed', 'normal', 'None']:\n",
    "    # if dataT == 'normal':\n",
    "    #     continue\n",
    "    args.seq_len = temp_seq_len\n",
    "    args.data_type = dataT\n",
    "    # train_set, train_loader = data_provider(args, 'train_c')\n",
    "    # valid_set, valid_loader = data_provider(args, 'valid')\n",
    "    # test_set, test_loader = data_provider(args, 'test')\n",
    "    # data_loaders = {'train': train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "    if dataT == 'mixed':\n",
    "        data_loaders = mixed_C_loaders\n",
    "    else:\n",
    "        data_loaders = normal_C_loaders\n",
    "    \n",
    "    for phase in phases:\n",
    "        # plot_cnt += 1\n",
    "        # plt.subplot(4, len(phases), plot_cnt)\n",
    "        for chT in chTs:\n",
    "            if args.data == '3D/new/':\n",
    "                args.seq_len = phase2len[phase]\n",
    "                args.input_channel = ch[chT]\n",
    "            for lossf in ['MSE/', 'CE/']:\n",
    "                if lossf == 'MSE/':\n",
    "                    criterion = nn.MSELoss().to(args.device)\n",
    "                else:\n",
    "                    criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "                load_ckpt = f'{args.checkpoints+args.data}{args.train_epochs}/{args.model}_{args.data_type}_{chT}_{phase}_'\n",
    "                save_ckpt = f'{args.checkpoints+args.data}{args.train_epochs}/{lossf+args.model}_{args.data_type}_{chT}_{phase}_'\n",
    "                \n",
    "                # print(f'{dataT} set {chT} featrue {phase} phase {lossf[:-1]} loss Classifier model training start')\n",
    "                encoder = ConvEncoder(args.input_channel, args.t, args.output_channel).to(args.device)\n",
    "                if dataT != 'None':\n",
    "                    encoder.load_state_dict(torch.load(load_ckpt+'encoder.pth'))\n",
    "                model = Classifier(args.seq_len//(2**args.layer_num)*args.output_channel, args.class_num).to(args.device)\n",
    "                if args.multi_gpu:\n",
    "                    encoder = nn.DataParallel(encoder)\n",
    "                    model = nn.DataParallel(model)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "                scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5) if args.scheduling else None\n",
    "                \n",
    "                trained_model, best_wts, train_loss_history, val_loss_history = train_classifier(encoder, model, data_loaders, criterion, optimizer, scheduler, args.train_epochs, phase, chT)\n",
    "                trained_wts = trained_model.module.state_dict() if args.multi_gpu else trained_model.state_dict()\n",
    "                torch.save(trained_wts, save_ckpt+'classifier.pth')\n",
    "                torch.save(best_wts, save_ckpt+'best_classifier.pth')\n",
    "                # print('model saved to %s' % save_ckpt)\n",
    "                \n",
    "                # plt.figure(figsize=(4, 3))\n",
    "                # plt.plot(train_loss_history, label=f'{phase} {chT} train')\n",
    "                # plt.plot(val_loss_history, label=f'{phase} {chT} val')\n",
    "        #     break\n",
    "        # break\n",
    "    args.seq_len = temp_seq_len\n",
    "    # break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.seq_len = 2651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed before Rmi classifier MSE/\n",
      "Accuracy: 51.33333333333333 %\n",
      "mixed before Rmi best_classifier MSE/\n",
      "Accuracy: 59.86666666666667 %\n",
      "mixed before Rmi classifier CE/\n",
      "Accuracy: 26.400000000000002 %\n",
      "mixed before Rmi best_classifier CE/\n",
      "Accuracy: 59.199999999999996 %\n",
      "mixed before Vmi classifier MSE/\n",
      "Accuracy: 26.13333333333333 %\n",
      "mixed before Vmi best_classifier MSE/\n",
      "Accuracy: 59.86666666666667 %\n",
      "mixed before Vmi classifier CE/\n",
      "Accuracy: 26.53333333333333 %\n",
      "mixed before Vmi best_classifier CE/\n",
      "Accuracy: 59.86666666666667 %\n",
      "mixed before Wmi classifier MSE/\n",
      "Accuracy: 23.733333333333334 %\n",
      "mixed before Wmi best_classifier MSE/\n",
      "Accuracy: 23.866666666666667 %\n",
      "mixed before Wmi classifier CE/\n",
      "Accuracy: 9.066666666666666 %\n",
      "mixed before Wmi best_classifier CE/\n",
      "Accuracy: 8.799999999999999 %\n",
      "mixed before Accm classifier MSE/\n",
      "Accuracy: 26.666666666666668 %\n",
      "mixed before Accm best_classifier MSE/\n",
      "Accuracy: 60.0 %\n",
      "mixed before Accm classifier CE/\n",
      "Accuracy: 61.33333333333333 %\n",
      "mixed before Accm best_classifier CE/\n",
      "Accuracy: 59.86666666666667 %\n",
      "mixed before angEuler classifier MSE/\n",
      "Accuracy: 48.4 %\n",
      "mixed before angEuler best_classifier MSE/\n",
      "Accuracy: 73.33333333333333 %\n",
      "mixed before angEuler classifier CE/\n",
      "Accuracy: 65.06666666666666 %\n",
      "mixed before angEuler best_classifier CE/\n",
      "Accuracy: 69.33333333333334 %\n",
      "mixed before FinOut classifier MSE/\n",
      "Accuracy: 58.266666666666666 %\n",
      "mixed before FinOut best_classifier MSE/\n",
      "Accuracy: 79.46666666666667 %\n",
      "mixed before FinOut classifier CE/\n",
      "Accuracy: 45.6 %\n",
      "mixed before FinOut best_classifier CE/\n",
      "Accuracy: 43.06666666666666 %\n",
      "mixed before FinCmd classifier MSE/\n",
      "Accuracy: 36.4 %\n",
      "mixed before FinCmd best_classifier MSE/\n",
      "Accuracy: 82.0 %\n",
      "mixed before FinCmd classifier CE/\n",
      "Accuracy: 38.93333333333333 %\n",
      "mixed before FinCmd best_classifier CE/\n",
      "Accuracy: 83.06666666666666 %\n",
      "mixed initial Rmi classifier MSE/\n",
      "Accuracy: 42.93333333333334 %\n",
      "mixed initial Rmi best_classifier MSE/\n",
      "Accuracy: 48.266666666666666 %\n",
      "mixed initial Rmi classifier CE/\n",
      "Accuracy: 43.86666666666667 %\n",
      "mixed initial Rmi best_classifier CE/\n",
      "Accuracy: 49.06666666666666 %\n",
      "mixed initial Vmi classifier MSE/\n",
      "Accuracy: 50.13333333333333 %\n",
      "mixed initial Vmi best_classifier MSE/\n",
      "Accuracy: 64.93333333333334 %\n",
      "mixed initial Vmi classifier CE/\n",
      "Accuracy: 64.4 %\n",
      "mixed initial Vmi best_classifier CE/\n",
      "Accuracy: 66.13333333333333 %\n",
      "mixed initial Wmi classifier MSE/\n",
      "Accuracy: 13.333333333333334 %\n",
      "mixed initial Wmi best_classifier MSE/\n",
      "Accuracy: 13.333333333333334 %\n",
      "mixed initial Wmi classifier CE/\n",
      "Accuracy: 13.333333333333334 %\n",
      "mixed initial Wmi best_classifier CE/\n",
      "Accuracy: 46.666666666666664 %\n",
      "mixed initial Accm classifier MSE/\n",
      "Accuracy: 72.53333333333333 %\n",
      "mixed initial Accm best_classifier MSE/\n",
      "Accuracy: 90.66666666666666 %\n",
      "mixed initial Accm classifier CE/\n",
      "Accuracy: 80.53333333333333 %\n",
      "mixed initial Accm best_classifier CE/\n",
      "Accuracy: 72.26666666666667 %\n",
      "mixed initial angEuler classifier MSE/\n",
      "Accuracy: 53.86666666666666 %\n",
      "mixed initial angEuler best_classifier MSE/\n",
      "Accuracy: 43.46666666666666 %\n",
      "mixed initial angEuler classifier CE/\n",
      "Accuracy: 64.4 %\n",
      "mixed initial angEuler best_classifier CE/\n",
      "Accuracy: 80.80000000000001 %\n",
      "mixed initial FinOut classifier MSE/\n",
      "Accuracy: 99.46666666666667 %\n",
      "mixed initial FinOut best_classifier MSE/\n",
      "Accuracy: 99.73333333333333 %\n",
      "mixed initial FinOut classifier CE/\n",
      "Accuracy: 99.46666666666667 %\n",
      "mixed initial FinOut best_classifier CE/\n",
      "Accuracy: 99.6 %\n",
      "mixed initial FinCmd classifier MSE/\n",
      "Accuracy: 89.86666666666666 %\n",
      "mixed initial FinCmd best_classifier MSE/\n",
      "Accuracy: 99.46666666666667 %\n",
      "mixed initial FinCmd classifier CE/\n",
      "Accuracy: 90.93333333333334 %\n",
      "mixed initial FinCmd best_classifier CE/\n",
      "Accuracy: 98.8 %\n",
      "mixed final Rmi classifier MSE/\n",
      "Accuracy: 45.33333333333333 %\n",
      "mixed final Rmi best_classifier MSE/\n",
      "Accuracy: 85.06666666666666 %\n",
      "mixed final Rmi classifier CE/\n",
      "Accuracy: 69.06666666666666 %\n",
      "mixed final Rmi best_classifier CE/\n",
      "Accuracy: 56.00000000000001 %\n",
      "mixed final Vmi classifier MSE/\n",
      "Accuracy: 88.53333333333333 %\n",
      "mixed final Vmi best_classifier MSE/\n",
      "Accuracy: 93.73333333333333 %\n",
      "mixed final Vmi classifier CE/\n",
      "Accuracy: 64.13333333333333 %\n",
      "mixed final Vmi best_classifier CE/\n",
      "Accuracy: 80.0 %\n",
      "mixed final Wmi classifier MSE/\n",
      "Accuracy: 9.466666666666667 %\n",
      "mixed final Wmi best_classifier MSE/\n",
      "Accuracy: 10.266666666666667 %\n",
      "mixed final Wmi classifier CE/\n",
      "Accuracy: 17.466666666666665 %\n",
      "mixed final Wmi best_classifier CE/\n",
      "Accuracy: 18.933333333333334 %\n",
      "mixed final Accm classifier MSE/\n",
      "Accuracy: 98.13333333333333 %\n",
      "mixed final Accm best_classifier MSE/\n",
      "Accuracy: 99.06666666666666 %\n",
      "mixed final Accm classifier CE/\n",
      "Accuracy: 98.66666666666667 %\n",
      "mixed final Accm best_classifier CE/\n",
      "Accuracy: 99.46666666666667 %\n",
      "mixed final angEuler classifier MSE/\n",
      "Accuracy: 37.6 %\n",
      "mixed final angEuler best_classifier MSE/\n",
      "Accuracy: 40.666666666666664 %\n",
      "mixed final angEuler classifier CE/\n",
      "Accuracy: 37.06666666666666 %\n",
      "mixed final angEuler best_classifier CE/\n",
      "Accuracy: 37.6 %\n",
      "mixed final FinOut classifier MSE/\n",
      "Accuracy: 98.66666666666667 %\n",
      "mixed final FinOut best_classifier MSE/\n",
      "Accuracy: 90.4 %\n",
      "mixed final FinOut classifier CE/\n",
      "Accuracy: 92.26666666666667 %\n",
      "mixed final FinOut best_classifier CE/\n",
      "Accuracy: 98.53333333333333 %\n",
      "mixed final FinCmd classifier MSE/\n",
      "Accuracy: 90.53333333333333 %\n",
      "mixed final FinCmd best_classifier MSE/\n",
      "Accuracy: 99.2 %\n",
      "mixed final FinCmd classifier CE/\n",
      "Accuracy: 90.8 %\n",
      "mixed final FinCmd best_classifier CE/\n",
      "Accuracy: 91.73333333333333 %\n",
      "normal before Rmi classifier MSE/\n",
      "Accuracy: 59.46666666666667 %\n",
      "normal before Rmi best_classifier MSE/\n",
      "Accuracy: 59.86666666666667 %\n",
      "normal before Rmi classifier CE/\n",
      "Accuracy: 46.666666666666664 %\n",
      "normal before Rmi best_classifier CE/\n",
      "Accuracy: 46.666666666666664 %\n",
      "normal before Vmi classifier MSE/\n",
      "Accuracy: 45.6 %\n",
      "normal before Vmi best_classifier MSE/\n",
      "Accuracy: 34.93333333333333 %\n",
      "normal before Vmi classifier CE/\n",
      "Accuracy: 29.599999999999998 %\n",
      "normal before Vmi best_classifier CE/\n",
      "Accuracy: 32.0 %\n",
      "normal before Wmi classifier MSE/\n",
      "Accuracy: 6.800000000000001 %\n",
      "normal before Wmi best_classifier MSE/\n",
      "Accuracy: 20.8 %\n",
      "normal before Wmi classifier CE/\n",
      "Accuracy: 55.2 %\n",
      "normal before Wmi best_classifier CE/\n",
      "Accuracy: 46.666666666666664 %\n",
      "normal before Accm classifier MSE/\n",
      "Accuracy: 26.400000000000002 %\n",
      "normal before Accm best_classifier MSE/\n",
      "Accuracy: 59.86666666666667 %\n",
      "normal before Accm classifier CE/\n",
      "Accuracy: 59.73333333333334 %\n",
      "normal before Accm best_classifier CE/\n",
      "Accuracy: 59.46666666666667 %\n",
      "normal before angEuler classifier MSE/\n",
      "Accuracy: 26.0 %\n",
      "normal before angEuler best_classifier MSE/\n",
      "Accuracy: 26.13333333333333 %\n",
      "normal before angEuler classifier CE/\n",
      "Accuracy: 27.6 %\n",
      "normal before angEuler best_classifier CE/\n",
      "Accuracy: 27.6 %\n",
      "normal before FinOut classifier MSE/\n",
      "Accuracy: 60.53333333333333 %\n",
      "normal before FinOut best_classifier MSE/\n",
      "Accuracy: 59.199999999999996 %\n",
      "normal before FinOut classifier CE/\n",
      "Accuracy: 60.13333333333334 %\n",
      "normal before FinOut best_classifier CE/\n",
      "Accuracy: 58.93333333333334 %\n",
      "normal before FinCmd classifier MSE/\n",
      "Accuracy: 22.26666666666667 %\n",
      "normal before FinCmd best_classifier MSE/\n",
      "Accuracy: 18.266666666666666 %\n",
      "normal before FinCmd classifier CE/\n",
      "Accuracy: 27.066666666666666 %\n",
      "normal before FinCmd best_classifier CE/\n",
      "Accuracy: 41.199999999999996 %\n",
      "normal initial Rmi classifier MSE/\n",
      "Accuracy: 46.666666666666664 %\n",
      "normal initial Rmi best_classifier MSE/\n",
      "Accuracy: 45.06666666666666 %\n",
      "normal initial Rmi classifier CE/\n",
      "Accuracy: 37.733333333333334 %\n",
      "normal initial Rmi best_classifier CE/\n",
      "Accuracy: 35.733333333333334 %\n",
      "normal initial Vmi classifier MSE/\n",
      "Accuracy: 83.33333333333334 %\n",
      "normal initial Vmi best_classifier MSE/\n",
      "Accuracy: 80.80000000000001 %\n",
      "normal initial Vmi classifier CE/\n",
      "Accuracy: 62.66666666666667 %\n",
      "normal initial Vmi best_classifier CE/\n",
      "Accuracy: 85.73333333333333 %\n",
      "normal initial Wmi classifier MSE/\n",
      "Accuracy: 13.733333333333334 %\n",
      "normal initial Wmi best_classifier MSE/\n",
      "Accuracy: 13.466666666666665 %\n",
      "normal initial Wmi classifier CE/\n",
      "Accuracy: 19.733333333333334 %\n",
      "normal initial Wmi best_classifier CE/\n",
      "Accuracy: 19.866666666666667 %\n",
      "normal initial Accm classifier MSE/\n",
      "Accuracy: 97.86666666666667 %\n",
      "normal initial Accm best_classifier MSE/\n",
      "Accuracy: 91.06666666666666 %\n",
      "normal initial Accm classifier CE/\n",
      "Accuracy: 52.26666666666666 %\n",
      "normal initial Accm best_classifier CE/\n",
      "Accuracy: 82.26666666666667 %\n",
      "normal initial angEuler classifier MSE/\n",
      "Accuracy: 17.333333333333336 %\n",
      "normal initial angEuler best_classifier MSE/\n",
      "Accuracy: 19.733333333333334 %\n",
      "normal initial angEuler classifier CE/\n",
      "Accuracy: 22.0 %\n",
      "normal initial angEuler best_classifier CE/\n",
      "Accuracy: 20.933333333333334 %\n",
      "normal initial FinOut classifier MSE/\n",
      "Accuracy: 48.266666666666666 %\n",
      "normal initial FinOut best_classifier MSE/\n",
      "Accuracy: 49.46666666666666 %\n",
      "normal initial FinOut classifier CE/\n",
      "Accuracy: 44.266666666666666 %\n",
      "normal initial FinOut best_classifier CE/\n",
      "Accuracy: 45.2 %\n",
      "normal initial FinCmd classifier MSE/\n",
      "Accuracy: 43.733333333333334 %\n",
      "normal initial FinCmd best_classifier MSE/\n",
      "Accuracy: 44.666666666666664 %\n",
      "normal initial FinCmd classifier CE/\n",
      "Accuracy: 41.733333333333334 %\n",
      "normal initial FinCmd best_classifier CE/\n",
      "Accuracy: 43.06666666666666 %\n",
      "normal final Rmi classifier MSE/\n",
      "Accuracy: 39.733333333333334 %\n",
      "normal final Rmi best_classifier MSE/\n",
      "Accuracy: 42.0 %\n",
      "normal final Rmi classifier CE/\n",
      "Accuracy: 77.73333333333333 %\n",
      "normal final Rmi best_classifier CE/\n",
      "Accuracy: 42.4 %\n",
      "normal final Vmi classifier MSE/\n",
      "Accuracy: 93.46666666666667 %\n",
      "normal final Vmi best_classifier MSE/\n",
      "Accuracy: 89.2 %\n",
      "normal final Vmi classifier CE/\n",
      "Accuracy: 93.46666666666667 %\n",
      "normal final Vmi best_classifier CE/\n",
      "Accuracy: 70.13333333333334 %\n",
      "normal final Wmi classifier MSE/\n",
      "Accuracy: 51.6 %\n",
      "normal final Wmi best_classifier MSE/\n",
      "Accuracy: 50.13333333333333 %\n",
      "normal final Wmi classifier CE/\n",
      "Accuracy: 11.600000000000001 %\n",
      "normal final Wmi best_classifier CE/\n",
      "Accuracy: 13.066666666666665 %\n",
      "normal final Accm classifier MSE/\n",
      "Accuracy: 57.199999999999996 %\n",
      "normal final Accm best_classifier MSE/\n",
      "Accuracy: 80.13333333333334 %\n",
      "normal final Accm classifier CE/\n",
      "Accuracy: 62.0 %\n",
      "normal final Accm best_classifier CE/\n",
      "Accuracy: 83.73333333333333 %\n",
      "normal final angEuler classifier MSE/\n",
      "Accuracy: 91.33333333333333 %\n",
      "normal final angEuler best_classifier MSE/\n",
      "Accuracy: 71.86666666666667 %\n",
      "normal final angEuler classifier CE/\n",
      "Accuracy: 96.26666666666667 %\n",
      "normal final angEuler best_classifier CE/\n",
      "Accuracy: 96.13333333333334 %\n",
      "normal final FinOut classifier MSE/\n",
      "Accuracy: 82.93333333333334 %\n",
      "normal final FinOut best_classifier MSE/\n",
      "Accuracy: 75.73333333333333 %\n",
      "normal final FinOut classifier CE/\n",
      "Accuracy: 61.199999999999996 %\n",
      "normal final FinOut best_classifier CE/\n",
      "Accuracy: 75.6 %\n",
      "normal final FinCmd classifier MSE/\n",
      "Accuracy: 98.13333333333333 %\n",
      "normal final FinCmd best_classifier MSE/\n",
      "Accuracy: 74.26666666666667 %\n",
      "normal final FinCmd classifier CE/\n",
      "Accuracy: 97.73333333333333 %\n",
      "normal final FinCmd best_classifier CE/\n",
      "Accuracy: 80.0 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "phases = ['before', 'initial', 'final'] if args.data == '3D/new/' else ['whole']\n",
    "phase2len = {'before':96, 'initial':352, 'final':2240} if args.data == '3D/new/' else {'whole':3200}\n",
    "seq_start = {'before':0, 'initial':80, 'final':411} if args.data == '3D/new/' else {'whole':0}\n",
    "chTs = ['Rmi', 'Vmi', 'Wmi', 'Accm', 'angEuler', 'FinOut', 'FinCmd'] if args.data == '3D/new/' else ['Rmi']\n",
    "ch = {'Rmi':3, 'Vmi':3, 'Wmi':3, 'Accm':3, 'angEuler':3, 'FinOut':4, 'FinCmd':4} if args.data == '3D/new/' else {'Rmi':2}\n",
    "channel_start = {'Rmi':0, 'Vmi':3, 'Wmi':6, 'Accm':9, 'angEuler':12, 'FinOut':15, 'FinCmd':19} if args.data == '3D/new/' else {'Rmi':0}\n",
    "plot_cnt = 0\n",
    "temp_seq_len = args.seq_len\n",
    "\n",
    "for dataT in ['mixed', 'normal']:\n",
    "    # if dataT == 'normal':\n",
    "    #     continue\n",
    "    args.seq_len = temp_seq_len\n",
    "    args.data_type = dataT\n",
    "    # train_set, train_loader = data_provider(args, 'train_c')\n",
    "    # valid_set, valid_loader = data_provider(args, 'valid')\n",
    "    # test_set, test_loader = data_provider(args, 'test')\n",
    "    # data_loaders = {'train': train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "    if dataT == 'mixed':\n",
    "        data_loaders = mixed_C_loaders\n",
    "    else:\n",
    "        data_loaders = normal_C_loaders\n",
    "    \n",
    "    for phase in phases:\n",
    "        for chT in chTs:\n",
    "            if args.data == '3D/new/':\n",
    "                args.seq_len = phase2len[phase]\n",
    "                args.input_channel = ch[chT]\n",
    "            for lossf in ['MSE/', 'CE/']:\n",
    "                if lossf == 'MSE/':\n",
    "                    criterion = nn.MSELoss().to(args.device)\n",
    "                else:\n",
    "                    criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "                encoder_ckpt = f'{args.checkpoints+args.data}{args.train_epochs}/{args.model}_{args.data_type}_{chT}_{phase}_'\n",
    "                classifier_ckpt = f'{args.checkpoints+args.data}{args.train_epochs}/{lossf+args.model}_{args.data_type}_{chT}_{phase}_'\n",
    "                \n",
    "                ts = seq_start[phase]\n",
    "                te = ts + args.seq_len\n",
    "                cs = channel_start[chT]\n",
    "                ce = cs + args.input_channel\n",
    "                \n",
    "                for classifierT in ['', 'best_']:\n",
    "                    # if classifierT == '':\n",
    "                    #     continue\n",
    "                    encoder = ConvEncoder(args.input_channel, args.t, args.output_channel).to(args.device)\n",
    "                    encoder.load_state_dict(torch.load(encoder_ckpt+'encoder.pth'))\n",
    "                    classifier = Classifier(args.seq_len//(2**args.layer_num)*args.output_channel, args.class_num).to(args.device)\n",
    "                    classifier.load_state_dict(torch.load(classifier_ckpt+classifierT+'classifier.pth'))\n",
    "                    \n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for inputs, labels in data_loaders[\"test\"]:\n",
    "                            inputs = inputs.to(args.device)[:,ts:te,cs:ce].type(torch.float32).transpose(1, 2)\n",
    "                            labels_onehot = F.one_hot(labels, num_classes=args.class_num)\n",
    "                            labels = labels.to(args.device).type(torch.float32)\n",
    "\n",
    "                            hidden = encoder(inputs)\n",
    "                            outputs = classifier(hidden.reshape(hidden.size(0), -1)).type(torch.float32)\n",
    "                            \n",
    "                            total += labels.size(0)\n",
    "                            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                            \n",
    "                            preds_onehot = F.one_hot(outputs.argmax(1), num_classes=args.class_num)\n",
    "                            \n",
    "                    print(args.data_type, phase, chT, classifierT+'classifier', lossf)\n",
    "                    print('Accuracy:', correct/total * 100, '%')\n",
    "        #     break\n",
    "        # break\n",
    "    args.seq_len = temp_seq_len\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
