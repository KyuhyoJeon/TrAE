{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from Models import ConvEncoder, ConvDecoder, TrAE, Classifier, TrVAE, VAE_loss\n",
    "from Dataset import data_provider\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import easydict\n",
    "\n",
    "args = easydict.EasyDict({\n",
    "    'is_training': 1,\n",
    "    'model': 'A', # ['A', 'B']\n",
    "    ### data loader\n",
    "    'root_path': './Data/',\n",
    "    'data': '3D/new/',\n",
    "    'data_path': 'csv/',\n",
    "    'data_type': 'normal',\n",
    "    'checkpoints': './checkpoints/',\n",
    "    'Nsamples': 500, \n",
    "    'ratio': [7, 1, 2],\n",
    "    ### forecasting task\n",
    "    'seq_len': 2651, # [2144, 4576, 2651] # 3200 in paper\n",
    "    'label_len': 100,\n",
    "    'pred_len': 100,\n",
    "    'class_num': 5,\n",
    "    'individual': False,\n",
    "    ### transformer\n",
    "    'layer_num': 1,\n",
    "    'dropout': 0.1,\n",
    "    'max_len': 5000,\n",
    "    'd_model': 512,\n",
    "    'd_h': 8,\n",
    "    'd_ff': 2048,\n",
    "    ### Layers\n",
    "    'layer_num': 5,\n",
    "    'input_channel': 23,\n",
    "    't': 128, \n",
    "    'output_channel': 23,\n",
    "    'do_predict': True,\n",
    "    ### optimization\n",
    "    'num_workers': 8,\n",
    "    'itr': 1,\n",
    "    'train_epochs': 100,\n",
    "    'batch_size': 64,\n",
    "    'patience': 3,\n",
    "    'lr': 0.005,\n",
    "    ### GPU\n",
    "    'device': 'cuda:0',\n",
    "    'use_gpu': True,\n",
    "    'multi_gpu': True,\n",
    "})\n",
    "\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(args.device)\n",
    "\n",
    "if args.data == '2D/':\n",
    "    args.data_path = ''\n",
    "    args.seq_len = 3200  # 3200 in paper\n",
    "    args.class_num = 4\n",
    "    args.input_channel = 2\n",
    "    args.t = 16\n",
    "    args.output_channel = 4\n",
    "    args.ratio = [3,1,1]\n",
    "elif args.data == '3D/old/':\n",
    "    args.data_path = ''\n",
    "    args.seq_len = 4576\n",
    "    args.input_channel = 3\n",
    "    args.t = 64\n",
    "    args.output_channel = 8\n",
    "elif args.data == '3D/new/':\n",
    "    args.input_channel = 3 # [3, 3, 3, 3, 3, 4, 4]\n",
    "    args.t = 16\n",
    "    args.output_channel = 4\n",
    "    \n",
    "if args.model == 'B':\n",
    "    args.seq_len = 400\n",
    "    \n",
    "temp_data_type = args.data_type\n",
    "args.data_type = 'normal'\n",
    "normal_train_set, normal_train_loader = data_provider(args, 'train')\n",
    "normal_train_C_set, normal_train_C_loader = data_provider(args, 'train_c')\n",
    "args.data_type = 'mixed'\n",
    "mixed_train_set, mixed_train_loader = data_provider(args, 'train')\n",
    "mixed_train_C_set, mixed_train_C_loader = data_provider(args, 'train_c')\n",
    "\n",
    "valid_set, valid_loader = data_provider(args, 'valid')\n",
    "test_set, test_loader = data_provider(args, 'test')\n",
    "\n",
    "normal_loaders = {'train': normal_train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "normal_C_loaders = {'train': normal_train_C_loader, 'val': valid_loader, 'test': test_loader}\n",
    "mixed_loaders = {'train': mixed_train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "mixed_C_loaders = {'train': mixed_train_C_loader, 'val': valid_loader, 'test': test_loader}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-Dimensional Trajectory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 .pickle 로드 및 전처리 후 .npy 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "'''\n",
    "Trajectory types: Normal, autopilot_lag, LOSrate_bias, LOSrate_delay\n",
    "Target distance = uniform(4500, 5500)\n",
    "states = [x, y, vm, path_angle] + 1 control input, 5 states\n",
    "label = 0, 1, 2, 3 for normal, autopilot_lag, LOSrate_bias, LOSrate_delay respectively, 4 classes\n",
    "minimum length of trajectory = 1360 (1360*0.01 = 13.6 seconds)\n",
    "maximum length of trajectory = 2131 (2131*0.01 = 21.31 seconds)\n",
    "fixed sequence length = 2144 (2400*0.01 = 24 seconds)\n",
    "'''\n",
    "start = time.time()\n",
    "with open('Data/2D/Trajectories.pickle', 'rb') as f:\n",
    "    Dataset = pickle.load(f)\n",
    "\n",
    "# normal = Dataset['normal_PNG']\n",
    "# lag = Dataset['autopilot_lag']\n",
    "# bias = Dataset['LOSrate_bias']\n",
    "# delay = Dataset['LOSrate_delay']\n",
    "seq_len = 2144\n",
    "\n",
    "label = 0\n",
    "x = []\n",
    "y = []\n",
    "for tr_type in Dataset:\n",
    "    states = Dataset[tr_type]['states']\n",
    "    inputs = Dataset[tr_type]['actions']\n",
    "    for i in range(len(states)):\n",
    "        tr = np.concatenate((states[i], np.insert(inputs[i], 0, 0).reshape(-1 ,1)), axis=1)\n",
    "        terminal = tr[-1].copy()\n",
    "        terminal[2]=terminal[4]=0\n",
    "        tr = np.concatenate((tr, np.tile(terminal, (seq_len-len(tr), 1))), axis=0)\n",
    "        x.append(tr)\n",
    "        y.append(label)\n",
    "    label += 1\n",
    "x = np.array(x)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "np.save('Data/2D/x.npy', x)\n",
    "np.save('Data/2D/y.npy', y)\n",
    "# x = np.load('Data/2D/x.npy')\n",
    "# y = np.load('Data/2D/y.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".npy 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('Data/2D/x.npy')\n",
    "y = np.load('Data/2D/y.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Dimentional Trajectory old version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 .pickle 로드 및 전처리 후 .npy 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trajectory types: Normal, Burn time, Xcp position, Thrust Tilt Angle, Fin bias\n",
    "Target Distance = 4000\n",
    "States = ['Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Accm_Cmd_1', 'Accm_Cmd_2', 'Accm_Cmd_3', 'PhiCmd'], 16 states\n",
    "label = [0, 1, 2, 3, 4] for normal, burn_time, xcp_pos, thrust_tilt, fin_bias respectively, 5 classes\n",
    "minimum length of trajectory = 412 (412*0.01 = 4.12 seconds)\n",
    "maximum length of trajectory = 4569 (4569*0.01 = 45.69 seconds)\n",
    "fixed sequence length = 4576 (4576*0.01 = 45.76 seconds)\n",
    "'''\n",
    "start = time.time()\n",
    "seq_len = 4576\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for i, tr_type in enumerate(['Normal', 'Burntime', 'Xcpposition', \"ThrustTiltAngle\", 'Finbias']):\n",
    "    gid = pd.read_excel(f'Data/3D/old/Gid_{i+1}.xlsx', sheet_name=None)\n",
    "    msl = pd.read_excel(f'Data/3D/old/Msl_{i+1}.xlsx', sheet_name=None)\n",
    "    \n",
    "    for sheet in gid:\n",
    "        tr = pd.merge(msl[sheet][['Time', 'Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3']], gid[sheet][['Time', 'Accm_Cmd_1', 'Accm_Cmd_2', 'Accm_Cmd_3', 'Phi_Cmd']]).to_numpy()[:, 1:]\n",
    "        terminal = tr[-1].copy()\n",
    "        terminal[3]=terminal[4]=terminal[5]=terminal[6]=terminal[7]=terminal[8]=terminal[12]=terminal[13]=terminal[14]=terminal[15]=0\n",
    "        tr = np.concatenate((tr, np.tile(terminal, (seq_len-len(tr), 1))), axis=0)\n",
    "        if sheet == 'Sheet1' and tr_type == 'Normal':\n",
    "            x = [tr for _ in range(140)]\n",
    "            y = [i for _ in range(140)]\n",
    "        else:\n",
    "            x.append(tr)\n",
    "            y.append(i)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "np.save('Data/3D/x_old.npy', x)\n",
    "np.save('Data/3D/y_old.npy', y)\n",
    "# x = np.load('Data/3D/x_old.npy')\n",
    "# y = np.load('Data/3D/y_old.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".npy 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "x = np.load('Data/3D/x_old.npy')\n",
    "y = np.load('Data/3D/y_old.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Dimensional Trajectory new version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원본 .pickle 로드 및 전처리 후 .npy 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Trajectory types: Normal, Burn time, Xcp position, Thrust Tilt Angle, Fin bias\n",
    "Target Distance = 4000\n",
    "Total states = ['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'err_FinBias_1', 'err_FinBias_2', 'err_FinBias_3', 'err_FinBias_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4', 'err_BurnTime', 'err_Tilt_1', 'err_Tilt_2', 'err_delXcp'], 31 states\n",
    "Used States = ['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4'], 23 states\n",
    "label = [0, 1, 2, 3, 4] for normal, burn_time, xcp_pos, thrust_tilt, fin_bias respectively, 5 classes\n",
    "minimum length of trajectory = 2449 (2448*0.01 = 24.48 seconds)\n",
    "maximum length of trajectory = 2635 (2635*0.01 = 26.35 seconds)\n",
    "fixed sequence length = 2656 (2656*0.01 = 26.56 seconds)\n",
    "'''\n",
    "start = time.time()\n",
    "seq_len = 2656\n",
    "x = []\n",
    "y = []\n",
    "tr = {}\n",
    "for i, tr_type in enumerate(['Normal', 'Burntime', 'Xcpposition', \"ThrustTiltAngle\", 'Finbias']):\n",
    "    N = 1 if i == 0 else 500\n",
    "    for tr_i in range(N):\n",
    "        tr = pd.read_csv(f'Data/3D/new/csv/Type_{i+1}_{tr_i+1}.csv', header=None)\n",
    "        tr.columns = ['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'err_FinBias_1', 'err_FinBias_2', 'err_FinBias_3', 'err_FinBias_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4', 'err_BurnTime', 'err_Tilt_1', 'err_Tilt_2', 'err_delXcp']\n",
    "        tr = tr[['Rmi_1', 'Rmi_2', 'Rmi_3', 'Vmi_1', 'Vmi_2', 'Vmi_3', 'Wmb_1', 'Wmb_2', 'Wmb_3', 'Accm_1', 'Accm_2', 'Accm_3', 'angEuler_1', 'angEuler_2', 'angEuler_3', 'FinOut_1', 'FinOut_2', 'FinOut_3', 'FinOut_4', 'FinCmd_1', 'FinCmd_2', 'FinCmd_3', 'FinCmd_4']].to_numpy()\n",
    "        tr[0] = 4000 - tr[0]\n",
    "        terminal = tr[-1].copy()\n",
    "        terminal[3]=terminal[4]=terminal[5]=terminal[6]=terminal[7]=terminal[8]=terminal[12]=terminal[13]=terminal[14]=terminal[19]=terminal[20]=terminal[21]=terminal[22]=0\n",
    "        tr = np.concatenate((tr, np.tile(terminal, (seq_len-len(tr), 1))), axis=0)\n",
    "        if i == 0:\n",
    "            x = [tr for _ in range(500)]\n",
    "            y = [i for _ in range(500)]\n",
    "        else:\n",
    "            x.append(tr)\n",
    "            y.append(i)\n",
    "\n",
    "x = np.array(x)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "np.save('Data/3D/x_new.npy', x)\n",
    "np.save('Data/3D/y_new.npy', y)\n",
    "# x = np.load('Data/3D/x_new.npy')\n",
    "# y = np.load('Data/3D/y_new.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".npy 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "x = np.load('Data/3D/x_new.npy')\n",
    "y = np.load('Data/3D/y_new.npy')\n",
    "x.shape, y.shape, time.time()-start"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_type = args.data_type\n",
    "args.data_type = 'normal'\n",
    "normal_train_set, normal_train_loader = data_provider(args, 'train')\n",
    "normal_train_C_set, normal_train_C_loader = data_provider(args, 'train_c')\n",
    "args.data_type = 'mixed'\n",
    "mixed_train_set, mixed_train_loader = data_provider(args, 'train')\n",
    "mixed_train_C_set, mixed_train_C_loader = data_provider(args, 'train_c')\n",
    "\n",
    "valid_set, valid_loader = data_provider(args, 'valid')\n",
    "test_set, test_loader = data_provider(args, 'test')\n",
    "\n",
    "normal_loaders = {'train': normal_train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "normal_C_loaders = {'train': normal_train_C_loader, 'val': valid_loader, 'test': test_loader}\n",
    "mixed_loaders = {'train': mixed_train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "mixed_C_loaders = {'train': mixed_train_C_loader, 'val': valid_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrAE(ConvEncoder, ConvDecoder, args).to(args.device)\n",
    "criterion = nn.MSELoss().to(args.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AutoEncoder(model, dataloaders, criterion, optimizer, scheduler, num_epochs=10, phase = None, channel_type = None):\n",
    "    \"\"\"\n",
    "    model: model to train\n",
    "    dataloaders: train, val, test data's loader\n",
    "    criterion: loss function\n",
    "    optimizer: optimizer to update your model\n",
    "    \"\"\"\n",
    "    if phase == 'before':\n",
    "        ts = 0\n",
    "    elif phase == 'initial':\n",
    "        ts = 80\n",
    "    elif phase == 'final':\n",
    "        ts = 411\n",
    "    else:\n",
    "        ts = 0\n",
    "    te = ts + args.seq_len\n",
    "    \n",
    "    if channel_type == 'Rmi':\n",
    "        cs = 0\n",
    "    elif channel_type == 'Vmi':\n",
    "        cs = 3\n",
    "    elif channel_type == 'Wmb':\n",
    "        cs = 6\n",
    "    elif channel_type == 'Accm':\n",
    "        cs = 9\n",
    "    elif channel_type == 'angEuler':\n",
    "        cs = 12\n",
    "    elif channel_type == 'FinOut':\n",
    "        cs = 15\n",
    "    elif channel_type == 'FinCmd':\n",
    "        cs = 19\n",
    "    else:\n",
    "        cs = 0\n",
    "    ce = cs + args.input_channel\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = 100000000\n",
    "    \n",
    "    epoch_iterater = tqdm(range(num_epochs), desc='AE Training', total=num_epochs)\n",
    "    # since = time.time()\n",
    "    for epoch in epoch_iterater:\n",
    "        epoch_iterater.set_description('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "\n",
    "        for mode in ['train', 'val']:\n",
    "            if mode == 'train':\n",
    "                model.train()            # Set model to training mode\n",
    "            else:\n",
    "                model.eval()            # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for inputs, _ in dataloaders[mode]:\n",
    "                inputs = inputs.to(args.device)[:, ts:te, cs:ce].type(torch.float32).transpose(1, 2)  # transfer inputs to GPU \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(mode == 'train'):\n",
    "                    outputs, encoded = model(inputs)\n",
    "                    loss = criterion(outputs, inputs)           # calculate a loss\n",
    "                    if mode == 'train':\n",
    "                        loss.backward()                             # perform back-propagation from the loss\n",
    "                        optimizer.step()                             # perform gradient descent with given optimizer\n",
    "                        scheduler.step()                             # update learning rate\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)                    \n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[mode].dataset)\n",
    "            # deep copy the model\n",
    "            if mode == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            if mode == 'val':\n",
    "                val_loss_history.append(epoch_loss)\n",
    "            if mode == 'val' and epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        epoch_iterater.set_postfix(f'{dataT} phase: {phases.index(phase)+1}/{len(phases)} states: {chTs.index(channel_type)+1}/{len(chTs)} train loss: {train_loss_history[-1]:.4f}, val loss: {val_loss_history[-1]:.4f}, best val loss: {best_val_loss:.4f}')\n",
    "\n",
    "    # time_elapsed = time.time() - since\n",
    "    # print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    # print('Best val Loss: {:4f}'.format(best_val_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "    return model, best_model_wts, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed set Rmi featrue before phase Auto Encoder model training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/100 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need more than 1 value to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mlr)\n\u001b[1;32m     33\u001b[0m scheduler \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, eta_min\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m trained_model, best_wts, train_loss_history, val_loss_history \u001b[39m=\u001b[39m train_AutoEncoder(model, data_loaders, criterion, optimizer, scheduler, args\u001b[39m.\u001b[39;49mtrain_epochs, phase, chT)\n\u001b[1;32m     36\u001b[0m torch\u001b[39m.\u001b[39msave(trained_model\u001b[39m.\u001b[39mstate_dict(), save_ckpt\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m torch\u001b[39m.\u001b[39msave(trained_model\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mstate_dict(), save_ckpt\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mencoder.pth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 77\u001b[0m, in \u001b[0;36mtrain_AutoEncoder\u001b[0;34m(model, dataloaders, criterion, optimizer, scheduler, num_epochs, phase, channel_type)\u001b[0m\n\u001b[1;32m     75\u001b[0m             best_val_loss \u001b[39m=\u001b[39m epoch_loss\n\u001b[1;32m     76\u001b[0m             best_model_wts \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(model\u001b[39m.\u001b[39mstate_dict())\n\u001b[0;32m---> 77\u001b[0m     epoch_iterater\u001b[39m.\u001b[39;49mset_postfix(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mdataT\u001b[39m}\u001b[39;49;00m\u001b[39m phase: \u001b[39;49m\u001b[39m{\u001b[39;49;00mphases\u001b[39m.\u001b[39;49mindex(phase)\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mlen\u001b[39;49m(phases)\u001b[39m}\u001b[39;49;00m\u001b[39m states: \u001b[39;49m\u001b[39m{\u001b[39;49;00mchTs\u001b[39m.\u001b[39;49mindex(channel_type)\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mlen\u001b[39;49m(chTs)\u001b[39m}\u001b[39;49;00m\u001b[39m train loss: \u001b[39;49m\u001b[39m{\u001b[39;49;00mtrain_loss_history[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m:\u001b[39;49;00m\u001b[39m.4f\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m, val loss: \u001b[39;49m\u001b[39m{\u001b[39;49;00mval_loss_history[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m:\u001b[39;49;00m\u001b[39m.4f\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m, best val loss: \u001b[39;49m\u001b[39m{\u001b[39;49;00mbest_val_loss\u001b[39m:\u001b[39;49;00m\u001b[39m.4f\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     79\u001b[0m \u001b[39m# time_elapsed = time.time() - since\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39m# print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m# print('Best val Loss: {:4f}'.format(best_val_loss))\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m \u001b[39m# load best model weights\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39m# model.load_state_dict(best_model_wts)\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mreturn\u001b[39;00m model, best_model_wts, train_loss_history, val_loss_history\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1429\u001b[0m, in \u001b[0;36mtqdm.set_postfix\u001b[0;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \u001b[39mSet/modify postfix (additional stats)\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \u001b[39mwith automatic formatting based on datatype.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[39mkwargs  : dict, optional\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m \u001b[39m# Sort in alphabetical order to be more deterministic\u001b[39;00m\n\u001b[0;32m-> 1429\u001b[0m postfix \u001b[39m=\u001b[39m OrderedDict([] \u001b[39mif\u001b[39;49;00m ordered_dict \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m ordered_dict)\n\u001b[1;32m   1430\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(kwargs\u001b[39m.\u001b[39mkeys()):\n\u001b[1;32m   1431\u001b[0m     postfix[key] \u001b[39m=\u001b[39m kwargs[key]\n",
      "\u001b[0;31mValueError\u001b[0m: need more than 1 value to unpack"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phases = ['before', 'initial', 'final'] if args.data == '3D/new/' else ['whole']\n",
    "phase2len = {'before':96, 'initial':352, 'final':2240} if args.data == '3D/new/' else {'whole':3200}\n",
    "chTs = ['Rmi', 'Vmi', 'Wmi', 'Accm', 'angEuler', 'FinOut', 'FinCmd'] if args.data == '3D/new/' else ['Rmi']\n",
    "ch = {'Rmi':3, 'Vmi':3, 'Wmi':3, 'Accm':3, 'angEuler':3, 'FinOut':4, 'FinCmd':4} if args.data == '3D/new/' else {'Rmi':2}\n",
    "plot_cnt = 0\n",
    "temp_seq_len = args.seq_len\n",
    "\n",
    "for dataT in ['mixed', 'normal']:\n",
    "    # if dataT == 'normal':\n",
    "    #     continue\n",
    "    args.data_type = dataT\n",
    "    # train_set, train_loader = data_provider(args, 'train')\n",
    "    # valid_set, valid_loader = data_provider(args, 'valid')\n",
    "    # test_set, test_loader = data_provider(args, 'test')\n",
    "    # data_loaders = {'train': train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "    if dataT == 'mixed':\n",
    "        data_loaders = mixed_loaders\n",
    "    else:\n",
    "        data_loaders = normal_loaders\n",
    "    \n",
    "    for phase in phases:\n",
    "        plot_cnt += 1\n",
    "        plt.subplot(4, len(phases), plot_cnt)\n",
    "        for chT in chTs:\n",
    "            if args.data == '3D/new/':\n",
    "                args.seq_len = phase2len[phase]\n",
    "                args.input_channel = ch[chT]\n",
    "            save_ckpt = f'{args.checkpoints+args.data+args.model}_{args.data_type}_{chT}_{phase}_'\n",
    "            print(f'{dataT} set {chT} featrue {phase} phase Auto Encoder model training start')\n",
    "            model = TrAE(ConvEncoder, ConvDecoder, args).to(args.device)\n",
    "            criterion = nn.MSELoss().to(args.device)\n",
    "            optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5)\n",
    "            \n",
    "            trained_model, best_wts, train_loss_history, val_loss_history = train_AutoEncoder(model, data_loaders, criterion, optimizer, scheduler, args.train_epochs, phase, chT)\n",
    "            torch.save(trained_model.state_dict(), save_ckpt+'model.pth')\n",
    "            torch.save(trained_model.encoder.state_dict(), save_ckpt+'encoder.pth')\n",
    "            trained_model.load_state_dict(best_wts)\n",
    "            torch.save(trained_model.state_dict(), save_ckpt+'best_model.pth')\n",
    "            torch.save(trained_model.encoder.state_dict(), save_ckpt+'best_encoder.pth')\n",
    "            print('model saved to %s' % save_ckpt)\n",
    "            \n",
    "            plt.figure(figsize=(4, 3))\n",
    "            plt.plot(train_loss_history, label=f'{phase} {chT} train')\n",
    "            plt.plot(val_loss_history, label=f'{phase} {chT} val')\n",
    "        #     break\n",
    "        # break\n",
    "    args.seq_len = temp_seq_len\n",
    "    # break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's draw a learning curve like below.\n",
    "plt.plot(train_loss_history, label='train')\n",
    "plt.plot(val_loss_history, label='val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4150302.8733333335\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in data_loaders[\"test\"]:\n",
    "        inputs = inputs.to(args.device)[:, 0:96, :4].type(torch.float32).transpose(1, 2)\n",
    "\n",
    "        outputs, encoded = trained_model(inputs)\n",
    "        test_loss = criterion(outputs, inputs)\n",
    "        \n",
    "        running_loss += test_loss.item() * inputs.size(0)\n",
    "\n",
    "    test_loss = running_loss / len(data_loaders[\"test\"].dataset)\n",
    "    print(test_loss)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = ['before', 'initial', 'final'] if args.data == '3D/new/' else ['whole']\n",
    "phase2len = {'before':96, 'initial':352, 'final':2240} if args.data == '3D/new/' else {'whole':3200}\n",
    "chTs = ['Rmi', 'Vmi', 'Wmi', 'Accm', 'angEuler', 'FinOut', 'FinCmd'] if args.data == '3D/new/' else ['Rmi']\n",
    "ch = {'Rmi':3, 'Vmi':3, 'Wmi':3, 'Accm':3, 'angEuler':3, 'FinOut':4, 'FinCmd':4} if args.data == '3D/new/' else {'Rmi':2}\n",
    "plot_cnt = 0\n",
    "temp_seq_len = args.seq_len\n",
    "\n",
    "for dataT in ['mixed', 'normal']:\n",
    "    # if dataT == 'normal':\n",
    "    #     continue\n",
    "    args.data_type = dataT\n",
    "    # train_set, train_loader = data_provider(args, 'train')\n",
    "    # valid_set, valid_loader = data_provider(args, 'valid')\n",
    "    # test_set, test_loader = data_provider(args, 'test')\n",
    "    # data_loaders = {'train': train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "    \n",
    "    if dataT == 'mixed':\n",
    "        data_loaders = mixed_loaders\n",
    "    else:\n",
    "        data_loaders = normal_loaders\n",
    "    \n",
    "    for phase in phases:\n",
    "        plot_cnt += 1\n",
    "        plt.subplot(4, len(phases), plot_cnt)\n",
    "        for chT in chTs:\n",
    "            args.seq_len = phase2len[phase]\n",
    "            args.input_channel = ch[chT]\n",
    "            for modelT in ['model', 'best_model']:\n",
    "                save_ckpt = f'{args.checkpoints+args.data+args.model}_{args.data_type}_{chT}_{phase}_{modelT}'\n",
    "                model = TrAE(ConvEncoder, ConvDecoder, args).to(args.device)\n",
    "                model.load_state_dict(torch.load(save_ckpt+'.pth'))\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    running_loss = 0.0\n",
    "                    for inputs, labels in data_loaders[\"test\"]:\n",
    "                        inputs = inputs.to(args.device)[:,:,:2].type(torch.float32).transpose(1, 2)\n",
    "\n",
    "                        # outputs, encoded = trained_model(inputs)\n",
    "                        outputs, encoded = model(inputs)\n",
    "                        test_loss = criterion(outputs, inputs)\n",
    "                        \n",
    "                        running_loss += test_loss.item() * inputs.size(0)\n",
    "\n",
    "                    test_loss = running_loss / len(data_loaders[\"test\"].dataset)\n",
    "                    print(test_loss)\n",
    "\n",
    "                tr = inputs.cpu().numpy()\n",
    "                tr_hat = outputs.cpu().numpy()\n",
    "\n",
    "                f, axes = plt.subplots(2, 5)\n",
    "                f.set_size_inches((15, 5))\n",
    "                plt.subplots_adjust(wspace = 0.3, hspace = 0.3)\n",
    "\n",
    "                for i, n in enumerate(range(0, len(tr)-1, len(tr)//5)):\n",
    "                    axes[0, i].plot(tr[n, 0, :], tr[n, 1, :])\n",
    "                    # axes[0, i].plot(tr[n, 0, :])\n",
    "                    # axes[0, i].plot(tr[n, 1, :])\n",
    "                    axes[1, i].plot(tr_hat[n, 0, :], tr_hat[n, 1, :])\n",
    "                    # axes[1, i].plot(tr_hat[n, 0, :])\n",
    "                    # axes[1, i].plot(tr_hat[n, 1, :])\n",
    "\n",
    "                plt.show()\n",
    "        #         break\n",
    "        #     break\n",
    "        # break\n",
    "    args.seq_len = temp_seq_len\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(encoder, model, dataloaders, criterion, optimizer, scheduler, num_epochs=10, phase = None, channel_type = None):\n",
    "    \"\"\"\n",
    "    model: model to train\n",
    "    dataloaders: train, val, test data's loader\n",
    "    criterion: loss function\n",
    "    optimizer: optimizer to update your model\n",
    "    \"\"\"\n",
    "    if phase == 'before':\n",
    "        ts = 0\n",
    "    elif phase == 'initial':\n",
    "        ts = 80\n",
    "    elif phase == 'final':\n",
    "        ts = 411\n",
    "    else:\n",
    "        ts = 0\n",
    "    te = ts + args.seq_len\n",
    "    \n",
    "    if channel_type == 'Rmi':\n",
    "        cs = 0\n",
    "    elif channel_type == 'Vmi':\n",
    "        cs = 3\n",
    "    elif channel_type == 'Wmb':\n",
    "        cs = 6\n",
    "    elif channel_type == 'Accm':\n",
    "        cs = 9\n",
    "    elif channel_type == 'angEuler':\n",
    "        cs = 12\n",
    "    elif channel_type == 'FinOut':\n",
    "        cs = 15\n",
    "    elif channel_type == 'FinCmd':\n",
    "        cs = 19\n",
    "    else:\n",
    "        cs = 0\n",
    "    ce = cs + args.input_channel\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = 100000000\n",
    "\n",
    "    encoder.eval()\n",
    "    \n",
    "    # since = time.time()\n",
    "    epoch_iterater = tqdm(range(num_epochs), desc='C Training', total=num_epochs)\n",
    "    for epoch in epoch_iterater:\n",
    "        epoch_iterater.set_description('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "\n",
    "        for mode in ['train', 'val']:\n",
    "            if mode == 'train':\n",
    "                model.train()            # Set model to training mode\n",
    "            else:\n",
    "                model.eval()            # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in dataloaders[mode]:\n",
    "                inputs = inputs.to(args.device)[:, ts:te, cs:ce].type(torch.float32).transpose(1, 2)  # transfer inputs to GPU \n",
    "                encoded = encoder(inputs)\n",
    "                labels = F.one_hot(labels, num_classes=args.class_num).to(args.device).type(torch.float32)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(mode == 'train'):\n",
    "                    outputs = model(encoded.reshape(encoded.size(0), -1))  # forward pass\n",
    "                    loss = criterion(outputs, labels)           # calculate a loss\n",
    "                    loss = criterion(outputs.view(-1), labels.view(-1))\n",
    "                    if mode == 'train':\n",
    "                        loss.backward()                             # perform back-propagation from the loss\n",
    "                        optimizer.step()                             # perform gradient descent with given optimizer\n",
    "                        scheduler.step()                             # update learning rate\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)                    \n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[mode].dataset)\n",
    "            # print('{} Loss: {:.4f}'.format(mode, epoch_loss))\n",
    "            \n",
    "            # deep copy the model\n",
    "            if mode == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "            if mode == 'val':\n",
    "                val_loss_history.append(epoch_loss)\n",
    "            if mode == 'val' and epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        epoch_iterater.set_postfix({'data':dataT, 'phase':f'{phases.index(phase)+1}/{len(phases)}', 'states': f'{chTs.index(channel_type)+1}/{len(chTs)}', 'loss':lossf[:-1],  'train loss': train_loss_history[-1], 'val loss': val_loss_history[-1], 'best val loss': best_val_loss})\n",
    "        # print()\n",
    "\n",
    "    # time_elapsed = time.time() - since\n",
    "    # print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    # print('Best val Loss: {:4f}'.format(best_val_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    # model.load_state_dict(best_model_wts)\n",
    "    return model, best_model_wts, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed set Rmi featrue before phase MSE loss Classifier model training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: 100%|██████████| 100/100 [01:50<00:00,  1.10s/it, data=mixed, phase=1/3, states=1/7, loss=MSE, train loss=0.16, val loss=0.161, best val loss=0.149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved to ./checkpoints/3D/new/MSE/A_mixed_Rmi_before_\n",
      "mixed set Rmi featrue before phase CE loss Classifier model training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100:   8%|▊         | 8/100 [00:08<01:40,  1.09s/it, data=mixed, phase=1/3, states=1/7, loss=CE, train loss=1.61, val loss=1.64, best val loss=1.57]Exception ignored in: <function _releaseLock at 0x7fd8eed2d000>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n",
      "Epoch 20/100:  19%|█▉        | 19/100 [00:20<01:28,  1.10s/it, data=mixed, phase=1/3, states=1/7, loss=CE, train loss=1.61, val loss=1.61, best val loss=1.57]"
     ]
    }
   ],
   "source": [
    "phases = ['before', 'initial', 'final'] if args.data == '3D/new/' else ['whole']\n",
    "phase2len = {'before':96, 'initial':352, 'final':2240} if args.data == '3D/new/' else {'whole':3200}\n",
    "chTs = ['Rmi', 'Vmi', 'Wmi', 'Accm', 'angEuler', 'FinOut', 'FinCmd'] if args.data == '3D/new/' else ['Rmi']\n",
    "ch = {'Rmi':3, 'Vmi':3, 'Wmi':3, 'Accm':3, 'angEuler':3, 'FinOut':4, 'FinCmd':4} if args.data == '3D/new/' else {'Rmi':2}\n",
    "plot_cnt = 0\n",
    "temp_seq_len = args.seq_len\n",
    "\n",
    "for dataT in ['mixed', 'normal']:\n",
    "    # if dataT == 'normal':\n",
    "    #     continue\n",
    "    args.seq_len = temp_seq_len\n",
    "    args.data_type = dataT\n",
    "    # train_set, train_loader = data_provider(args, 'train_c')\n",
    "    # valid_set, valid_loader = data_provider(args, 'valid')\n",
    "    # test_set, test_loader = data_provider(args, 'test')\n",
    "    # data_loaders = {'train': train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "    if dataT == 'mixed':\n",
    "        data_loaders = mixed_C_loaders\n",
    "    else:\n",
    "        data_loaders = normal_C_loaders\n",
    "    \n",
    "    for phase in phases:\n",
    "        plot_cnt += 1\n",
    "        plt.subplot(4, len(phases), plot_cnt)\n",
    "        for chT in chTs:\n",
    "            if args.data == '3D/new/':\n",
    "                args.seq_len = phase2len[phase]\n",
    "                args.input_channel = ch[chT]\n",
    "            for lossf in ['MSE/', 'CE/']:\n",
    "                if lossf == 'MSE/':\n",
    "                    criterion = nn.MSELoss().to(args.device)\n",
    "                else:\n",
    "                    criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "                load_ckpt = f'{args.checkpoints+args.data+args.model}_{args.data_type}_{chT}_{phase}_'\n",
    "                save_ckpt = f'{args.checkpoints+args.data+lossf+args.model}_{args.data_type}_{chT}_{phase}_'\n",
    "                \n",
    "                print(f'{dataT} set {chT} featrue {phase} phase {lossf[:-1]} loss Classifier model training start')\n",
    "                encoder = ConvEncoder(args.input_channel, args.t, args.output_channel).to(args.device)\n",
    "                encoder.load_state_dict(torch.load(load_ckpt+'encoder.pth'))\n",
    "                model = Classifier(args.seq_len//(2**args.layer_num)*args.output_channel, args.class_num).to(args.device)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "                scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-5)\n",
    "                \n",
    "                trained_model, best_wts, train_loss_history, val_loss_history = train_classifier(encoder, model, data_loaders, criterion, optimizer, scheduler, args.train_epochs, phase, chT)\n",
    "                torch.save(trained_model.state_dict(), save_ckpt+'classifier.pth')\n",
    "                torch.save(best_wts, save_ckpt+'best_classifier.pth')\n",
    "                print('model saved to %s' % save_ckpt)\n",
    "                \n",
    "                plt.figure(figsize=(4, 3))\n",
    "                plt.plot(train_loss_history, label=f'{phase} {chT} train')\n",
    "                plt.plot(val_loss_history, label=f'{phase} {chT} val')\n",
    "        #     break\n",
    "        # break\n",
    "    args.seq_len = temp_seq_len\n",
    "    # break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 2, 2, 3, 1, 3, 1, 4, 1, 1, 0, 0, 4, 3, 2, 2, 2, 4, 0, 4, 0, 2, 2,\n",
       "        4, 2, 2, 1, 4, 0, 1, 1, 0, 1, 0, 4, 3, 2, 3, 2, 1, 4, 3, 4, 2, 4, 0, 0,\n",
       "        1, 3, 2, 3, 4, 2, 1, 3, 3, 2, 1, 0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(mixed_C_loaders['train']))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 5]), torch.Size([64, 5]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(next(iter(mixed_C_loaders['train']))[1], num_classes=args.class_num).shape, model(encoder(next(iter(mixed_C_loaders['train']))[0][:,:96,:3].to(args.device).transpose(1,2).type(torch.float32)).reshape(64, -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289],\n",
       "        [0.0175, 0.0351, 0.0154, 0.0103, 0.0289]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(encoder(next(iter(mixed_C_loaders['train']))[0][:,:96,:3].to(args.device).transpose(1,2).type(torch.float32)).reshape(64, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.seq_len = 2651"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixed classifier MSE/\n",
      "Accuracy: 13.333333333333334 %\n",
      "mixed best_classifier MSE/\n",
      "Accuracy: 46.666666666666664 %\n",
      "mixed classifier CE/\n",
      "Accuracy: 13.333333333333334 %\n",
      "mixed best_classifier CE/\n",
      "Accuracy: 46.666666666666664 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "phases = ['before', 'initial', 'final'] if args.data == '3D/new/' else ['whole']\n",
    "phase2len = {'before':96, 'initial':352, 'final':2240} if args.data == '3D/new/' else {'whole':3200}\n",
    "seq_start = {'before':0, 'initial':80, 'final':411} if args.data == '3D/new/' else {'whole':0}\n",
    "chTs = ['Rmi', 'Vmi', 'Wmi', 'Accm', 'angEuler', 'FinOut', 'FinCmd'] if args.data == '3D/new/' else ['Rmi']\n",
    "ch = {'Rmi':3, 'Vmi':3, 'Wmi':3, 'Accm':3, 'angEuler':3, 'FinOut':4, 'FinCmd':4} if args.data == '3D/new/' else {'Rmi':2}\n",
    "channel_start = {'Rmi':0, 'Vmi':3, 'Wmi':6, 'Accm':9, 'angEuler':12, 'FinOut':15, 'FinCmd':19} if args.data == '3D/new/' else {'Rmi':0}\n",
    "plot_cnt = 0\n",
    "temp_seq_len = args.seq_len\n",
    "\n",
    "for dataT in ['mixed', 'normal']:\n",
    "    # if dataT == 'normal':\n",
    "    #     continue\n",
    "    args.seq_len = temp_seq_len\n",
    "    args.data_type = dataT\n",
    "    # train_set, train_loader = data_provider(args, 'train_c')\n",
    "    # valid_set, valid_loader = data_provider(args, 'valid')\n",
    "    # test_set, test_loader = data_provider(args, 'test')\n",
    "    # data_loaders = {'train': train_loader, 'val': valid_loader, 'test': test_loader}\n",
    "    if dataT == 'mixed':\n",
    "        data_loaders = mixed_C_loaders\n",
    "    else:\n",
    "        data_loaders = normal_C_loaders\n",
    "    \n",
    "    for phase in phases:\n",
    "        for chT in chTs:\n",
    "            if args.data == '3D/new/':\n",
    "                args.seq_len = phase2len[phase]\n",
    "                args.input_channel = ch[chT]\n",
    "            for lossf in ['MSE/', 'CE/']:\n",
    "                if lossf == 'MSE/':\n",
    "                    criterion = nn.MSELoss().to(args.device)\n",
    "                else:\n",
    "                    criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "                encoder_ckpt = f'{args.checkpoints+args.data+args.model}_{args.data_type}_{chT}_{phase}_'\n",
    "                classifier_ckpt = f'{args.checkpoints+args.data+lossf+args.model}_{args.data_type}_{chT}_{phase}_'\n",
    "                \n",
    "                ts = seq_start[phase]\n",
    "                te = ts + args.seq_len\n",
    "                cs = channel_start[chT]\n",
    "                ce = cs + args.input_channel\n",
    "                \n",
    "                for classifierT in ['', 'best_']:\n",
    "                    encoder = ConvEncoder(args.input_channel, args.t, args.output_channel).to(args.device)\n",
    "                    encoder.load_state_dict(torch.load(encoder_ckpt+'encoder.pth'))\n",
    "                    classifier = Classifier(args.seq_len//(2**args.layer_num)*args.output_channel, args.class_num).to(args.device)\n",
    "                    classifier.load_state_dict(torch.load(classifier_ckpt+classifierT+'classifier.pth'))\n",
    "                    \n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        for inputs, labels in data_loaders[\"test\"]:\n",
    "                            inputs = inputs.to(args.device)[:,ts:te,cs:ce].type(torch.float32).transpose(1, 2)\n",
    "                            labels_onehot = F.one_hot(labels, num_classes=args.class_num).to(args.device).type(torch.float32)\n",
    "                            labels = labels.to(args.device).type(torch.float32)\n",
    "\n",
    "                            hidden = encoder(inputs)\n",
    "                            outputs = classifier(hidden.reshape(hidden.size(0), -1)).type(torch.float32)\n",
    "                            \n",
    "                            total += labels.size(0)\n",
    "                            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                    print(args.data_type, classifierT+'classifier', lossf)\n",
    "                    print('Accuracy:', correct/total * 100, '%')\n",
    "            break\n",
    "        break\n",
    "    args.seq_len = temp_seq_len\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9900, 0.9199, 0.9946, 0.4191, 1.0881],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9901, 0.9199, 0.9946, 0.4191, 1.0881],\n",
       "        [1.9903, 0.9199, 0.9948, 0.4194, 1.0879],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9902, 0.9199, 0.9947, 0.4192, 1.0880],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9902, 0.9199, 0.9947, 0.4192, 1.0880],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9904, 0.9199, 0.9948, 0.4195, 1.0879],\n",
       "        [1.9903, 0.9205, 0.9946, 0.4193, 1.0887],\n",
       "        [1.9895, 0.9200, 0.9944, 0.4183, 1.0884],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9895, 0.9200, 0.9943, 0.4182, 1.0884],\n",
       "        [1.9902, 0.9199, 0.9947, 0.4192, 1.0880],\n",
       "        [1.9902, 0.9199, 0.9947, 0.4193, 1.0880],\n",
       "        [1.9900, 0.9199, 0.9946, 0.4191, 1.0881],\n",
       "        [1.9901, 0.9199, 0.9946, 0.4192, 1.0880],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9899, 0.9199, 0.9945, 0.4189, 1.0881],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9885, 0.9199, 0.9939, 0.4168, 1.0889],\n",
       "        [1.9895, 0.9199, 0.9943, 0.4182, 1.0884],\n",
       "        [1.9903, 0.9205, 0.9946, 0.4193, 1.0887],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9899, 0.9199, 0.9945, 0.4189, 1.0881],\n",
       "        [1.9902, 0.9193, 0.9947, 0.4193, 1.0872],\n",
       "        [1.9902, 0.9199, 0.9947, 0.4193, 1.0880],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9901, 0.9199, 0.9946, 0.4192, 1.0880],\n",
       "        [1.9890, 0.9200, 0.9941, 0.4174, 1.0887],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9902, 0.9199, 0.9947, 0.4192, 1.0880],\n",
       "        [1.9903, 0.9199, 0.9948, 0.4194, 1.0879],\n",
       "        [1.9904, 0.9199, 0.9948, 0.4195, 1.0879],\n",
       "        [1.9903, 0.9199, 0.9948, 0.4194, 1.0879],\n",
       "        [1.9888, 0.9199, 0.9940, 0.4172, 1.0888],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879],\n",
       "        [1.9903, 0.9199, 0.9947, 0.4194, 1.0879]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 0., 2., 3., 0., 3., 0., 0., 0., 3., 0., 3., 4., 1., 0., 1., 3.,\n",
       "        3., 2., 3., 0., 0., 2., 0., 0., 0., 1., 1., 4., 0., 0., 2., 4., 4., 0.,\n",
       "        2., 1., 0., 3., 3., 3., 3., 1., 0., 0.], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_onehot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
